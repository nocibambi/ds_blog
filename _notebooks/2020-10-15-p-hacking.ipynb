{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.8.2 64-bit ('stackoverflow': conda)",
   "display_name": "Python 3.8.2 64-bit ('stackoverflow': conda)",
   "metadata": {
    "interpreter": {
     "hash": "f3131ab0c53afd587e929ab5f3e0081bb3b1675889ab2c259292a9bd8773e05a"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Programmatic p-hacking from scratch with Python\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Below is a short script to demonstrate the 'process of p-hacking'.\n",
    "\n",
    "From the [Data Science from Scratch book](https://www.oreilly.com/library/view/data-science-from/9781492041122/)."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "import random\n",
    "from typing import List"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 1,
   "outputs": []
  },
  {
   "source": [
    "First we define a usual experiment consisting of 1000 binomial trials with 0.5 probability."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Proportion of heads: 0.51\nFirst 10 elements: [True, True, True, False, False, False, False, True, True, True]\n"
     ]
    }
   ],
   "source": [
    "def run_experiment(trials) -> List[bool]:\n",
    "    return [random.random() < 0.5 for _ in range(trials)]\n",
    "\n",
    "experiment = run_experiment(1000)\n",
    "\n",
    "print(\"Proportion of heads:\", sum(experiment) / len(experiment))\n",
    "print(\"First 10 elements:\", experiment[:10])"
   ]
  },
  {
   "source": [
    "Then we examine whether the outcome an experiment is beyond the 95% confidence levels around p = 0.5, that is, the hypothesis of having a fair coin."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "def reject_fairness(experiment: List[bool]) -> bool:\n",
    "    num_heads = sum(experiment)\n",
    "    return num_heads < 469 or num_heads > 531\n",
    "\n",
    "reject_fairness(experiment)"
   ]
  },
  {
   "source": [
    "We run 1000 independent experiments with the exact same parameters."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(42)\n",
    "experiments = [run_experiment(1000) for _ in range(1000)]"
   ]
  },
  {
   "source": [
    "Now we can simply pick those experiments which fall outside the confidence level."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Number of experiments 'showing' that the coin if unfair: 42\n\nProbabilities:\n0.532\t0.539\t0.535\t0.461\t0.466\t0.539\t0.467\t0.468\t0.54\t0.458\t0.468\t0.463\t0.467\t0.46\t0.461\t0.463\t0.541\t0.464\t0.538\t0.542\t0.461\t0.465\t0.468\t0.538\t0.466\t0.46\t0.468\t0.534\t0.535\t0.468\t0.537\t0.468\t0.535\t0.538\t0.451\t0.537\t0.463\t0.466\t0.46\t0.536\t0.466\t0.467\n"
     ]
    }
   ],
   "source": [
    "number_of_unfair = sum([reject_fairness(experiment) for experiment in experiments])\n",
    "\n",
    "print(\"Number of experiments 'showing' that the coin if unfair:\", number_of_unfair)\n",
    "print(\"\\nProbabilities:\")\n",
    "print(\"\\t\".join([str(sum(experiment) / len(experiment)) for experiment in experiments if reject_fairness(experiment)]))"
   ]
  }
 ]
}