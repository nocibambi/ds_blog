{
  
    
        "post0": {
            "title": "How to use BigQuery in a Jupyter notebook?",
            "content": "When we try to analyze huge datasets (like blockchain data) through BigQuery, it is useful to run the data load and conversion in the cloud and use our local environment mostly for the final transformations and visualization. For these cases, a jupyter notebook seems to be a fitting environment. . There are at least three main ways by which we can access BigQuery data from a notebook: . The most simple one is to use the pandas-gbq library (we already covered it in this previous post) | Using google&#39;s BigQuery notebook extension | The BigQuery python API | Here we will introduce the last two versions. . (This post is based on this official tutorial.) . Setting credentials . Compared to the pandas-gbq library, we need to define the credentials explicitly. . import os os.environ[&quot;GOOGLE_APPLICATION_CREDENTIALS&quot;]=&quot;/home/andras/.credentials/Notebook bigquery-c422e406404b.json&quot; . BigQuery extension . For the extension to work, we need to install the google-cloud-bigquery library. . conda install -c conda-forge google-cloud-bigquery . Then, we load the bigquery extension . %load_ext google.cloud.bigquery . By using the %%bigquery magic command, we immediately define the result of a query as a pandas dataframe. . Here, we pull the total Ethereum token spendings for each day during 2019. . %%bigquery token_transfers SELECT SUM(SAFE_CAST(value AS FLOAT64)) AS total_spent, SAFE_CAST(EXTRACT(DATE FROM block_timestamp) AS DATETIME) AS date FROM `bigquery-public-data.ethereum_blockchain.token_transfers` WHERE EXTRACT(YEAR FROM block_timestamp) = 2019 GROUP BY date ORDER BY date DESC . token_transfers . total_spent date . 0 2.331472e+79 | 2019-12-31 | . 1 4.078401e+79 | 2019-12-30 | . 2 1.773262e+79 | 2019-12-29 | . 3 3.553959e+79 | 2019-12-28 | . 4 2.751157e+79 | 2019-12-27 | . ... ... | ... | . 360 4.226642e+78 | 2019-01-05 | . 361 4.122376e+80 | 2019-01-04 | . 362 1.804472e+80 | 2019-01-03 | . 363 5.833122e+78 | 2019-01-02 | . 364 4.245749e+78 | 2019-01-01 | . 365 rows × 2 columns . import altair as alt alt.data_transformers.disable_max_rows() label = alt.selection_single( # encodings=[&#39;x&#39;], on=&#39;mouseover&#39;, nearest=True, empty=&#39;none&#39; ) chart = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;total_spent:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)) ) alt.layer( chart, chart.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), alt.Chart().mark_rule(color=&#39;darkgray&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), chart.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=alt.Text(&#39;total_spent:Q&#39;, format=&#39;,.2e&#39;) ).transform_filter(label), chart.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=alt.Text(&#39;total_spent:Q&#39;, format=&#39;,.2e&#39;) ).transform_filter(label), data=token_transfers ).properties(width=600, height=400, title=&#39;Daily token spending during 2019 (log scale)&#39;) . BigQuery module . from google.cloud import bigquery client = bigquery.Client() . query =&quot;&quot;&quot; SELECT from_address, SUM(SAFE_CAST(value AS FLOAT64)) AS total_spent, AVG(SAFE_CAST(value AS INT64)) AS average_spent, COUNT(1) AS times_spent FROM `bigquery-public-data.ethereum_blockchain.token_transfers` WHERE EXTRACT(YEAR FROM block_timestamp) = 2019 AND EXTRACT(MONTH FROM block_timestamp) = 09 AND EXTRACT(DAY FROM block_timestamp) = 24 AND EXTRACT(HOUR FROM block_timestamp) = 12 GROUP BY from_address ORDER BY times_spent DESC&quot;&quot;&quot; . For this example we calculate the number of spendings and their total and average values from addresses over a hour. . df = client.query(query).to_dataframe() df . from_address total_spent average_spent times_spent . 0 0x7ba732b1bb952155b720250b477ce154e19ad62f | 1.686990e+12 | 1.963900e+09 | 859 | . 1 0x262e4155e8c5519e668ec26f353d75dd9c18e78f | 3.132121e+23 | NaN | 365 | . 2 0xbd8da72e2f42f5c68b59ee02c2245599ccd702dc | 2.592921e+24 | NaN | 294 | . 3 0x0000000000000000000000000000000000000000 | 7.401857e+24 | 3.058231e+17 | 268 | . 4 0xa71c8bae673f99ac6c0f32c56efc89a8ddb9a501 | 3.896125e+12 | 1.504295e+10 | 259 | . ... ... | ... | ... | ... | . 5279 0xcd338611d74243844f3190b621eb781db53d20b4 | 1.630439e+23 | NaN | 1 | . 5280 0xa9d6b0ad82e46db1895a412ec96b00e18bf95b49 | 1.000000e+09 | 1.000000e+09 | 1 | . 5281 0x4aee792a88edda29932254099b9d1e06d537883f | 5.740766e+22 | NaN | 1 | . 5282 0x71e29ec9e13a39062269fc5c8cba155bb850b23a | 2.270000e+10 | 2.270000e+10 | 1 | . 5283 0x140d6fac06496b21efd086e107d5eca1a16592b3 | 4.335616e+08 | 4.335616e+08 | 1 | . 5284 rows × 4 columns . alt.Chart(df).mark_rect().encode( alt.X(&#39;times_spent:Q&#39;, bin=alt.BinParams(maxbins=20)), alt.Y(&#39;total_spent:Q&#39;, bin=alt.BinParams(maxbins=20)), alt.Color(&#39;count()&#39;), alt.Tooltip(&#39;count()&#39;) ).properties(title=&#39;Total spending value and spending frequency&#39;) . As expected, there are a few number of addresses responsible for the highest spending frequency and the highest spending value during that hour. .",
            "url": "andrasnovoszath.com/2020/09/25/bigquery-jupyter.html",
            "relUrl": "/2020/09/25/bigquery-jupyter.html",
            "date": " • Sep 25, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Accessing Ethereum token transaction data with ``pandas-gbq``",
            "content": "One way to access blockchain bigquery data with Google BigQuery is to use the pandas-gbq library. . It makes it really easy to take a BigQuery SQL query and download its results as a pandas DataFrame. . Big Query authentication . In order to use this library, you need to authenticate a credential with BigQuery. I found creating a &#39;Service account&#39; to be the most meaningful. You can follow the steps here. . Installation . Installation with conda is straightforward. . $ conda install pandas-gbq --channel conda-forge . Basic usage . We can download a table by defining an SQL query and passing it to the read_gbq method. . import altair as alt import pandas_gbq . As Ethereum data is big, we constrain our query to a single hour. . sql = &quot;&quot;&quot; SELECT block_timestamp, value FROM `bigquery-public-data.ethereum_blockchain.token_transfers` WHERE EXTRACT(YEAR FROM block_timestamp) = 2019 AND EXTRACT(MONTH FROM block_timestamp) = 09 AND EXTRACT(DAY FROM block_timestamp) = 24 AND EXTRACT(HOUR FROM block_timestamp) = 12 &quot;&quot;&quot; . token = pandas_gbq.read_gbq(sql, project_id=&quot;pandas-gbq-test-290508&quot;) . Downloading: 100%|██████████| 15392/15392 [00:03&lt;00:00, 4846.48rows/s] . token . block_timestamp value . 0 2019-09-24 12:47:43+00:00 | 50058584428 | . 1 2019-09-24 12:47:43+00:00 | 30000000 | . 2 2019-09-24 12:47:43+00:00 | 14069000000 | . 3 2019-09-24 12:47:43+00:00 | 170000000 | . 4 2019-09-24 12:47:43+00:00 | 59478199 | . ... ... | ... | . 15387 2019-09-24 12:40:54+00:00 | 3577720000000000000000 | . 15388 2019-09-24 12:40:54+00:00 | 2314760000000000000000 | . 15389 2019-09-24 12:40:54+00:00 | 2399000000000000000000 | . 15390 2019-09-24 12:40:54+00:00 | 5701000000000000000000 | . 15391 2019-09-24 12:40:54+00:00 | 3608000000000000000000 | . 15392 rows × 2 columns . Data transformation . After loading the dataset, it requires some transformations. . First, we want to sort it by timestamps. . token.dtypes . block_timestamp datetime64[ns, UTC] value object dtype: object . token = token.sort_values(&#39;block_timestamp&#39;) . Then we want to convert the values column to float. . token[&#39;value&#39;] = token[&#39;value&#39;].astype(float) . Visualization . As altair does not allow visualization with more than 5000 rows, we need to manually set it possible. . alt.data_transformers.disable_max_rows() . DataTransformerRegistry.enable(&#39;default&#39;) . We plot the average transaction values by minute. As we there are a few number of very high value transactions, we use a log scale. . alt.Chart(token).mark_line().encode( alt.X(&#39;utchoursminutes(block_timestamp):T&#39;), alt.Y(&#39;average(value):Q&#39;, scale=alt.Scale(type=&#39;log&#39;)) ).properties(width=800, title=&#39;Average token transaction values (log scale)&#39;) .",
            "url": "andrasnovoszath.com/2020/09/24/pandas-gbq.html",
            "relUrl": "/2020/09/24/pandas-gbq.html",
            "date": " • Sep 24, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Random prediction",
            "content": "When we try to build a prediction algorithm, it is a useful practice to first try out a baseline algorithm and see how they perform. Here we will describe a case of random prediction. . (Inspiration and examples are from Jason Brownlee&#39;s Machine Learning Algorithms from Scratch book.) . import pandas as pd import altair as alt import numpy as np from vega_datasets import data . np.random.seed(42) . For the examples we will use the vega &#39;volcano&#39; dataset. The width and height values are constant, so we work only with the values column. . volcano = data(&#39;volcano&#39;) volcano.head() . width height values . 0 87 | 61 | 103 | . 1 87 | 61 | 104 | . 2 87 | 61 | 104 | . 3 87 | 61 | 105 | . 4 87 | 61 | 105 | . The random prediction algorithm . takes a training and a test set | generates the prediction by selecting random elements from the training set | We split the dataset to training and test sets, by taking the first 2/3 and last 1/3 of the data respectively. . train, test = volcano.iloc[: volcano.shape[0] * 2 // 3, :], volcano.iloc[volcano.shape[0] * 2 // 3 :, :] . A small check that the split did not leave out an entry or did not result in an overlap. . assert train.index[-1] + 1 == test.index[0] . We plot the training and the test sets, respectively. . alt.Chart(train.reset_index()).mark_line().encode(alt.Y(&#39;values&#39;), alt.X(&#39;index:Q&#39;), alt.Tooltip(&#39;values&#39;)).interactive().properties(title=&#39;Train data&#39;) . alt.Chart(test.reset_index()).mark_line().encode(alt.Y(&#39;values&#39;), alt.X(&#39;index:Q&#39;), alt.Tooltip(&#39;values&#39;)).interactive().properties(title=&#39;Test data&#39;) . First, we generate the random predictions with replacement. That is, the same values can occur more than once. . predictions = np.random.choice(train[&#39;values&#39;], size=test.shape[0], replace=True) predictions . array([166, 179, 96, ..., 105, 157, 95]) . We calculate the error terms . errors = test[&#39;values&#39;] - predictions errors . 3538 -16 3539 -29 3540 54 3541 54 3542 56 .. 5302 -8 5303 2 5304 -7 5305 -60 5306 2 Name: values, Length: 1769, dtype: int64 . We calculate the root mean squared error of the predictions. . def calculate_rmse(observed, predicted): return np.sqrt(sum((observed - predicted) ** 2)/len(observed)) . The RMSE is around 34.59 which stands for about 1.34 STD. . rmse = calculate_rmse(test[&#39;values&#39;], predictions) rmse . 34.93380641982711 . For reference, if we would simply use the simple mean, we would get an RMSE of 18.42, . calculate_rmse(test[&#39;values&#39;], test[&#39;values&#39;].mean()) . 18.420942507684327 . Let&#39;s plot the results . to_compare = pd.concat( [ test[&#39;values&#39;].rename(&#39;observed&#39;).reset_index(drop=True), pd.Series(predictions).rename(&#39;predictions&#39;).reset_index(drop=True) ], axis=1 ).stack().reset_index().rename(columns={&#39;level_1&#39;: &#39;status&#39;, &#39;level_0&#39;: &#39;index&#39;, 0: &#39;values&#39;}) to_compare . index status values . 0 0 | observed | 150 | . 1 0 | predictions | 166 | . 2 1 | observed | 150 | . 3 1 | predictions | 179 | . 4 2 | observed | 150 | . ... ... | ... | ... | . 3533 1766 | predictions | 105 | . 3534 1767 | observed | 97 | . 3535 1767 | predictions | 157 | . 3536 1768 | observed | 97 | . 3537 1768 | predictions | 95 | . 3538 rows × 3 columns . alt.Chart(to_compare).mark_line().encode( alt.X(&#39;index:Q&#39;), alt.Y(&#39;values&#39;), alt.Color(&#39;status&#39;), alt.OpacityValue(0.7) ).properties(width=1200, title=&#39;Predictions with replacement&#39;) . We put the steps into a function and rerun the prediction without replacement. . def predict_randomly(train, test, replace=True): predictions = np.random.choice(train, size=test.shape[0], replace=True) errors = test - predictions rmse = calculate_rmse(test, predictions) return predictions, rmse . predictions, rmse = predict_randomly(train[&#39;values&#39;], test[&#39;values&#39;], replace=False) rmse . 35.90392934559341 . Finally, we also put the plotting steps into a function . def plot_predictions(observed, predicted): to_compare = pd.concat( [observed.rename(&#39;observed&#39;).reset_index(drop=True), pd.Series(predicted).rename(&#39;predictions&#39;).reset_index(drop=True)], axis=1 ).stack().reset_index().rename(columns={&#39;level_1&#39;: &#39;status&#39;, &#39;level_0&#39;: &#39;index&#39;, 0: &#39;values&#39;}) chart = alt.Chart(to_compare).mark_line().encode( alt.X(&#39;index:Q&#39;), alt.Y(&#39;values&#39;), alt.Color(&#39;status&#39;), alt.OpacityValue(0.7) ).properties(width=1200) chart.display() . The predictions without replacement . plot_predictions(test[&#39;values&#39;], predictions) .",
            "url": "andrasnovoszath.com/2020/09/23/random_predictions.html",
            "relUrl": "/2020/09/23/random_predictions.html",
            "date": " • Sep 23, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "How does data standardization work?",
            "content": "With standardization, we transform the data so its distribution&#39;s center will become 0 and its standard deviation will become 1. . import altair as alt from vega_datasets import data . For this demo, we use the sp500 price index. . sp500 = data(&#39;sp500&#39;) sp500.head() . date price . 0 2000-01-01 | 1394.46 | . 1 2000-02-01 | 1366.42 | . 2 2000-03-01 | 1498.58 | . 3 2000-04-01 | 1452.43 | . 4 2000-05-01 | 1420.60 | . alt.Chart(sp500).mark_line(point=True, strokeWidth=1).encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price&#39;), alt.Tooltip(&#39;price&#39;) ).configure_point(size=10) . The original distribution of prices. . alt.Chart(sp500).mark_bar().encode( x=alt.X(&#39;price:Q&#39;, bin=alt.BinParams(maxbins=20)), y=alt.Y(&#39;count()&#39;), tooltip=alt.Tooltip([&#39;count()&#39;]) ) . We get the standardized value by dividing the values&#39; difference from the mean by the standard deviation. . $ text{standardized value} = frac { text{value} - text{mean}} { text{standard deviation}} $ . The mean and standard deviation of the prices . print(f&quot;&quot;&quot; mean: {sp500[&#39;price&#39;].mean():.2f} std: {sp500[&#39;price&#39;].std():.2f} &quot;&quot;&quot;) . mean: 1184.43 std: 195.41 . We standardize the data set. . standardized_prices = sp500.copy() standardized_prices[&#39;price&#39;] = (sp500[&#39;price&#39;] - sp500[&#39;price&#39;].mean()) / sp500[&#39;price&#39;].std() standardized_prices . date price . 0 2000-01-01 | 1.074812 | . 1 2000-02-01 | 0.931317 | . 2 2000-03-01 | 1.607646 | . 3 2000-04-01 | 1.371473 | . 4 2000-05-01 | 1.208583 | . ... ... | ... | . 118 2009-11-01 | -0.454452 | . 119 2009-12-01 | -0.354814 | . 120 2010-01-01 | -0.565809 | . 121 2010-02-01 | -0.409111 | . 122 2010-03-01 | -0.225086 | . 123 rows × 2 columns . alt.Chart(standardized_prices).mark_bar().encode( x=alt.X(&#39;price:Q&#39;, bin=alt.BinParams(maxbins=20)), y=alt.Y(&#39;count()&#39;), tooltip=alt.Tooltip([&#39;count()&#39;]) ) . The new mean and standard deviation . print(f&quot;&quot;&quot; mean: {standardized_prices[&#39;price&#39;].mean():.2f} std: {standardized_prices[&#39;price&#39;].std():.2f} &quot;&quot;&quot;) . mean: -0.00 std: 1.00 . When plotted on the dates, it provides a similar graph as before, but now with a value domain around 0. . alt.Chart(standardized_prices).mark_line(point=True, strokeWidth=1).encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price&#39;), alt.Tooltip(&#39;price&#39;) ).configure_point(size=10) .",
            "url": "andrasnovoszath.com/2020/09/22/standardization.html",
            "relUrl": "/2020/09/22/standardization.html",
            "date": " • Sep 22, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "Data scale normalization",
            "content": "Normalization is a common technique used in machine learning to render the scales of different magnitudes to a common range between 0 and 1. . Here we demonstrate how this is done with pandas and altair. . Original inspiration: (Jason Brownlee: Machine Learning Algorithms from Scratch)[https://machinelearningmastery.com/machine-learning-algorithms-from-scratch/] . import altair as alt # alt.renderers.enable(&#39;default&#39;) alt.renderers . RendererRegistry(active=&#39;default&#39;, registered=[&#39;colab&#39;, &#39;default&#39;, &#39;html&#39;, &#39;json&#39;, &#39;jupyterlab&#39;, &#39;kaggle&#39;, &#39;mimetype&#39;, &#39;notebook&#39;, &#39;nteract&#39;, &#39;png&#39;, &#39;svg&#39;, &#39;zeppelin&#39;]) . from vega_datasets import data . We use the Gapminder health and income dataset . health_income = data(&#39;gapminder-health-income&#39;) health_income.head() . country income health population . 0 Afghanistan | 1925 | 57.63 | 32526562 | . 1 Albania | 10620 | 76.00 | 2896679 | . 2 Algeria | 13434 | 76.50 | 39666519 | . 3 Andorra | 46577 | 84.10 | 70473 | . 4 Angola | 7615 | 61.00 | 25021974 | . income_domain = [health_income[&#39;income&#39;].min(), health_income[&#39;income&#39;].max()] health_domain = [health_income[&#39;health&#39;].min(), health_income[&#39;health&#39;].max()] alt.Chart(health_income).mark_point().encode( alt.X(&#39;income:Q&#39;, scale=alt.Scale(domain=income_domain)), alt.Y(&#39;health:Q&#39;, scale=alt.Scale(domain=health_domain)), alt.Size(&#39;population:Q&#39;), alt.Tooltip(&#39;country:N&#39;) ).properties(height=600, width=800) . The process: . Take the values&#39; difference from the smallest one; | Take the value range, that is, the difference between the largest and smallest values; | Divide the reduced values with the range. | $ text {scaled value} = frac{value - min} {max - min} $ . The first step ensures that the smallest value will become 0. Dividing the reduced values by the range &#39;compresses&#39; the values so the new maximum becomes 1. . quantitative_columns = [&#39;income&#39;, &#39;health&#39;, &#39;population&#39;] . The original minimum and maximum values . health_income.loc[health_income[quantitative_columns].idxmin(), :] . country income health population . 32 Central African Republic | 599 | 53.8 | 4900274 | . 93 Lesotho | 2598 | 48.5 | 2135022 | . 105 Marshall Islands | 3661 | 65.1 | 52993 | . minimums = health_income[quantitative_columns].min() minimums . income 599.0 health 48.5 population 52993.0 dtype: float64 . health_income.loc[health_income[quantitative_columns].idxmax(), :] . country income health population . 134 Qatar | 132877 | 82.0 | 2235355 | . 3 Andorra | 46577 | 84.1 | 70473 | . 35 China | 13334 | 76.9 | 1376048943 | . maximums = health_income[quantitative_columns].max() maximums . income 1.328770e+05 health 8.410000e+01 population 1.376049e+09 dtype: float64 . Difference of values from the column minimum . health_income[quantitative_columns] - minimums . income health population . 0 1326.0 | 9.13 | 32473569.0 | . 1 10021.0 | 27.50 | 2843686.0 | . 2 12835.0 | 28.00 | 39613526.0 | . 3 45978.0 | 35.60 | 17480.0 | . 4 7016.0 | 12.50 | 24968981.0 | . ... ... | ... | ... | . 182 5024.0 | 28.00 | 93394608.0 | . 183 3720.0 | 26.70 | 4615473.0 | . 184 3288.0 | 19.10 | 26779222.0 | . 185 3435.0 | 10.46 | 16158774.0 | . 186 1202.0 | 11.51 | 15549758.0 | . 187 rows × 3 columns . Value ranges: the difference between the maximum and the minimum . maximums - minimums . income 1.322780e+05 health 3.560000e+01 population 1.375996e+09 dtype: float64 . Let&#39;s normalize the dataset . def normalize_dataset(dataset, quantitative_columns): dataset = dataset.copy() minimums = dataset[quantitative_columns].min() maximums = dataset[quantitative_columns].max() dataset[quantitative_columns] = (dataset[quantitative_columns] - minimums) / (maximums - minimums) return dataset . normalized_health_income = normalize_dataset(health_income, quantitative_columns) normalized_health_income . country income health population . 0 Afghanistan | 0.010024 | 0.256461 | 0.023600 | . 1 Albania | 0.075757 | 0.772472 | 0.002067 | . 2 Algeria | 0.097030 | 0.786517 | 0.028789 | . 3 Andorra | 0.347586 | 1.000000 | 0.000013 | . 4 Angola | 0.053040 | 0.351124 | 0.018146 | . ... ... | ... | ... | ... | . 182 Vietnam | 0.037981 | 0.786517 | 0.067874 | . 183 West Bank and Gaza | 0.028123 | 0.750000 | 0.003354 | . 184 Yemen | 0.024857 | 0.536517 | 0.019462 | . 185 Zambia | 0.025968 | 0.293820 | 0.011743 | . 186 Zimbabwe | 0.009087 | 0.323315 | 0.011301 | . 187 rows × 4 columns . The new minimum and maximum values . normalized_health_income.loc[normalized_health_income[quantitative_columns].idxmin(), :] . country income health population . 32 Central African Republic | 0.000000 | 0.148876 | 0.003523 | . 93 Lesotho | 0.015112 | 0.000000 | 0.001513 | . 105 Marshall Islands | 0.023148 | 0.466292 | 0.000000 | . normalized_health_income.loc[normalized_health_income[quantitative_columns].idxmax(), :] . country income health population . 134 Qatar | 1.000000 | 0.941011 | 0.001586 | . 3 Andorra | 0.347586 | 1.000000 | 0.000013 | . 35 China | 0.096275 | 0.797753 | 1.000000 | . Plotting the normalized data, we got the same results, but with the income, health, and population scales all normalized to the [0, 1] range. . Maximum values . alt.Chart(normalized_health_income).mark_point().encode( alt.X(&#39;income:Q&#39;,), alt.Y(&#39;health:Q&#39;), alt.Size(&#39;population:Q&#39;), alt.Tooltip(&#39;country:N&#39;) ).properties(height=600, width=800) .",
            "url": "andrasnovoszath.com/2020/09/21/normalize-scale.html",
            "relUrl": "/2020/09/21/normalize-scale.html",
            "date": " • Sep 21, 2020"
        }
        
    
  
    
        ,"post5": {
            "title": "How to categorize your notes?",
            "content": "I take notes . I take lots of notes. Many of them are ideas to research, explore, and think about, while others are rather tasks or possible projects. . As I collect them continuously, their number grows. After a while, it becomes hard to look through them and prioritize for action. . The obvious way to do this, of course, is to categorize them under topics or themes. . Concept hierarchies . This produces a hierarchical tree-like structure. There a few problems with this: . There always be some idea or task which does not belong to a single category alone but under multiple ones. | As the number of notes and, therefore, the number of categories and sub-categories becomes high, you need to dig very deep to find some notes. If the hierarchies are obvious, this is not really a problem. However, as the number of hierarchy levels and branches grow, the previous issue (i.e., notes belonging under multiple categories) become even more prominent. | . Concept drift . There is also the problem of concept drift. While we refer to a particular thing or category with a specific term, later on, we start to refer to it with a related but different one. Similarly, we do the same with categories. . This shift can happen for multiple reasons, but the most obvious ones are terminological shifts or our changing ‘use’ of the original idea or category. . Concept drift, deep hierarchies, and the overlapping category boundaries can lead to the ‘forgetting’ of notes. They can also lead to making multiple, ‘almost similar’ but ‘slightly different’ versions of them at different places of the hierarchy. . And, even whether the relationship between two ideas should be hierarchical or not also can change from use case to use case. . Chaos everywhere . And note-taking is just the most obvious example for me. I had the same experience with software development (both as writing code and managing versions), technical writing, academic research, work organization, file organization, etc. .",
            "url": "andrasnovoszath.com/note-taking/categorization/hierarchies/concept%20drift/chaos%20&%20order/2020/09/20/note-categorization-pain.html",
            "relUrl": "/note-taking/categorization/hierarchies/concept%20drift/chaos%20&%20order/2020/09/20/note-categorization-pain.html",
            "date": " • Sep 20, 2020"
        }
        
    
  
    
        ,"post6": {
            "title": "The sum of stock trading",
            "content": "I thought further about my previous position on whether stock trading is a zero-sum game. Now I see a bigger space for it not to be one. . Trading growth stocks . The situation I am thinking about is an argument I found during my google search. . Very simply, one buys a stock at one point in time and sells it to someone else in another. . Let’s take the ‘classical’ position on stocks and consider it merely an investment asset. In this case, we can argue that stock buyers lent money to a company in exchange for a part of its profits. They sell the stocks when it wants to get their money back, say for their retirement. . Here the value comes for the original owner in that the price will be higher when they sell. . This is the same hope the sellers have for the time when they want to realize their investment. That is, when the original buyers sell the stock, the new buyers can get a ‘fair’ price if, in that given time, the stock is still promising some price growth. . This seems more positive-sum for me. However, this view works if we strip away the ‘speculative’ (?), arbitrage-seeking trading aspect of security trading. In this view, stock trading seems to resemble the trade of a good tool one does not need anymore. . Remaining questions . There are, of course, a number of issues still unclear for me. . The first is whether ‘speculation’ or ‘arbitrage seeking’ inevitably leads to a zero-sum outcome? Also, can we separate ‘straightforward’ investment from these activities? . The next is about the buying price: when is the price too high/low? . The buyer could always want to ask for a higher price. Aren’t selling a stock at a too high price basically strips away the seller’s possible gains? . On the other hand, one might argue that, if the price reflects the market price, there is less space for unfairness (or at least not directly between the two parties). . Third, there is the question of whether a company can grow infinitely or trading stock is always based on this false promise. . Finally, how do we calculate the outcomes when the ‘gain’, in big part, comes from luck and decisions made years before their realization. .",
            "url": "andrasnovoszath.com/stock%20trading/zero-sum/finance/valuation/2020/09/19/stock-trading-sum.html",
            "relUrl": "/stock%20trading/zero-sum/finance/valuation/2020/09/19/stock-trading-sum.html",
            "date": " • Sep 19, 2020"
        }
        
    
  
    
        ,"post7": {
            "title": "How to build up a writing habit?",
            "content": "If you tend to analyze a lot but want to build up a consistent writing habit, the following tips might help. . Take notes . Have a notepad or a very light-weight text editor/app that you can open at any time and dump your ideas effortlessly. . Find out what interests you . Review your notes, todo lists, emails, past writing, or any other traces about yourself. Look for frequent themes about which you feel strongly and, even better, about which you developed some depth in skill or knowledge. . Prioritize your themes . Take these themes and group or merge the closely related ones. Then, prioritize them based on their importance. . When you have them ranked, drop the last 80% and focus on the remaining 20%. If you have too too many themes remained (e.g., more than 10), drop even more. . Brainstorm post ideas . For each remaining theme, write as many post ideas as possible, but a minimum of ten. Do not bother with how good or viable they are. The important thing is to have a high number of them. . Rotate your themes . Now you have a list of topics organized under themes. Outline a rotating schedule where you order the themes into a sequence, so, in each day, you write about a topic belonging to the next theme. . Write . Each day, you take the topics belonging under that day’s theme and pick one randomly. Write about it impatiently, badly, and experimentally for a minimum 5 minutes. . When you find that you could write about multiple things but you do not know which to pick, pick the first one and move the second into your ideas under the same theme. . When you run out of time, you will find yourself wanting to write more about something. Move it as a new topic under the same theme. . Publish . Publish the writing to somewhere, where nobody sees is. For example, create an account on Medium or any blogging platform under some alias. . Repeat . When you arrive at the end of the sequence, start again. If you could fill out all your 5 minutes for each day for a week, add an extra 5 minutes for your writing time. .",
            "url": "andrasnovoszath.com/writing/habit/2020/09/18/writing-habit.html",
            "relUrl": "/writing/habit/2020/09/18/writing-habit.html",
            "date": " • Sep 18, 2020"
        }
        
    
  
    
        ,"post8": {
            "title": "Is finance a zero-sum activity?",
            "content": "I recently thought about finance and whether it is a purely zero-sum game or has some value-add to it. I would like to know this to see if it is worthy of support either as a personal wealth source or as a social institution. . I did not have time to read academic papers, so I simply did a google search (1-2 hours) and reviewed the top results. . What I found . Just based on this short research, I barely found any substantial arguments against the zero-sum claim. . What counterarguments I found were acknowledged that some forms, like forex or short-term trading, are zero-sum. They argued that, compared to the former types, ‘real’ stock trading can bring about benefits for both parties. Most of these were conflating terminology and concepts (like ‘luck’, or ‘value growth’). . There are probably more substantial arguments in academic literature. Also, probably, there is the additional question of the social usefulness of financial markets for overall capital allocation, but that’s a separate thing. . How I see it now . Of course, as I started to dive into the topic, I quickly realized that is is a bit more complex than I thought. . So let’s break up the question to the following cases: . Pure commercial relationship | Trading of goods | Finance | . Pure commercial transaction . When two people get into a commercial relationship, one is a ‘seller’, and the other is a ‘buyer’. Regarding the price, they have opposite aims. One’s income is another’s cost. . So, in this sense, every commercial transaction is zero-sum. . Trade of ‘comparatively advantageous’ goods . When the seller gives away something they cannot use, but the buyer can, the exchange might benefit both. There is still the issue of the price for which they exchange the good, but the buyer’s benefit from possessing the good might surpass its price. . So, here we can imagine a non-zero-sum relationship. . Finance . Most of the people who trade and invest mostly want to get the most payoff for their buck. So this primarily seems zero-sum. . The mutual benefit might come about when their relationship to their asset is more complex than just trying to get the highest price difference. . Conclusion . Again, I can see that I could think and write about this topic for a long time. Some ideas . How do we measure and compare ‘comparatively advantages’? | What are examples of not purely ‘buy low, sell high’ uses of finance? | How to measure the utility ‘ratio’ between price gain/loss and usability of the result of a buy event? | Is the social benefit of efficient capital distribution via financial markets a fluke, or has empirical evidence behind it? | Can we replace financial markets with an AI and delegate human creativity to more productive things? | .",
            "url": "andrasnovoszath.com/finance/zero-sum/valuation/2020/09/17/finance-zero-sum.html",
            "relUrl": "/finance/zero-sum/valuation/2020/09/17/finance-zero-sum.html",
            "date": " • Sep 17, 2020"
        }
        
    
  
    
        ,"post9": {
            "title": "The specific you",
            "content": "When you are thinking about what business area to pursue, it is useful to focus on your specific situation and circumstance. . Why is it useful to find a specialization? . When you do not look at your own specificities, you basically do not use knowledge and skills that others could use but cannot. . You have your history, your dis/abilities, your interests, and your own conceptual framing. Most of it might be useless, especially regarding all the people in the world. . However, there are a few problems which maybe you are the only one who can solve it. . This is not about ‘finding your passion’ but rather to find out your skills, knowledge, approach, etc. These are actually quite tangible. . How to know your specificities? . To see yourself better, write down your characteristics in as many dimensions as possible. . Here are some dimensions which you can look for: . Knowledge, | Skills, | Experience, | Interest. | . One thing you can do is to create a spreadsheet and write as many of these things as possible. Then, value them on a scale (e.g., 1-5) showing how specific these traits and dimensions are to you, especially compared to others. .",
            "url": "andrasnovoszath.com/self-knowledge/specialization/2020/09/16/specific-you.html",
            "relUrl": "/self-knowledge/specialization/2020/09/16/specific-you.html",
            "date": " • Sep 16, 2020"
        }
        
    
  
    
        ,"post10": {
            "title": "Valuation technology experimentation",
            "content": "It might be useful and promising to experiment with technologies of valuation. . How does valuation happen now, and how does technology play a role in it? . Current technologies of valuation are in place because of different reasons. However, implementers rarely take into account the way the given technology plays a role in valuing things. . Examples of the use of technology in valuation and some trends . Money is arguably the most prominent example. Probably this is not independent of that most of us still do not really understand all its effects. . Other examples are in accountancy, trade, work management, economic metrics, environmental management. . An obvious note is that, of course, other, not strictly technological components, like language, moral values, physical factors, etc., also have critial roles. . Think about and experiment with different technologies to generate new valuation situations . Accordingly, it might be beneficial to try out technologies with explicit valuation functions in mind. . For the most part, and especially in early stages, this is concept-driven (i.e., we are looking for a particular effect to emerge). However, an uncertain but promising benefit would be discovering ‘unthought’ features due to things’ specific and idiosyncratic character. . The process of valuation technology experiments and its main challenges . Most of the time, we cannot implement large scale infrastructures for the sake of experimentation. . Instead, we can try other paths: . Implementation of a small-scale but very focused version | Scenario building | Simulation | Game development | The empirical analysis of similar processes | The empirical analysis of current technologies’ edge cases | Speculation | Incremental implementation | . Current action possibilities and possible future directions . As, currently, I this all is very high level and conceptual, and I am also mostly interested in discovery activities: . Collecting ideas | Conceptual exploration of background ideas | Case study analysis | Code experimentations | . In the future, I might be able to focus it down and branch out toward more serious development work. .",
            "url": "andrasnovoszath.com/technology/valuation/experimentation/2020/09/15/technnology_valuation_experimentation.html",
            "relUrl": "/technology/valuation/experimentation/2020/09/15/technnology_valuation_experimentation.html",
            "date": " • Sep 15, 2020"
        }
        
    
  
    
        ,"post11": {
            "title": "Note-taking obsessions",
            "content": "I have an obsession with the process of note-taking. . I often write down my ideas in the form of notes or tasks. I do this through multiple channels. Then I spend a considerable amount of time organizing them. . One common frustration with organizing notes is that I can do this only along one hierarchical dimension. . Perhaps one reason I like to code is that you can implement hierarchies along multiple dimensions more easily, although not always. . Because of the single hierarchy, some notes tend to become forgotten as I put them in some deep end part of a category branch. This then requires me to constantly re- and review my notes and to recategorize and reorganize them. . I have looked into multiple note-taking software, but there is not really a perfect solution for this. . One thing I found helpful is to require only a single function from a particular tool. So, for instance, I would use one particular tool or process for note-taking, then another for categorization, and perhaps a further one for review or note management. . This, however, needs that the different tools speak to each other relatively well. Ready-made solutions often do not allow it, so I found myself moving toward text-based tools. However, you need to utilize some basic scripting to make them work. .",
            "url": "andrasnovoszath.com/note-taking/2020/09/14/note-taking-obsessions.html",
            "relUrl": "/note-taking/2020/09/14/note-taking-obsessions.html",
            "date": " • Sep 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About",
          "content": "Online mental dump to put things out. Currently, I practice writing posts within 30 minutes. Still getting there… .",
          "url": "andrasnovoszath.com/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

}