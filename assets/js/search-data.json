{
  
    
        "post0": {
            "title": "Setting up AWS Cloud for InfluxDB",
            "content": "Here are the preparation steps you need to do if you would like to use AWS Cloudwatcher with InfluxDB. . Based on this post . Setting up an EC2 virtual server . Create an AWS account | Select the appropriate region: | Search for “EC2” in the Find Services search box and select EC2 (Virtual Servers in the Cloud) (Please not that you have to provide credit card information and it may take up to 24 hours for amazon to activate): | Select Key Pairs and the create a key pair: | Download the keys. | Make the key readable only by the owner: chmod 400 name-of-the-key.pem | Open IAM from the Services menu: | Create a user and grant permissions to it: Select Add user: | Name the user and add access rights: | On the Set Permissions screen click on Add user to group and Create a group: | Define a name and add a policy for the new group: | At the end of the process you will see your access details: | Note that AWS does not save your secret after you close your final screen. On the other hand, you will always be able to create a new key for your users. . | Install the AWS CLI on your system. . On a linux machine, you can do this as here: . curl &quot;https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip&quot; -o &quot;awscliv2.zip&quot; unzip awscliv2.zip sudo ./aws/install . Verify your version: . aws --version . For further instructions see here. . | Also create a folder where you will store your keys and your yaml configuration. . mkdir ~/Projects/influxdb_aws_cloudformation cd ~/Projects/influxdb_aws_cloudformation mv ~/Downloads/name-of-the-key.pem . touch config.yml . | Configure AWS with aws configure. This is where you need to add your Access Key ID, your Secret Access Key, and the region name (e.g. eu-central-1). You can skip the output format. | Amazon Machine Image . Open the Amazon AWS Marketplace. . Find a public image . . Defining resources . Resources Logical ID Instance properties AMI (Amazon Machine Image) ID . Resources: Appnode: Type: AWS::EC2::Instance Properties: InstanceType: t2.nano ImageId: ami-06a719e5f8e22c33b # The AMI instance ID Keyname: InfluxDB_AWS_example # The name of your key pair SecurityGroups: - !Ref AppnodeSecurityGroup # Reference the security group defined below . You need to define a security group. Security groups act as virtual firewalls for your incoming/outgoing traffic with the help of rules. You can read more about them here. Here, we define an inbound security group. For particular rules, see here. . We define the security group and link it to our app node definition with the Ref function. . # Define the security group AppnodeSecurityGroup: Type: AWS::EC2::SecurityGroup Properties: GroupDescription: SSH enabled app nodes # Inbound security rule # Expose the HTTP port 80 with tcp to inbound traffic from andy Ipv4 addresses SecurityGroupIngress: - IpProtocol: tcp FromPort: &#39;80&#39; ToPort: &#39;80&#39; CidrIp: 0.0.0.0/0 . Define Bash script to install docker and influxdb on the image. . The full yaml: . Resources: AppNode: Type: AWS::EC2::Instance Properties: InstanceType: t2.micro ImageId: ami-03d85bfa79ad10274 KeyName: InfluxDB_AWS_example SecurityGroups: - !Ref AppNodeSG UserData: !Base64 | #!/bin/bash apt-get update -qq apt-get install -y apt-transport-https ca-certificates apt-key adv apt-key adv --keyserver hkp://p80.pool.sks-keyservers.net:80 --recv-keys 58118E89F3A912897C070ADBF76221572C52609D apt-get update -qq apt-get purge lxc-docker || true curl -fsSL https://download.docker.com/linux/ubuntu/gpg | apt-key add - add-apt-repository &quot;deb [arch=amd64] https://download.docker.com/linux/ubuntu bionic stable&quot; apt-get -y install linux-image-extra-$(uname -r) linux-image-extra-virtual apt-get -y install docker-ce usermod -aG docker unbuntu docker image pull quay.io/influxdb/influxdb:2.0.0-alpha docker container run -p 80:9999 quay.io/influxdb/influxdb:2.0.0-alpha wget https://dl.influxdata.com/telegraf/releases/telegraf_1.10.4-1_amd64.deb sudo dpkg -i telegraf_1.10.4-1_amd64.deb AppNodeSG: Type: AWS::EC2::SecurityGroup Properties: GroupDescription: for the app nodes that allow ssh SecurityGroupIngress: - IpProtocol: tcp FromPort: &#39;80&#39; ToPort: &#39;80&#39; CidrIp: 0.0.0.0/0 - IpProtocol: tcp FromPort: &#39;22&#39; ToPort: &#39;22&#39; CidrIp: 0.0.0.0/0 . Create stack . aws cloudformation create-stack --stack-name influxdb-trial-stack --region eu-central-1 --template-body file://$PWD/stack.yaml . . Create a connection to our instance: . ssh -v -i InfluxDB_AWS_example.pem ubuntu@ec2-3-122-XXX-XX.eu-central-1.compute.amazonaws.com . Install InfluxDB . Maybe this: https://websiteforstudents.com/how-to-install-influxdb-on-ubuntu-18-04-16-04/ . wget https://dl.influxdata.com/influxdb/releases/influxdb_2.0.2_amd64.deb sudo dpkg -i influxdb_2.0.2_amd64.deb . Setup InfluxDB configuration settings . influx setup . Here you need to add a username, a password, and name your organization and your primary bucket. You can skip on the retention period question. . Run influxdb service . Start the influxdb service. . sudo systemctl start influxdb.service . https://devconnected.com/how-to-install-influxdb-on-ubuntu-debian-in-2019/#II_Installing_InfluxDB_20 . Install telegraf . We need to install telegraf onto our instance. . On an Ubuntu instance this will look like this: . Adding the repository: . wget -qO- https://repos.influxdata.com/influxdb.key | sudo apt-key add - source /etc/lsb-release echo &quot;deb https://repos.influxdata.com/${DISTRIB_ID,,} ${DISTRIB_CODENAME} stable&quot; | sudo tee /etc/apt/sources.list.d/influxdb.list . Installing telegraf: . sudo apt-get update &amp;&amp; sudo apt-get install telegraf . j . telegraf --test . Create an example data collection setting config and test it . telegraf -sample-config --input-filter cpu:mem --output-filter influxdb &gt; telegraf_test.conf telegraf --config telegraf.conf --test . https://docs.influxdata.com/telegraf/v1.16/guides/using_http/ . For other systems and futher information see this link. . Install Chronograph . wget https://dl.influxdata.com/chronograf/releases/chronograf_1.8.8_amd64.deb sudo dpkg -i chronograf_1.8.8_amd64.deb . Configure Telegraf . https://docs.influxdata.com/telegraf/v1.16/guides/using_http/ | . On Ubuntu instance the config file path is at /etc/telegraf/telegraf.conf. . https://docs.influxdata.com/telegraf/v1.15/administration/configuration/ | https://docs.influxdata.com/telegraf/v1.16/introduction/getting-started/ | . For further options and for configuration on other systems, see the documentation. . Starting the Telegraph service: . sudo systemctl start telegraf sudo systemctl status telegraf . Instance . . . . . Continue to Configuration: | | | | | | | .",
            "url": "https://andrasnovoszath.com/influxdb/aws/2020/11/18/indluxdb-aws-cloud-setup.html",
            "relUrl": "/influxdb/aws/2020/11/18/indluxdb-aws-cloud-setup.html",
            "date": " • Nov 18, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Grouping data with Flux in InfluxDB",
            "content": "Grouping happens based on group keys which effectilve are the name of columns used for the grouping. A group then consists of rows having identical values in the group key columns. . Example . Let’s consider the http api request duration data. . from(bucket: &quot;trial_bucket&quot;) |&gt; range(start: -15m) |&gt; filter(fn: (r) =&gt; r._measurement == &quot;http_api_request_duration_seconds&quot;) . In respect to the time columns, it is ‘grouped’, by default, by its ‘start’ and ‘end’ times. . “Default grouping” . “Default grouping as chart” . Grouping . Now, with the following snippet, we group data along the _measurement and _field columns. . from(bucket: &quot;trial_bucket&quot;) |&gt; range(start: -15m) |&gt; filter(fn: (r) =&gt; r._measurement == &quot;http_api_request_duration_seconds&quot;) |&gt; group(columns: [&quot;_measurement&quot;, &quot;_field&quot;]) . This will change the group keys: . . And will also shape the charts: . . As observations of the same time now belong to the same data, plotting them as a line chart on the time axis produces vertical lines. . Aggregate and ungroup . For the next step let’s calculate the mean values of the request durations. . from(bucket: &quot;trial_bucket&quot;) |&gt; range(start: -15m) |&gt; filter(fn: (r) =&gt; r._measurement == &quot;http_api_request_duration_seconds&quot;) |&gt; group(columns: [&quot;_time&quot;]) |&gt; mean() |&gt; group() . While we calucalted the mean values for each timestamp, this also resulted in separate tables consisting of a single value. We can use the group() function to ungroup data, that is, to merge all tables into a single table. . This will generate the following chart. . . We can produce the same result by explicitly turning off grouping on specific columns with the mode parameter. . mode . The group() function also accepts a mode parameter with the following values: . by: Group by the columns defined in the columns parameter. | except: Group by the columns not defined in the columns parameter. | . Accordingly, we can modify the above code to get the same result: . from(bucket: &quot;trial_bucket&quot;) |&gt; range(start: -15m) |&gt; filter(fn: (r) =&gt; r._measurement == &quot;http_api_request_duration_seconds&quot;) |&gt; group(columns: [&quot;_time&quot;]) |&gt; mean() |&gt; group(columns: [&quot;_time&quot;, &quot;_value&quot;], mode: &quot;except&quot;) .",
            "url": "https://andrasnovoszath.com/influxdb/flux/time-series/2020/11/17/influxdb-flux-group.html",
            "relUrl": "/influxdb/flux/time-series/2020/11/17/influxdb-flux-group.html",
            "date": " • Nov 17, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Filtering with Flux in InfluxDB",
            "content": "The filter() function behaves similarly to the WHERE command in SQL. . from(bucket: &quot;trial_bucket&quot;) |&gt; range(start: v.timeRangeStart, stop: v.timeRangeStop) |&gt; filter(fn: (r) =&gt; r[&quot;_measurement&quot;] == &quot;service_org_duration&quot;) . The fn parameter expects a predicate function, that is an anonymous function producing a boolean (true/false) output. The filtering will include rows for which fn returns true based on the expression. | The r argument of the fn parameter stands for the rows. | .",
            "url": "https://andrasnovoszath.com/influxdb/flux/filtering/querying/2020/11/15/influxdb-flux-filter.html",
            "relUrl": "/influxdb/flux/filtering/querying/2020/11/15/influxdb-flux-filter.html",
            "date": " • Nov 15, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "Writing data to InfluxDB with Telegraf",
            "content": "What is Telegraf? . Telegraf is InfluxData’s time series data collection and reporting agent. Its main functionality is to collect data with the help of its plugins from a wide range of sources and to feed it to an InfluxDB database. It also can ingest data from InfluxDB databases and generate reports on them. . At the time of writing, it has more than 200 integrations, like Amazon ECS and Docker. You can see the full list here. . Using Telegraf with InfluxDB most importantly consists of configuring Telegraf either automatically or manually. Configurations are stored in telegraf.conf. . The following steps assume that you have Telegraf installed and running. If this is not the case, you can follow the instructions here. . Automatic configuration . You can set up a subset of Telegraf plugins automatically (for the rest, do manual configuration). . Steps in the InfluxDB UI: . Open the Data screen from the navigation menu | Select the Telegraf tab | Select Create Configuration | In the Bucket dropdown, choose the bucket to where you want to write the data | Select the plugin groups you would like to use and click Continue. This will open a new window | In the window, add a name and description (optional) | If needed, configure the individual plugins within the group. (A green checkmark denotes plugins with no need of configuration) | Click Create and Verify. This opens a test page. | In the test page, follow the instructions to set up telegraf and click Listen to Data. This should result in a Connection Found! message. | You can close the screen with the Finish button. | For using Telegraf with Window host monitoring, you may need to do some following configuration, see here. .",
            "url": "https://andrasnovoszath.com/influxdb/time-series/database/2020/11/14/influxdb-telegraf.html",
            "relUrl": "/influxdb/time-series/database/2020/11/14/influxdb-telegraf.html",
            "date": " • Nov 14, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "Notifications in InfluxDB",
            "content": "Notification endpoints . Notification endpoints define connections to third party services. . Create notification endpoints . Select the Alerts screen from the navigation menu; | At the top of the Notification Endpoints section click the Create button; | Select a Destination from the drop-down menu; | Add a name and a description (optional); | Add the destination-specific configurations; | Click Create. | For specific details for the different endpoints, see the documentation. . Manage notification endpoints . In the Alerts menu, under the Notification Endpoints section, you can manage notifications. In particular, you can: . Review the list of the existing notification endpoints; | Review their history (eye icon); | Update a notification behavior (by clicking on their name); | Rename them (pen icon next to their name); | Turn them on/off (toggle button); | Add labels to them; | Delete them (trash icon). | . Notification rules . Notification rules generate notifications by triggering notificaton endpoints based on a given condition regarding the outcome of checks. . Create notification rules . Before your first notification rule, you need to have at least one notification endpoint in place. . Select the Alerts screen from the navigation menu. | At the top of the Notification Rules section click the Create button; | In the About section add Name | Schedule: how often you would like the notification to review checks and trigger a message | Offset: the delay relative to the start time of the intervals set under Schedule. | | In the Conditions section, define the rule that will trigger the notification based on check statuses and (optionally) on tag values. | In the Messages section select which endpoint should the rule trigger. | Click Create Notification Rule. | Manage notification rules . In the Alerts screen, under the Notification Rules section, you can manage notifications. In particular, you can: . Review the list of the existing notification rules; | Review their history (eye icon, View History); | Create a clone of an existing notification (clone icon) | Update a notification behavior (by clicking on their name); | Rename them (pen icon next to their name); | Turn them on/off (toggle button); | Add labels to them; | Delete them (trash icon). | .",
            "url": "https://andrasnovoszath.com/influxdb/monitoring/time-series/notification/2020/11/13/influxdb-notifications.html",
            "relUrl": "/influxdb/monitoring/time-series/notification/2020/11/13/influxdb-notifications.html",
            "date": " • Nov 13, 2020"
        }
        
    
  
    
        ,"post5": {
            "title": "Checks in InlfuxDB",
            "content": "You can use InfluxDB to monitor your data and generate notifications reporting its status. . Concepts . Check: generate statuses based on a queries expressing a conditino. It stores them in the _monitoring bucket (under statuses measurement) | Notification rule: check the statuses measurement and, based on its own conditions, triggers the notification endpoint. It also stores notifications in the _monitoring bucket (under notifications measurement) | Notification endpoint: | . Checks . The elements of a check: . Query: Defines what to monitor. | Configuration: defines check interval, messages, and status. | Check types . Threshold check: monitors a value against (high/low) thresholds and assigns one of the following values: CRIT, WARN, INFO, OK. | Deadman check: monitors if a series or group responds within a given amount of time. | . Create a check . Select the Alerts menu in the navigation pane. | Click on Create | Select the check type. | Query . By default, you will find yourself in the Define Query screen. . Select the bucket, the measurement, the field, and the tag sets. | For threshold check: Select an aggregate function (e.g. mean, median, std, etc.). | You can preview the results of the query by clicking on Submit. . After these steps, select the Configure Check screen. . Define the check schedule interval (this updates the aggregate function interval). | Filter the check based on tags (Optional). | Edit the status message template (Optional). | For threshold check: define the thresholds and rules that will trigger the specific statuses. You can also set them with the mouse by moving their respective labels on the preview screen. | For deadman check: specify the duration to check for, the status the lack of series or groups should trigger, and time after which it should stop checking the series / group. | . Finally, name the check at the top of the screen. . Click the green checkmark to save the check. .",
            "url": "https://andrasnovoszath.com/2020/11/12/influxdb-check.html",
            "relUrl": "/2020/11/12/influxdb-check.html",
            "date": " • Nov 12, 2020"
        }
        
    
  
    
        ,"post6": {
            "title": "InfluxDB tasks",
            "content": "Tasks are scheduled Flux queries that allow you to process data with InfluxDB. . Task components . Tasks should have the following components: . Options | Data source | Processing steps | Destination | . Task Options . You can specify options that tasks will use. This makes it easier to reuse and modify your tasks. . option task = { // Task name name: &quot;monitor task&quot;, // Run interval every: 5m, // Cron job cron : &quot;0 * * * * *&quot; // Task start delay offset: 10m, // Maximum number of concurrent tasks concurrency: 2, // Number of tries before getting failed status retry: 5, } . Task data source . You can use the from() function to specify the data you try to get. . data = from(bucket: &quot;status info&quot;) // `-task.every` is a task option passed here |&gt; range(start: -task.every) |&gt; filter(fn: (r) =&gt; r._measurement == &quot;temperature&quot; and r.host == &quot;dataSource&quot; ) . Data processing and destination . You can use the to() function to store the results in another bucket. . data // Process |&gt; aggregateWindow( every: 1h, fn: sum ) // Store results |&gt; to(bucket: &quot;status_log&quot;, org: &quot;logger&quot;) . You can learn more about data processing in the documentation. . Create a task in the UI . In the UI, you can create tasks by the following ways . The Data Explorer screen | The Task screen | Import (in the Task screen) | Create from template (in the Settings screen) | Clone a taks (in the Task screen) | . Data Explorer . Open the Data Explorer | Select Task | Define the task options. | Select a token. | Click Save as Task | Task UI . Open the Task screen | Select Create Task | Click New Task | Define the task options. | Select a token. | Enter your script | Click Save |",
            "url": "https://andrasnovoszath.com/2020/11/05/influxdb-tasks.html",
            "relUrl": "/2020/11/05/influxdb-tasks.html",
            "date": " • Nov 5, 2020"
        }
        
    
  
    
        ,"post7": {
            "title": "Custom dashboard variables in InfluxDB",
            "content": "You can create a new variable in the Data Explorer, in the Settings or import from InfluxDB in JSON. Here we review the steps to create a variable in the Data Explorer . Dashboard variable types . There are three types of dashboard variables. The types refert to the structuring of the variable&#39;s possible values. . Map: stores values as key/value pairs in a comma separated format. The UI presents the keys but works with the values. | Query: uses values from a Flux query&#39;s _value column. (If the values are in another colum, you can first rename that column to _value) | CSV: uses a CSV-formatted list. | . Create variable in Data Explorer . Open the Data Explorer | Build a query with the Query Builder or the Script Editor | Select Save As at the top right corner | Select the Variable tab. | Add a name for the variable and click Save as Variable | Viewing dashboard variables . You can view variables which belong to your organization. . You can review them in two places: . In the Settings menu on the Variables tab | In the Data Explorer, in Script Editor mode under the Variables tab. | Update, export, or delete a dashboard variable . Open the Settings menu in the navigation bar | Select the Variables tab. | For update, click on a variable&#39;s name in the list. To rename, export, or delete, select the appropriate icon in the top right corner of their card. | Variable queries . List buckets . // List buckets in the current organization buckets() // Rename the `name` column to `_value` |&gt; rename(columns: {&quot;name&quot;: &quot;_value&quot;}) // Return a table only with the specific columns |&gt; keep(columns: [&quot;_value&quot;]) . List measurements . // List all measurements in the bucket // Import the schema package import &quot;influxdata/influxdb/schema&quot; // List the measurements of the specified bucket schema.measurements(bucket: &quot;bucket-name&quot;) . List fields in a measurement . List field in the specified bucket + measurement. . // Import the schema package import &quot;influxdata/influxdb/schema&quot; schema.measurementTagValues( bucket: &quot;bucket-name&quot;, measurement: &quot;measurement-name&quot;, tag: &quot;_field&quot; ) . List unique tag values . List the unique values of the host tag. . import &quot;influxdata/influxdb/schema&quot; schame.measurementTagValues(bucket: &quot;bucket-name&quot;, tag: &quot;host&quot;) .",
            "url": "https://andrasnovoszath.com/2020/11/04/influxdb-custom-dashboard-variables.html",
            "relUrl": "/2020/11/04/influxdb-custom-dashboard-variables.html",
            "date": " • Nov 4, 2020"
        }
        
    
  
    
        ,"post8": {
            "title": "Dashboard variables in InfluxDB",
            "content": "Variables . Dashboard variables are useful when you would like to modify dashboard cells without editing the original queries based on which the dashboard is built. You can also use Flux to build queries for dashboard cells. . Dashboard variables can be . Predefined: v.timeRangeStart, v.timeRangeStop, v.windowPeriod | Custom: defined in the user interface | . Quering a variable . Variables are stored in a v standing for a particular dashboard . from(bucket: v.bucket) |&gt; range(start: v.timeRangeStart, stop: v.timeRangeStop) |&gt; filter(fn: (r) =&gt; r._measurement == v._measurement and r.field == v.field) |&gt; aggregateWindow(every: v.windowPeriod, fn: mean) . Adding a variable . Check available variables on the right hand side under the Variables tab | Select the name of the variable you would like to add | Select a value from the Value dropdown | Linking a dashboard with variables . After adding the variable, the dashboard&#39;s url will have the following structure appended to its end &amp;vars[variable_name]=value. This allows you to directly share the dashboard with the variable included with a simple url. . Custom variables . You can create variables for your dashboard in the following ways: . in the Data Explorer | in Settings | via import | . Naming restrictions . Variable names has to . be unique | start with and underscore &#39;_&#39; | they cannot be one of the Flux reserved keywords: and, import, not, return, option, test, empty, in, or, package, and builtin. | .",
            "url": "https://andrasnovoszath.com/2020/11/03/influxdb-dashboard-variables.html",
            "relUrl": "/2020/11/03/influxdb-dashboard-variables.html",
            "date": " • Nov 3, 2020"
        }
        
    
  
    
        ,"post9": {
            "title": "Data visualization and dashboards with InfluxDB",
            "content": "Visualizations . Open Data Explorer | Create a flux query (either with the panel at the bottom, or writing manually in the Script Editor) | Press Submit to run the query. This will generate the graph preview. | You can add further queries with the + button | You an hide/show the visualizations by clicking on the eye icon | . You can choose a different graph type from the left menu. . To see you data in a tablet format, turn on the View Raw Data switch. . You can refresh the dashboard with the refresh button. You can change the auto-refresh interval (by default it is stopped and shows a pause sign). . You can save your query as a dashboard cell by clicking on the Save as . Dashboards . Select the Boards menu item in the navigation menu. | Select Create Dashboard menu in the middle and select New Dashboard | Name the dashboard at the top field (saying Name this dashboard) | You can also import a dashboard from a file or simply pasting a JSON file. . You can also create a new dashboard from templates. . You can also clone your dashboard and change it for another need. . For these operations you need to open a dashboard (e.g. by clicking on its name in the Boards screen). . You can add further data to your dashboard with the Add cell button at the top left corner. | You can add notes to your dashboard with the Add Note buttom at the top left corner. | Change time zone | Manually refresh | Set automatic refresh interval | Select time range | . Further operations . Export: Click on the gear icon and select Export. You can export the dashboard as JSON file | A template Or you can copy the JSON to the clipboard. | . | . Delete | .",
            "url": "https://andrasnovoszath.com/2020/11/02/influxdb-visualization-dashboard.html",
            "relUrl": "/2020/11/02/influxdb-visualization-dashboard.html",
            "date": " • Nov 2, 2020"
        }
        
    
  
    
        ,"post10": {
            "title": "InfluxDB Design principles, Line Protocol, Data Schema",
            "content": "InfluxDB design principles . Time ordering | Restricted delete and update | Priority of read/write over consistency | Schemaless design allowing discontinous data | Priority of datasets over invdividual points. Points do not have IDs besides their timestamp and series. | Assuming duplicates status for identical points. If only field value is different, update with the latest. | . Line protocol . The elements of the line protocol . measurement | tag set | field set: double quote field values | . | timestamp | . # An example myMeasurement,tag1=value1,tag2=value2 fieldKey=&quot;fieldValue&quot; 1556813561098000000 . The line protocol is whitespace sensitive and n newlines separate single points. . measurementName,tagKey=tagValue [whitespace] fieldKey=&quot;fieldValue&quot; [whitespace] 1465839830100400200 . For further information read the documentation . Data Schema . Tabular data schema . You can use it to view raw data and return query results with annotated syntax. . annotation rows: describe column properties (e.g. datatype) | header row: column labels | data rows: the values defined in the header for each point | other columns: they are optional annotation | result | table | . | group keys: use for grouping | .",
            "url": "https://andrasnovoszath.com/2020/11/01/influxdb-design-schema-line-protocol.html",
            "relUrl": "/2020/11/01/influxdb-design-schema-line-protocol.html",
            "date": " • Nov 1, 2020"
        }
        
    
  
    
        ,"post11": {
            "title": "InfluxDB organizing concepts (Organization, Buckets, Data elements)",
            "content": "Organizations . Organizations behave as workspaces. They can possess . a user or a group of users, | dashboards | tasks | buckets | . Buckets . InfluxDB data is stored in buckets. Each bucket belongs to an organization. . Buckets are basically databases with the important addition of retention policies. . A bucket&#39;s retention policy determines the time duration for which its datapoints persist. . Bucket creation . You can create bucket on the UI either in the . Data Explorer, or | in the Load Data menu points. | . Here you need to give a name and also set when would you like to delete the data. . Edit buckets . You can view your existing buckets and edit them under the Data / Load Data menu points. . Delete buckets . You can delet buckets under the Data / Load Data menu points. . Explore bucket data . In the Data / Load Data menu point, you can simply click on a bucket to open it in the Data Explorer . Data elements . _time, timestamp (in epoch nanosecond) | _measurement, string | _field, string: field key | _value, string, floats, integers, or booleans: field value | . Fields are not indexed, and required. Tags are indexed but are not required. . An additional element is a &#39;field set&#39; which are the collection of field key-value pairs having the same timestamp. . Tags: these are simple column-like structure where the column name is the &#39;tag key&#39; and its content are &#39;tag values&#39;. . Tag sets are the tag-key value combinations in the data. . As tags are indexed, they are more performant than fields. For this reason, often, you might want to restructure your tables so your fields will become tags and the other way around. . With tags having many unique values, however, look out for series cardinality as it can eat up your memory! . Series . Series key is a collection of points which have the same . measurement, | tag set, and | field key. | . Their use is a core element in definint the data schema. . Points . A Point consists of a series key, a field value, and a timestamp. .",
            "url": "https://andrasnovoszath.com/2020/10/30/influxdb-concepts-data-bucket.html",
            "relUrl": "/2020/10/30/influxdb-concepts-data-bucket.html",
            "date": " • Oct 30, 2020"
        }
        
    
  
    
        ,"post12": {
            "title": "Querying InfluxDB with Flux",
            "content": "Flux is a functional data scripting language designed specifically for time series data exploration and data processing with InfluxDB. . Key concepts . pipe-forward operator: |&gt;: chain operators together by passing a table to the next step | tables: flux formats data as annotated CSV | table group key: describe the table&#39;s contents as a list of those columns with non-unique values | . Example query . Queries data from the example-bucket bucket | Returns minute average cpu usage (based on tags) from the last hour | . flux from(bucket:&quot;example-bucket&quot;) |&gt; range(start:-1h) |&gt; filter(fn:(r)) =&gt; r._measurement == &quot;cpu&quot; and r.cpu == &quot;cpu-total&quot; ) |&gt; aggregateWindow(every: 1m, fn:mean) |&gt; yield() . Elements of a Flux query . Every flux query requires at least the following parameters . data source: from(bucket:&quot;bucket-name&quot;) | time range: |&gt; range(start: starting timestamp/time preceding &quot;now&quot;, stop: end time preceding &quot;now&quot;) timestamp: 2018-11-05T23:30:00Z | duration preceding now: -15m (always negative) | . | filter: filter(fn(r)), the only parameter is fn which expects an anonymous function (here r) | r: reports or rows | chain operators with and | . | result output: yield() (Flux automatically assumes this, yield() is necessary only to return the results of multiple queries when it also need to name the output tables) | . Exploring data with InfluxDB UI&#39;s Data Explorer . The most straightforward way to explore InfluxDB is to use the UI&#39;s Data Explorer . Here we use flux scripting together with the interactive UI. . In the following flux query, we do not pass all the parameters manually but allow the use the UI to set them dynamically: . |&gt; range(start: v.timeRangeStart, stop: v.timeRangeStop): here v.timeRangeStart and v.timeRangeStop are set with time range selector dropdown and the on the graph (with the mouse) | |&gt; aggregateWindow(every: v.windowPeriod, fn: mean, createEmpty: false): we define the window with v.windowPeriod | . from(bucket: &quot;trial_bucket&quot;) |&gt; range(start: v.timeRangeStart, stop: v.timeRangeStop) |&gt; filter(fn: (r) =&gt; r[&quot;_measurement&quot;] == &quot;boltdb_reads_total&quot;) |&gt; aggregateWindow(every: v.windowPeriod, fn: mean, createEmpty: false) .",
            "url": "https://andrasnovoszath.com/2020/10/29/influxdb-query-flux.html",
            "relUrl": "/2020/10/29/influxdb-query-flux.html",
            "date": " • Oct 29, 2020"
        }
        
    
  
    
        ,"post13": {
            "title": "Scraping, writing, and querying data with InfluxDB",
            "content": "Scraping data . InfluxDB can scrape data . Can collect data in specified intervals | Saves data into a InfluxDB bucket | It collects data from endpoints that accept HTTP(S) and that provide data in the [prometheus data format][https://prometheus.io/docs/instrumenting/exposition_formats/]. | . Steps to scrape data . Here we will use a the sample metrics data what influxDB generates and can be accessed at http://localhost:8086/metrics . Find the navigation menu and select Data | There select the Scrapers tab | Click on the Select Scraper button | In the popup window, enter a name and select a bucket. | For the Target URL add http://localhost:8086/metrics (or the url from where you would like to scrape your data) | Click Create | Querying data . You can mainly query data in the following ways . Flux, InfluxData&#39;s functional scripting language | The InfluxDB user interface | The influx command line interface | Directly through the InfluxDB API (/api/v2/query) | Using one of the InfluxDB clients |",
            "url": "https://andrasnovoszath.com/2020/10/28/influxdb-scrape-query.html",
            "relUrl": "/2020/10/28/influxdb-scrape-query.html",
            "date": " • Oct 28, 2020"
        }
        
    
  
    
        ,"post14": {
            "title": "InfluxDB ebook outline",
            "content": "I am taking part of the Gumroad 14 day product challenge where I decided to write an introductory ebook for influxDB as it currently has the biggest chance to succeed in my current workflow. . Audience . My target audience is data scientists who would like to start to use InfluxDB. They are not total beginners as they have an idea of what time series and databases are. Still, they have no experience with time series databases as such and zero experience with influxDB. . Content strategy . I am really behind this. I identified the following platforms where I could promote the process: . Social media channels . Twitter | LinkedIn | Facebook (especially groups) | Instagram | . Blogging sites . Medium (esp data science publications) | Dev.to | Hackernoon | KDnuggets | Dzone | . Quora . Link sharing sites . Hackerrank | Reddit (subreddits) | . Social aggregators . I am also trying to find a social media aggregator in this other post. Any suggestions are wellcome! . Outline . Title: InfluxDB for beginners . Introduction . What are time-series databases | How InfluxDB compares to others | The outline of this book | . Installation and setup . Download | Install | Configuration | Getting the Influx version | . Functional overview . The overview of InfluxDB and its place within the influxdata ecosystem (Tick stack) | InfluxDB OSS vs. Cloud | Main functionalities | Workflow | . Data format overview . InfluxDB time-series format | Tags and fields | . Loading/storing data . Loading sample data | Scraping data | Storage size | Data export/import | How to format time | Single vs. multiple measurements | value limits | Creating a database | . Querying . Discussing flux its functional data scripting language | The query language: select, where, group by, into, joins | Math operations: count, min, max, arithmetic | functions | retention policy and continuous query | . Common issues (based on Stackoverflow) . (Maybe I remix this into the normal chapters) . getting first/last values | select where value is null | How to delete data | How to connect with grafana | . Use case study . An example case study also perhaps using other features of the ecosystem. . Resources . Where to learn more | Learning strategies | Ideas for implementation | . Optional (if I will have time) . Discussing the python client .",
            "url": "https://andrasnovoszath.com/influxdb/gumroad/ebook/content%20strategy/2020/10/27/influxdb-ebook-outline.html",
            "relUrl": "/influxdb/gumroad/ebook/content%20strategy/2020/10/27/influxdb-ebook-outline.html",
            "date": " • Oct 27, 2020"
        }
        
    
  
    
        ,"post15": {
            "title": "Starting off with InfluxDB",
            "content": "InfluxDB is perhaps the most popular Time Series databases out there. . It is maintained by influxdata who also provide a whole hosted service with the database, metrics collection, dashboards, and scripting (flux). . Main operations . The main operations we can do with influxDB on data: . Collect | Write | Process | Visualize | Monitor | . Download . https://portal.influxdata.com/downloads/ . Select the latest version | Follow the instructions either for the docker file or for your OS. | . Installation . Follow the installation instructions for the open source version (OSS) | Run it with influxd from the terminal. | You can set up influxDB either by opening http://localhost:8086 in your browser, or by running influx setup in the terminal. | . There go through the setup instructions. If you want also set up the influx CLI. . The line protocol . You can write data into InfluxDB in a line protocol format. Its specifications: . One line is one data point | Dimensions: measurement | field set | tag set | timestamp | . | . A few example lines (from the official tutorial) . mem,host=host1 used_percent=23.43234543 1556892576842902000 cpu,host=host1 usage_user=3.8234,usage_system=4.23874 1556892726597397000 mem,host=host1 used_percent=21.83599203 1556892777007291000 . Scraping . InfluxDB can scrape data in the prometheus data format. .",
            "url": "https://andrasnovoszath.com/2020/10/26/influxdb-setup.html",
            "relUrl": "/2020/10/26/influxdb-setup.html",
            "date": " • Oct 26, 2020"
        }
        
    
  
    
        ,"post16": {
            "title": "Time series databases",
            "content": "Time series databases (TSDB) are specifically designed to store time series data, most importantly by storing time/value pairs. The use of time as a key index is perhaps the most important factor distinguishing them from, for example, relational ones. . Because of the increased use of machine-generated data (e.g. IoT devices) they gained popularity relative to other database categories (by 180% over the last 24 months based on DB-Engines) . Main types . There are many different types of time series databases. As of October 2020 DB-Engines ranks the following ones as the top five: . InfluxDB | Kdb+ | Prometheus | Graphite | RRDtool |",
            "url": "https://andrasnovoszath.com/2020/10/25/time-series-databases.html",
            "relUrl": "/2020/10/25/time-series-databases.html",
            "date": " • Oct 25, 2020"
        }
        
    
  
    
        ,"post17": {
            "title": "Principal Component Analysis from scatch - preparations",
            "content": "From the Data Science from Scratch book. . Libraries and helper functions . import math as m import random import pandas as pd import numpy as np import altair as alt . from typing import List Vector = List[float] . def add(vector1: Vector, vector2: Vector) -&gt; Vector: assert len(vector1) == len(vector2) return [v1 + v2 for v1, v2 in zip(vector1, vector2)] . def subtract(vector1: Vector, vector2:Vector) -&gt; Vector: assert len(vector1) == len(vector2) return [v1 - v2 for v1, v2 in zip(vector1, vector2)] . def vector_sum(vectors: List[Vector]) -&gt; Vector: assert vectors vector_length = len(vectors[0]) assert all(len(v) == vector_length for v in vectors) sums = [0] * vector_length for vector in vectors: sums = add(sums, vector) return sums . def scalar_multiply(c: float, vector: Vector) -&gt; Vector: return [c * v for v in vector] . def vector_mean(vector: Vector) -&gt; float: n = len(vector) return scalar_multiply(1/n, vector) . def dot(vector1: Vector, vector2: Vector) -&gt; float: assert len(vector1) == len(vector2) return sum(v1 * v2 for v1, v2 in zip(vector1, vector2)) . def sum_of_squares(v: Vector) -&gt; Vector: return dot(v, v) . def magnitude(v: Vector) -&gt; Vector: return m.sqrt(sum_of_squares(v)) . def gradient_step(v: Vector, gradient: Vector, step_size: float) -&gt; Vector: &quot;&quot;&quot;Return vector adjusted with step. Step is gradient times step size. &quot;&quot;&quot; step = scalar_multiply(step_size, gradient) return add(v, step) . Steps . intercept = random.randint(-30, 30) coefficient = random.uniform(-1, 1) n = 30 xs = np.random.randint(-50, 10 + 1, 30) ys = np.random.randint(-20, 50 + 1, 30) df = pd.DataFrame({&#39;x&#39;: xs, &#39;y&#39;: ys}) print(intercept, coefficient) alt.Chart(df).mark_point().encode( alt.X(&#39;x:Q&#39;), alt.Y(&#39;y:Q&#39;), alt.Tooltip([&#39;x&#39;, &#39;y&#39;]) ) . -10 0.9679420748641416 . De-meaning . def de_mean(data: List[Vector]) -&gt; List[Vector]: # mean = vector_mean(data) return [vector - np.mean(vector) for vector in data] . xs_demean, ys_demean = de_mean([xs, ys]) df = pd.DataFrame({&#39;x&#39;: xs_demean, &#39;y&#39;: ys_demean}) alt.Chart(df).mark_point().encode( alt.X(&#39;x:Q&#39;), alt.Y(&#39;y:Q&#39;), alt.Tooltip([&#39;x&#39;, &#39;y&#39;]) ) . Direction . def direction(w: Vector) -&gt; Vector: mag = magnitude(w) return [w_i / mag for w_i in w] direction(xs) . [-0.22863117335525085, -0.11431558667762542, -0.07396890902669881, -0.24208006590555972, -0.21518228080494198, -0.02017333882546331, -0.1277644792279343, 0.04034667765092662, -0.07396890902669881, -0.24208006590555972, -0.22863117335525085, -0.2353556196304053, -0.10759114040247099, -0.06724446275154437, -0.053795570201235494, -0.04707112392608106, -0.31604897493225853, 0.04707112392608106, -0.10759114040247099, -0.1815600494291698, -0.29587563610679524, -0.2017333882546331, -0.1613867106037065, -0.006724446275154437, -0.19500894197947868, -0.18828449570432423, -0.21518228080494198, -0.32949786748256743, -0.27570229728133194, -0.08741780157700768] . xs_dir = direction(xs_demean) ys_dir = direction(ys_demean) df = pd.DataFrame({&#39;x&#39;: xs_dir, &#39;y&#39;: ys_dir}) alt.Chart(df).mark_point().encode( alt.X(&#39;x:Q&#39;), alt.Y(&#39;y:Q&#39;), alt.Tooltip([&#39;x&#39;, &#39;y&#39;]) ) .",
            "url": "https://andrasnovoszath.com/2020/10/24/pca-preparations.html",
            "relUrl": "/2020/10/24/pca-preparations.html",
            "date": " • Oct 24, 2020"
        }
        
    
  
    
        ,"post18": {
            "title": "Named tuples and dataclasses",
            "content": "A quick review of named tuples (standad and typed) and dataclasses. . From the Data Science from Scratch book. . namedtuple . Similar to dictionaries but . fields are named | values are immutable | . from collections import namedtuple import datetime . StockPrice = namedtuple(&#39;StockPrice&#39;, [&#39;symbol&#39;, &#39;date&#39;, &#39;closing_price&#39;]) . type(StockPrice) . type . price = StockPrice(&#39;MSFT&#39;, datetime.date(2018, 12, 14), 106.03) price . StockPrice(symbol=&#39;MSFT&#39;, date=datetime.date(2018, 12, 14), closing_price=106.03) . type(price) . __main__.StockPrice . price.symbol, price.date, price.closing_price . (&#39;MSFT&#39;, datetime.date(2018, 12, 14), 106.03) . NamedTuple . Simlar to namedtuple but . it is a class | fields have to be typed | as it is a class, it can have methods | . from typing import NamedTuple . class StockPrice(NamedTuple): symbol: str date: datetime.date closing_price: float def is_high_tech(self) -&gt; bool: return self.symbol in [&#39;MSFT&#39;, &#39;GOOG&#39;, &#39;FB&#39;, &#39;AMZN&#39;, &#39;AAPL&#39;] . stockprice = StockPrice(&#39;MSFT&#39;, datetime.date(2019, 12, 14), 106.83) stockprice . StockPrice(symbol=&#39;MSFT&#39;, date=datetime.date(2019, 12, 14), closing_price=106.83) . stockprice.is_high_tech() . True . StockPrice(&#39;TOOT&#39;, 12, 12).is_high_tech() . False . Dataclasses . Similar to NamedTuple but . fields are mutable | . from dataclasses import dataclass . @dataclass class StockPrice2: symbol: str date: datetime.date closing_price: float def is_high_tech(self) -&gt; bool: return self.symbol in [&#39;MSFT&#39;, &#39;GOOG&#39;, &#39;FB&#39;, &#39;AMZN&#39;, &#39;AAPL&#39;] . stockprice2 = StockPrice2(&#39;MSFt&#39;, datetime.date(2018, 12, 14), 106.03) stockprice2 . StockPrice2(symbol=&#39;MSFt&#39;, date=datetime.date(2018, 12, 14), closing_price=106.03) . stockprice2.closing_price /= 2 stockprice2.closing_price . 53.015 . For NamedTuple this does not work . stockprice.closing_price /= 2 . AttributeError Traceback (most recent call last) &lt;ipython-input-16-7ab00711d514&gt; in &lt;module&gt; -&gt; 1 stockprice.closing_price /= 2 AttributeError: can&#39;t set attribute .",
            "url": "https://andrasnovoszath.com/2020/10/23/namedtuple-dataclasses.html",
            "relUrl": "/2020/10/23/namedtuple-dataclasses.html",
            "date": " • Oct 23, 2020"
        }
        
    
  
    
        ,"post19": {
            "title": "Stochastic gradient descent",
            "content": "From the Data Science from Scratch book. . Libraries and helper functions . from typing import List import random . Vector = List[float] . def add(vector1: Vector, vector2: Vector) -&gt; Vector: assert len(vector1) == len(vector2) return [v1 + v2 for v1, v2 in zip(vector1, vector2)] def scalar_multiply(c: float, vector: Vector) -&gt; Vector: return [c * v for v in vector] def gradient_step(v: Vector, gradient: Vector, step_size: float) -&gt; Vector: &quot;&quot;&quot;Return vector adjusted with step. Step is gradient times step size. &quot;&quot;&quot; step = scalar_multiply(step_size, gradient) return add(v, step) def linear_gradient(x: float, y: float, theta: Vector) -&gt; Vector: slope, intercept = theta predicted = slope * x + intercept error = (predicted - y) #** 2 # print(x, y, theta, predicted, error) return [2 * error * x, 2 * error] . Stochastic gradients . Here we use one training example at a time to calculate the gradient steps . inputs = [(x, 20 * x + 5) for x in range(-50, 50)] theta = [random.uniform(-1, 1), random.uniform(-1, 1)] learning_rate = 0.001 for epoch in range(100): for x, y in inputs: grad = linear_gradient(x, y, theta) theta = gradient_step(theta, grad, -learning_rate) print(epoch, theta) . 0 [20.108274621088928, -0.3890550572184463] 1 [20.103628550173042, -0.15784430337372637] 2 [20.09918250047512, 0.06344662581205483] 3 [20.094927182760102, 0.2752433412342787] 4 [20.090854449810823, 0.47795318030884215] 5 [20.086956448727392, 0.6719660044610173] 6 [20.0832257045743, 0.8576549486610185] 7 [20.07965500742264, 1.0353771386943718] 8 [20.076237509713653, 1.2054743780282082] 9 [20.07296662801438, 1.3682738056445862] 10 [20.06983608174483, 1.5240885250583422] 11 [20.06683986242782, 1.6732182068542554] 12 [20.063972181799524, 1.8159496643823287] 13 [20.06122752813499, 1.9525574051756995] 14 [20.05860064833006, 2.083304159961789] 15 [20.056086446554076, 2.2084413869124764] 16 [20.05368014168373, 2.328209755991209] 17 [20.051377044125662, 2.442839611270341] 18 [20.049172772009843, 2.552551414011119] 19 [20.047063092087537, 2.6575561678668613] 20 [20.045043898683737, 2.758055822780714] 21 [20.04311134626141, 2.8542436641396707] 22 [20.04126169735246, 2.9463046849793786] 23 [20.039491436402585, 3.0344159418586236] 24 [20.037797100119768, 3.118746894557255] 25 [20.0361754521855, 3.19945973173537] 26 [20.034623390938133, 3.276709684335131] 27 [20.03313793652982, 3.3506453237233305] 28 [20.03171617359179, 3.421408845892839] 29 [20.030355435914238, 3.4891363463999814] 30 [20.02905307411413, 3.5539580824049044] 31 [20.02780658595349, 3.6159987219880825] 32 [20.026613585687915, 3.6753775847836647] 33 [20.025471758004617, 3.732208870958282] 34 [20.024378926653775, 3.7866018810682345] 35 [20.023332973880006, 3.8386612263190933] 36 [20.022331891127603, 3.8884870294569893] 37 [20.021373782454166, 3.936175118240505] 38 [20.020456760427248, 3.981817208697045] 39 [20.019579095445714, 4.0255010817522585] 40 [20.018739087243826, 4.067310752589061] 41 [20.017935091021968, 4.107326630985043] 42 [20.017165620461952, 4.14562567751338] 43 [20.016429140217177, 4.182281550851487] 44 [20.015724268710606, 4.217364749099938] 45 [20.015049636287394, 4.250942746103223] 46 [20.014403952714495, 4.283080120704653] 47 [20.01378597502875, 4.313838681185705] 48 [20.013194514460565, 4.343277583993619] 49 [20.012628414045615, 4.371453447141006] 50 [20.01208658825722, 4.398420459215999] 51 [20.011568035372246, 4.424230484791519] 52 [20.011071730850883, 4.448933163459061] 53 [20.010596714895236, 4.472576004466116] 54 [20.01014208044218, 4.495204478772731] 55 [20.009706939528638, 4.5168621063046] 56 [20.009290475161325, 4.5375905399679235] 57 [20.008891875158, 4.557429645750311] 58 [20.008510370231065, 4.576417578971538] 59 [20.00814524887334, 4.594590858346274] 60 [20.007795789585977, 4.611984435823321] 61 [20.00746133605139, 4.628631763741193] 62 [20.00714119205082, 4.644564858428533] 63 [20.00683481657478, 4.659814363112007] 64 [20.00654157694565, 4.674409606833622] 65 [20.00626093700885, 4.688378660053233] 66 [20.0059923009138, 4.701748388289354] 67 [20.005735205576336, 4.714544504461379] 68 [20.00548916033653, 4.726791619391082] 69 [20.00525365468124, 4.738513287317169] 70 [20.005028243503418, 4.749732051371429] 71 [20.004812513452155, 4.760469488056405] 72 [20.0046060409254, 4.770746248364355] 73 [20.004408419316054, 4.780582096925131] 74 [20.004219291985194, 4.789995950689194] 75 [20.004038256610244, 4.799005914654827] 76 [20.00386500754045, 4.807629317187821] 77 [20.003699183947933, 4.815882743439104] 78 [20.00354046799362, 4.823782066495483] 79 [20.003388552379008, 4.831342478409528] 80 [20.003243202425267, 4.838578520565049] 81 [20.003104049179598, 4.8455041097305935] 82 [20.002970873552083, 4.852132564899691] 83 [20.0028434121415, 4.858476634395222] 84 [20.00272141143189, 4.864548519281559] 85 [20.002604655271004, 4.870359897369896] 86 [20.002492920554257, 4.875921945826025] 87 [20.00238595286676, 4.881245361497983] 88 [20.002283589604943, 4.886340382432449] 89 [20.0021855928701, 4.8912168074015] 90 [20.002091842376988, 4.8958840153869385] 91 [20.002002090950178, 4.90035098289899] 92 [20.001916203724086, 4.904626300833264] 93 [20.001833987421463, 4.908718191643037] 94 [20.00175530458321, 4.912634524898607] 95 [20.001680007220763, 4.916382832992619] 96 [20.001607906672934, 4.919970324321837] 97 [20.001538916328176, 4.923403898248704] 98 [20.001472898762803, 4.926690159008146] 99 [20.001409710601003, 4.929835427066589] .",
            "url": "https://andrasnovoszath.com/2020/10/22/stochastic-gradient-descent.html",
            "relUrl": "/2020/10/22/stochastic-gradient-descent.html",
            "date": " • Oct 22, 2020"
        }
        
    
  
    
        ,"post20": {
            "title": "Gradient Descent Minibatch",
            "content": "Gradiant descent with minibatches . Libraries and helper functions . import random from typing import TypeVar, List, Iterator . Vector = List[float] . def add(vector1: Vector, vector2: Vector) -&gt; Vector: assert len(vector1) == len(vector2) return [v1 + v2 for v1, v2 in zip(vector1, vector2)] . def vector_sum(vectors: List[Vector]) -&gt; Vector: assert vectors vector_length = len(vectors[0]) assert all(len(v) == vector_length for v in vectors) sums = [0] * vector_length for vector in vectors: sums = add(sums, vector) return sums . def scalar_multiply(c: float, vector: Vector) -&gt; Vector: return [c * v for v in vector] . def vector_mean(vectors: List[Vector]) -&gt; Vector: n = len(vectors) return scalar_multiply(1/n, vector_sum(vectors)) . def gradient_step(v: Vector, gradient: Vector, step_size: float) -&gt; Vector: &quot;&quot;&quot;Return vector adjusted with step. Step is gradient times step size. &quot;&quot;&quot; step = scalar_multiply(step_size, gradient) return add(v, step) . def linear_gradient(x: float, y: float, theta: Vector) -&gt; Vector: slope, intercept = theta predicted = slope * x + intercept error = (predicted - y) #** 2 # print(x, y, theta, predicted, error) return [2 * error * x, 2 * error] . Minibatch gradient . T = TypeVar(&#39;T&#39;) . def minibatches(dataset: List[T], batch_size=int, shuffle: bool = True) -&gt; Iterator[List[T]]: batch_starts = [start for start in range(0, len(dataset), batch_size)] if shuffle: random.shuffle(batch_starts) for start in batch_starts: end = start + batch_size yield dataset[start:end] . inputs = [(x, 20 * x + 5) for x in range(-50, 50)] inputs[:10] . [(-50, -995), (-49, -975), (-48, -955), (-47, -935), (-46, -915), (-45, -895), (-44, -875), (-43, -855), (-42, -835), (-41, -815)] . for batch in minibatches(inputs, batch_size=5, shuffle=False): print(batch) . [(-50, -995), (-49, -975), (-48, -955), (-47, -935), (-46, -915)] [(-45, -895), (-44, -875), (-43, -855), (-42, -835), (-41, -815)] [(-40, -795), (-39, -775), (-38, -755), (-37, -735), (-36, -715)] [(-35, -695), (-34, -675), (-33, -655), (-32, -635), (-31, -615)] [(-30, -595), (-29, -575), (-28, -555), (-27, -535), (-26, -515)] [(-25, -495), (-24, -475), (-23, -455), (-22, -435), (-21, -415)] [(-20, -395), (-19, -375), (-18, -355), (-17, -335), (-16, -315)] [(-15, -295), (-14, -275), (-13, -255), (-12, -235), (-11, -215)] [(-10, -195), (-9, -175), (-8, -155), (-7, -135), (-6, -115)] [(-5, -95), (-4, -75), (-3, -55), (-2, -35), (-1, -15)] [(0, 5), (1, 25), (2, 45), (3, 65), (4, 85)] [(5, 105), (6, 125), (7, 145), (8, 165), (9, 185)] [(10, 205), (11, 225), (12, 245), (13, 265), (14, 285)] [(15, 305), (16, 325), (17, 345), (18, 365), (19, 385)] [(20, 405), (21, 425), (22, 445), (23, 465), (24, 485)] [(25, 505), (26, 525), (27, 545), (28, 565), (29, 585)] [(30, 605), (31, 625), (32, 645), (33, 665), (34, 685)] [(35, 705), (36, 725), (37, 745), (38, 765), (39, 785)] [(40, 805), (41, 825), (42, 845), (43, 865), (44, 885)] [(45, 905), (46, 925), (47, 945), (48, 965), (49, 985)] . batch . [(45, 905), (46, 925), (47, 945), (48, 965), (49, 985)] . theta = [random.uniform(-1, 1), random.uniform(-1, 1)] vector_mean([linear_gradient(x, y, theta) for x, y in batch]) . [-85269.18707965006, -1812.6065734463045] . inputs = [(x, 20 * x + 5) for x in range(-50, 50)] theta = [random.uniform(-1, 1), random.uniform(-1, 1)] learning_rate = 0.001 minibatch_results = [] for epoch in range(1000): for batch in minibatches(inputs, batch_size=20): grad = vector_mean([linear_gradient(x, y, theta) for x, y in batch]) theta = gradient_step(theta, grad, -learning_rate) minibatch_results.append([epoch, theta]) . Last twenty epochs . minibatch_results[-20:] . [[980, [20.00000231135919, 4.999994332476798]], [981, [19.99999999323285, 4.999994612954206]], [982, [20.000000061792846, 4.999994646372994]], [983, [19.99999994338701, 4.999994665516976]], [984, [19.99999978951187, 4.999994715352837]], [985, [20.000000045649184, 4.999994748354316]], [986, [19.99999982652112, 4.999994803275764]], [987, [20.000000353260692, 4.999994822918053]], [988, [20.000000203297585, 4.99999512614361]], [989, [20.000000013241355, 4.999995351690553]], [990, [20.00000018064206, 4.999995396803261]], [991, [20.00000031292562, 4.999995409193917]], [992, [19.99999796367368, 4.999995522343663]], [993, [19.9999998507973, 4.999995639004705]], [994, [20.00000000615185, 4.999995667269925]], [995, [19.999999823836678, 4.999995780888271]], [996, [20.000000260844455, 4.999995797933743]], [997, [20.00000006542973, 4.999995821702883]], [998, [19.999999347865106, 4.999995882305616]], [999, [20.000000771099824, 4.999995951108064]]] .",
            "url": "https://andrasnovoszath.com/2020/10/21/gradient-descent-minibatch.html",
            "relUrl": "/2020/10/21/gradient-descent-minibatch.html",
            "date": " • Oct 21, 2020"
        }
        
    
  
    
        ,"post21": {
            "title": "Gradient descent from scratch with Python",
            "content": "From the Data Science from Scratch book. . Libraries and helper functions . import random from typing import List, Callable import pandas as pd import altair as alt . Vector = List[float] Vector . typing.List[float] . def add(vector1: Vector, vector2: Vector) -&gt; Vector: assert len(vector1) == len(vector2) return [v1 + v2 for v1, v2 in zip(vector1, vector2)] . def scalar_multiply(c: float, vector: Vector) -&gt; Vector: return [c * v for v in vector] . def dot(vector1: Vector, vector2: Vector) -&gt; float: assert len(vector1) == len(vector2) return sum(v1 * v2 for v1, v2 in zip(vector1, vector2)) . def sum_of_squares(v: Vector) -&gt; Vector: return dot(v, v) . def magnitude(v: Vector) -&gt; Vector: return m.sqrt(sum_of_squares(v)) . def distance(vector1: Vector, vector2: Vector) -&gt; Vector: return magnitude(subtract(vector1, vector2)) . def vector_sum(vectors: List[Vector]) -&gt; Vector: assert vectors vector_length = len(vectors[0]) assert all(len(v) == vector_length for v in vectors) sums = [0] * vector_length for vector in vectors: sums = add(sums, vector) return sums . def vector_mean(vectors: List[Vector]) -&gt; Vector: n = len(vectors) return scalar_multiply(1/n, vector_sum(vectors)) . Estimate gradient . def estimate_gradient( f: Callable[[Vector], float], v: Vector, h: float = 0.0001 ): return [ partial_difference_quotient(f, v, i, h) for i in range(len(v)) ] . v = [random.uniform(-10, 10) for i in range(3)] v . [-9.684378319623278, 4.813838863175313, 3.1311841279856303] . Adjusting with gradients . def gradient_step(v: Vector, gradient: Vector, step_size: float) -&gt; Vector: &quot;&quot;&quot;Return vector adjusted with step. Step is gradient times step size. &quot;&quot;&quot; step = scalar_multiply(step_size, gradient) return add(v, step) v, gradient_step(v, [1, 1, 1], step_size=0.1) . ([-9.684378319623278, 4.813838863175313, 3.1311841279856303], [-9.584378319623278, 4.913838863175313, 3.2311841279856304]) . def sum_of_squares_gradient(v: Vector) -&gt; Vector: return [2 * v_i for v_i in v] sum_of_squares_gradient(v) . [-19.368756639246556, 9.627677726350626, 6.262368255971261] . steps = pd.DataFrame() for epoch in range(1000): grad = sum_of_squares_gradient(v) v = gradient_step(v, grad, -0.01) steps = steps.append( {&#39;x&#39;: v[0], &#39;y&#39;: v[1], &#39;z&#39;: v[2]}, ignore_index=True ) steps . x y z . 0 4.718042e+00 | -7.666161e+00 | -5.877492e-01 | . 1 4.623681e+00 | -7.512838e+00 | -5.759942e-01 | . 2 4.531207e+00 | -7.362581e+00 | -5.644743e-01 | . 3 4.440583e+00 | -7.215329e+00 | -5.531848e-01 | . 4 4.351771e+00 | -7.071023e+00 | -5.421211e-01 | . ... ... | ... | ... | . 995 8.784299e-09 | -1.427326e-08 | -1.094302e-09 | . 996 8.608613e-09 | -1.398780e-08 | -1.072416e-09 | . 997 8.436440e-09 | -1.370804e-08 | -1.050968e-09 | . 998 8.267712e-09 | -1.343388e-08 | -1.029949e-09 | . 999 8.102357e-09 | -1.316520e-08 | -1.009350e-09 | . 1000 rows × 3 columns . alt.Chart(steps.reset_index().melt(id_vars=&#39;index&#39;)).mark_point().encode( alt.X(&#39;index:Q&#39;), alt.Y(&#39;value:Q&#39;), alt.Color(&#39;variable:N&#39;) ).properties(title=&#39;Gradient steps&#39;) . Changing the step size . inputs = [(x, 20 * x + 5) for x in range(-50, 50)] inputs[:20] . [(-50, -995), (-49, -975), (-48, -955), (-47, -935), (-46, -915), (-45, -895), (-44, -875), (-43, -855), (-42, -835), (-41, -815), (-40, -795), (-39, -775), (-38, -755), (-37, -735), (-36, -715), (-35, -695), (-34, -675), (-33, -655), (-32, -635), (-31, -615)] . We use a simple linear gradient . def linear_gradient(x: float, y: float, theta: Vector) -&gt; Vector: slope, intercept = theta predicted = slope * x + intercept error = (predicted - y) #** 2 return [2 * error * x, 2 * error] x, y = inputs[0][0], inputs[0][1] theta = [random.uniform(-1, 1), random.uniform(-1, 1)] x, y, theta, linear_gradient(x, y, theta) . (-50, -995, [-0.2222082595361492, -0.7215025177939673], [-100538.89104590134, 2010.777820918027]) . theta = [random.uniform(-1, 1), random.uniform(-1, 1)] learning_rate = 0.001 for epoch in range(20): grad = vector_mean([linear_gradient(x, y, theta) for x, y in inputs]) theta = gradient_step(theta, grad, -learning_rate) print(epoch, theta) . 0 [33.27214426853234, 0.961944418352982] 1 [11.143441717307283, 0.9832926737848086] 2 [25.903307667229832, 0.9824695301545462] 3 [16.05847625548786, 0.996407898761467] 4 [22.624992745488356, 1.0004735592194318] 5 [18.245130312318487, 1.0110976048464813] 6 [21.16650917928842, 1.0173205399491068] 7 [19.217955697954576, 1.026452408048497] 8 [20.517650001872347, 1.0336174589303544] 9 [19.650761066210077, 1.042067874014366] 10 [20.228984436711894, 1.0496344993325475] 11 [19.8433170152125, 1.0577642147705943] 12 [20.100565315068035, 1.0654920033562656] 13 [19.928988426852978, 1.0734615846646212] 14 [20.04343818087373, 1.081243649922145] 15 [19.967107977007146, 1.0891246008031743] 16 [20.018028103937038, 1.096913459578575] 17 [19.984072168133576, 1.104737660763355] 18 [20.00672860151567, 1.1125122576099618] 19 [19.991624535046657, 1.1202939616962575] .",
            "url": "https://andrasnovoszath.com/2020/10/20/gradient-descent.html",
            "relUrl": "/2020/10/20/gradient-descent.html",
            "date": " • Oct 20, 2020"
        }
        
    
  
    
        ,"post22": {
            "title": "Difference quotients from scratch with Python",
            "content": "From the Data Science from Scratch book. . Libraries and helper functions . import pandas as pd import altair as alt . from typing import List Vector = List[float] Vector . typing.List[float] . def dot(vector1: Vector, vector2: Vector) -&gt; float: assert len(vector1) == len(vector2) return sum(v1 * v2 for v1, v2 in zip(vector1, vector2)) assert dot([1, 2, 3], [4, 5, 6]) == 32 . def sum_of_squares(v: Vector) -&gt; Vector: return dot(v, v) assert sum_of_squares([1, 2, 3]) == 14 . Difference quotient . from typing import Callable def difference_quotient( f: Callable[[float], float], x: float, h: float ) -&gt; float : return (f(x + h) - f(x)) / h . A simple estimate . def square(x: float) -&gt; float: return x * x . def derivative_x2(x: float) -&gt; float: return 2 * x . xs = range(-10, 11) actuals = [derivative_x2(x) for x in xs] actuals . [-20, -18, -16, -14, -12, -10, -8, -6, -4, -2, 0, 2, 4, 6, 8, 10, 12, 14, 16, 18, 20] . estimates = [difference_quotient(square, x, h=0.001) for x in xs] estimates . [-19.998999999984335, -17.998999999988996, -15.999000000007868, -13.999000000005424, -11.99900000000298, -9.999000000004088, -7.998999999999867, -5.998999999999199, -3.9989999999994197, -1.998999999999973, 0.001, 2.0009999999996975, 4.000999999999699, 6.000999999999479, 8.0010000000037, 10.001000000002591, 12.001000000005035, 14.00100000000748, 16.000999999988608, 18.000999999983947, 20.000999999993496] . df = pd.DataFrame({&#39;actuals&#39;: actuals, &#39;estimates&#39;: estimates}).reset_index() df = df.melt(id_vars=&#39;index&#39;) df.sample(10) . index variable value . 5 5 | actuals | -10.000 | . 13 13 | actuals | 6.000 | . 34 13 | estimates | 6.001 | . 2 2 | actuals | -16.000 | . 19 19 | actuals | 18.000 | . 0 0 | actuals | -20.000 | . 7 7 | actuals | -6.000 | . 3 3 | actuals | -14.000 | . 24 3 | estimates | -13.999 | . 11 11 | actuals | 2.000 | . alt.Chart(df).mark_circle(opacity=0.75).encode( alt.X(&#39;index:Q&#39;), alt.Y(&#39;value:Q&#39;), alt.Size(&#39;variable:N&#39;), alt.Color(&#39;variable:N&#39;) ).properties(title=&#39;Actual derivatives and estimated quotients&#39;) . Calculating an i-th difference quotient . def partial_difference_quotient( f: Callable[[Vector], float], v: Vector, i: int, h: float ) -&gt; float: &quot;&quot;&quot;Return i-th parital difference quotient of `f` at a`v`&quot;&quot;&quot; w = [ v_j + (h if j == i else 0) for j, v_j in enumerate(v) ] return (f(w) - f(v)) / h . def estimate_gradient( f: Callable[[Vector], float], v: Vector, h: float = 0.0001 ): return [ partial_difference_quotient(f, v, i, h) for i in range(len(v)) ] .",
            "url": "https://andrasnovoszath.com/2020/10/19/difference_quotient_estimate.html",
            "relUrl": "/2020/10/19/difference_quotient_estimate.html",
            "date": " • Oct 19, 2020"
        }
        
    
  
    
        ,"post23": {
            "title": "Using the Beta distribution",
            "content": "We try to use beta distributions to estimate if our coin is fair. . From the Data Science from Scratch book. . Libraries and helper functions . import math as m import numpy as np import altair as alt import pandas as pd . def B(alpha: float, beta: float) -&gt; float: &quot;This scales the parameters between 0 and 1&quot; return m.gamma(alpha) * m.gamma(beta) / m.gamma(alpha + beta) def beta_pdf(x: float, alpha: float, beta:float) -&gt; float: if x &lt;= 0 or x &gt;=1: return 0 return x ** (alpha - 1) * (1 - x) ** (beta - 1) / B(alpha, beta) . Example 1 . We do not want to make assumptions beforehand, so we choose both $ alpha$ and $ beta$ to be 1: $B(1, 1)$ | We flip the coin 10 times and get 3 heads | Our new posterior distribution becomes $B(4, 8)$ centered around 0.33 | We have to assume that the observed probabilty is the real | Example 2 . We have a strong assumption that the coin is fair so we choose a $B(20, 20) | Again, we got 3 heads out of 10 | Our new Beta is $B(23, 27)$ centered around 0.46 | It suggest that the coin is slightly biased toward tails | Example 3 . We believe that the coin is biased toward head by 75% of the time, so we choose $B(30, 10)$ | Again, we got 3 heads out of 10 | Our poseterior distribution is $B(33, 17)$ centered around 0.66 | It suggest that the coin is biased toward the head, although less strongly as we believed | df = pd.DataFrame() Beta_combinations = [(1, 1), (4, 8), (20, 20), (23, 27), (30, 10), (33, 17)] for Beta in Beta_combinations: alpha, beta = Beta df_B = pd.DataFrame() df_B[&#39;x&#39;] = pd.Series(np.arange(0.01, 1, .01)) df_B[&#39;y&#39;] = df_B[&#39;x&#39;].apply(lambda x: beta_pdf(x, alpha, beta)) df_B[&#39;Beta&#39;] = f&#39;({alpha}, {beta})&#39; df = pd.concat([df, df_B]) . alt.Chart(df).mark_line().encode( alt.X(&#39;x:Q&#39;), alt.Y(&#39;y:Q&#39;), alt.Color(&#39;Beta&#39;), tooltip=[&#39;x&#39;, &#39;y&#39;, &#39;Beta&#39;], strokeDash=&#39;Beta&#39; ).properties(width=600, title=&#39;Beta distributions&#39;) .",
            "url": "https://andrasnovoszath.com/2020/10/18/beta-coin-examples.html",
            "relUrl": "/2020/10/18/beta-coin-examples.html",
            "date": " • Oct 18, 2020"
        }
        
    
  
    
        ,"post24": {
            "title": "Beta distribution",
            "content": "When trying to to Bayesian interference to estimate probability, we can use Beta distribution as a prior. Below are some steps to calculate it. . From the Data Science from Scratch book. . import math as m . $ f(x, alpha, beta) = x^{ alpha - 1} (1 - x)^{ beta - 1} frac {1} { text{B}( alpha, beta)} $ . where . $ text{B} = Gamma( alpha) Gamma( beta) frac{1} { Gamma( alpha + beta)} $ . where . $ Gamma(n)$ is the Gamma function which, for positive integers is . $ Gamma(n) = (n - 1)!$ . def B(alpha: float, beta: float) -&gt; float: &quot;This scales the parameters between 0 and 1&quot; return m.gamma(alpha) * m.gamma(beta) / m.gamma(alpha + beta) . def beta_pdf(x: float, alpha: float, beta:float) -&gt; float: if x &lt;= 0 or x &gt;=1: return 0 return x ** (alpha - 1) * (1 - x) ** (beta - 1) / B(alpha, beta) . import numpy as np import pandas as pd import altair as alt . df = pd.DataFrame() Beta_combinations = [(1, 1), (10, 10), (4, 16), (16, 4), (30, 30)] for Beta in Beta_combinations: alpha, beta = Beta df_B = pd.DataFrame() df_B[&#39;x&#39;] = pd.Series(np.arange(0.01, 1, .01)) df_B[&#39;y&#39;] = df_B[&#39;x&#39;].apply(lambda x: beta_pdf(x, alpha, beta)) df_B[&#39;Beta&#39;] = f&#39;({alpha}, {beta})&#39; df = pd.concat([df, df_B]) . The distribution centers around $ alpha frac{1} { alpha + beta} $ . Beta(1, 1) is the uniform distribution in [1, 1] | When alpha is greater than beta the distribution is skewed to the left (and respectively, in the opposite case) | The greater alpha and beta are the &#39;tighter&#39; is the distribution | . alt.Chart(df).mark_line().encode(alt.X(&#39;x:Q&#39;), alt.Y(&#39;y:Q&#39;), alt.Color(&#39;Beta&#39;), tooltip=[&#39;x&#39;, &#39;y&#39;, &#39;Beta&#39;], strokeDash=&#39;Beta&#39;).properties(width=600) .",
            "url": "https://andrasnovoszath.com/2020/10/17/beta-distribution.html",
            "relUrl": "/2020/10/17/beta-distribution.html",
            "date": " • Oct 17, 2020"
        }
        
    
  
    
        ,"post25": {
            "title": "A/B testing with Python",
            "content": "Introduction to A/B testing with python. . From the Data Science from Scratch book. . Libraries and helper functions . import math as m from typing import Tuple . def normal_probability_below(x: float, mu: float = 0, sigma: float = 1) -&gt; float: return (1 + m.erf((x - mu) / m.sqrt(2) / sigma)) / 2 def normal_probability_above(lo: float, mu: float = 0, sigma: float = 1) -&gt; float: return 1 - normal_probability_below(lo, mu, sigma) . def two_sided_p_value(x: float, mu: float = 0, sigma: float = 1) -&gt; float: &quot;&quot;&quot;Return the probability of getting at least as extreme value as `x`, given that our values are from a normal distribution with `mu` mean and `sigma` std. &quot;&quot;&quot; # If x is greater than the mean return everything above x if x &gt;= mu: return 2 * normal_probability_above(x, mu, sigma) # If x is less than the mean than return everything below x else: return 2 * normal_probability_below(x, mu, sigma) . A/B test . def estimate_parameters(N: int, true: int) -&gt; Tuple[float, float]: p = true / N sigma = m.sqrt(p * (1 - p) / N) return p, sigma . $H_0$: $p_a$ and $p_b$ are the same . With simplification this means that $p_a - p_b = 0$ . def a_b_test_statistic(N_A: int, A: int, N_B: int, B:int) -&gt; float: p_A, sigma_A = estimate_parameters(N_A, A) p_B, simga_B = estimate_parameters(N_B, B) return (p_B - p_A) / m.sqrt(sigma_A ** 2 + simga_B ** 2) . Example 1 . z = a_b_test_statistic(1000, 200, 1000, 180) z, two_sided_p_value(z) . (-1.1403464899034472, 0.2541419765422359) . That is, the probability of at least such a big difference occurring assuming that the two probabilities are the same is ~0.25. . Example 2 . Let&#39;s decrease the occurance of $b$ even more. . z = a_b_test_statistic(1000, 200, 1000, 150) z, two_sided_p_value(z) . (-2.948839123097944, 0.003189699706216853) . That is the probability of at least this large difference to occur if the probabilities are the same is 0.03. .",
            "url": "https://andrasnovoszath.com/2020/10/16/ab-testing.html",
            "relUrl": "/2020/10/16/ab-testing.html",
            "date": " • Oct 16, 2020"
        }
        
    
  
    
        ,"post26": {
            "title": "Programmatic p-hacking from scratch with Python",
            "content": "Below is a short script to demonstrate the &#39;process of p-hacking&#39;. . From the Data Science from Scratch book. . import random from typing import List . First we define a usual experiment consisting of 1000 binomial trials with 0.5 probability. . def run_experiment(trials) -&gt; List[bool]: return [random.random() &lt; 0.5 for _ in range(trials)] experiment = run_experiment(1000) print(&quot;Proportion of heads:&quot;, sum(experiment) / len(experiment)) print(&quot;First 10 elements:&quot;, experiment[:10]) . Proportion of heads: 0.51 First 10 elements: [True, True, True, False, False, False, False, True, True, True] . Then we examine whether the outcome an experiment is beyond the 95% confidence levels around p = 0.5, that is, the hypothesis of having a fair coin. . def reject_fairness(experiment: List[bool]) -&gt; bool: num_heads = sum(experiment) return num_heads &lt; 469 or num_heads &gt; 531 reject_fairness(experiment) . False . We run 1000 independent experiments with the exact same parameters. . random.seed(42) experiments = [run_experiment(1000) for _ in range(1000)] . Now we can simply pick those experiments which fall outside the confidence level. . number_of_unfair = sum([reject_fairness(experiment) for experiment in experiments]) print(&quot;Number of experiments &#39;showing&#39; that the coin if unfair:&quot;, number_of_unfair) print(&quot; nProbabilities:&quot;) print(&quot; t&quot;.join([str(sum(experiment) / len(experiment)) for experiment in experiments if reject_fairness(experiment)])) . Number of experiments &#39;showing&#39; that the coin if unfair: 42 Probabilities: 0.532 0.539 0.535 0.461 0.466 0.539 0.467 0.468 0.54 0.458 0.468 0.463 0.467 0.46 0.461 0.463 0.541 0.464 0.538 0.542 0.461 0.465 0.468 0.538 0.466 0.46 0.468 0.534 0.535 0.468 0.537 0.468 0.535 0.538 0.451 0.537 0.463 0.466 0.46 0.536 0.466 0.467 .",
            "url": "https://andrasnovoszath.com/2020/10/15/p-hacking.html",
            "relUrl": "/2020/10/15/p-hacking.html",
            "date": " • Oct 15, 2020"
        }
        
    
  
    
        ,"post27": {
            "title": "Confidence intervals from scratch with Python",
            "content": "From the Data Science from Scratch book. . Libraries and helper functions . import math as m . def calc_normal_cdf(x: float, mu: float = 0, sigma: float = 1) -&gt; float: return (1 + m.erf((x - mu) / m.sqrt(2) / sigma)) / 2 . def calc_inverse_normal_cdf(p: float, mu:float = 0, sigma: float = 1, tolerance: float = 1E-5, show_steps=False) -&gt; float: if p == 0: return -np.inf if p == 1: return np.inf # In case it is not a standard normal distribution, calculate the standard normal first and then rescale if mu != 0 or sigma != 1: return mu + sigma * calc_inverse_normal_cdf(p, tolerance=tolerance) low_z = -10 hi_z = 10 if show_steps: print(f&quot;{&#39;&#39;:&lt;19}&quot;.join([&#39;low_z&#39;, &#39;mid_z&#39;, &#39;hi_z&#39;]), &quot; n&quot;) while hi_z - low_z &gt; tolerance: mid_z = (low_z + hi_z) / 2 mid_p = calc_normal_cdf(mid_z) if mid_p &lt; p: low_z = mid_z else: hi_z = mid_z if show_steps: print(&quot; t&quot;.join(map(to_string, [low_z, mid_z, hi_z]))) return mid_z . def normal_upper_bound(probabilty: float, mu: float = 0, sigma: float = 1) -&gt; float: &quot;&quot;&quot;Return z for which P(Z &lt;= z) = probability&quot;&quot;&quot; return calc_inverse_normal_cdf(probabilty, mu, sigma) def normal_lower_bound(probabilty: float, mu: float = 0, sigma: float = 1) -&gt; float: &quot;&quot;&quot;Return z for which P(Z &gt;= z) = probability&quot;&quot;&quot; return calc_inverse_normal_cdf(1 - probabilty, mu, sigma) . def normal_two_sided_bounds(probability: float, mu: float = 0, sigma: float = 1) -&gt; float: if probability == 0: return 0, 0 tail_probability = (1 - probability) / 2 lower_bound = normal_upper_bound(tail_probability, mu, sigma) upper_bound = normal_lower_bound(tail_probability, mu, sigma) return lower_bound, upper_bound . Example 1 . We would like to know if a coin is &#39;fair&#39; or not (i.e. p=0.5) . We flip the coin 1000 times and observe 525 heads. . As we do not know the &#39;real&#39; probability, we need to rely on our observed results. . p_hat = 525 / 1000 mu = p_hat sigma = m.sqrt(p_hat * (1 - p_hat) / 1000) p_hat, mu, sigma . (0.525, 0.525, 0.015791611697353755) . While we cannot tell if this is really the probabiliy of heads, we can calculate the interval within which we expect the real probability fall with our chosen confidence (here: 95%) . confidence = 0.95 normal_two_sided_bounds(confidence, mu, sigma) . (0.4940490278129096, 0.5559509721870904) . As 0.5 is within the interval, we cannot reject the hypothesis that the coin is fair. . Example 2 . In another example we observe 540 heads in 1000 flips. . p_hat = 540 / 1000 mu = p_hat sigma = m.sqrt(p_hat * (1 - p_hat) / 1000) p_hat, mu, sigma . (0.54, 0.54, 0.015760710643876435) . Going through the same steps as above, we get that 0.5 falls outside our 95% confidence interval and therefore we reject the hypothesis that the coin is &#39;fair&#39;. . normal_two_sided_bounds(confidence, mu, sigma) . (0.5091095927295919, 0.5708904072704082) .",
            "url": "https://andrasnovoszath.com/2020/10/14/confidence-intervals.html",
            "relUrl": "/2020/10/14/confidence-intervals.html",
            "date": " • Oct 14, 2020"
        }
        
    
  
    
        ,"post28": {
            "title": "p-values with Python from scratch",
            "content": "Calculating p-values with python. . Libraries and helper functions . from typing import Tuple import math as m . def normal_probability_below(x: float, mu: float = 0, sigma: float = 1) -&gt; float: return (1 + m.erf((x - mu) / m.sqrt(2) / sigma)) / 2 . def normal_probability_above(lo: float, mu: float = 0, sigma: float = 1) -&gt; float: return 1 - normal_probability_below(lo, mu, sigma) . def normal_approximation_to_binomial(n: int, p: float) -&gt; Tuple[float, float]: mu = p * n sigma = m.sqrt(p * (1 - p) * n) return mu, sigma . Two-sided p-values . def two_sided_p_value(x: float, mu: float = 0, sigma: float = 1) -&gt; float: &quot;&quot;&quot;Return the probability of getting at least as extreme value as `x`, given that our values are from a normal distribution with `mu` mean and `sigma` std. &quot;&quot;&quot; # If x is greater than the mean return everything above x if x &gt;= mu: return 2 * normal_probability_above(x, mu, sigma) # If x is less than the mean than return everything below x else: return 2 * normal_probability_below(x, mu, sigma) . E.g. if our normal distribution has a mean of 500 and standard deviation of 15, the probabilty to get 530 or above is ~5.78% . mu_0, sigma_0 = normal_approximation_to_binomial (1000 , 0.5 ) mu_0, sigma_0 . (500.0, 15.811388300841896) . two_sided_p_value(529.5, mu_0, sigma_0) . 0.06207721579598835 . As the p-value is greater than 5%, so we don&#39;t reject the null hypothesis. . However, if we look at values beyond 32 distance from the mean, however, the probability will be less than 5%. . two_sided_p_value(531.5, mu_0, sigma_0) . 0.046345287837786575 . Validating with simulation . import random # Run 1000 simulations with 1000 binomial trials extreme_value_count = 0 for _ in range(1000): num_heads = sum(1 if random.random() &lt; 0.5 else 0 for _ in range(1000)) # Count the &#39;extreme&#39; values (i.e. beyond 30 distance from the mean) if num_heads &gt;= 530 or num_heads &lt;= 470: extreme_value_count += 1 extreme_value_count / 1000 # assert 610 &lt; extreme_value_count &lt; 630, f&quot;{extreme_value_count}&quot; . 0.07 . One-sided p-values . normal_probability_above(524.5, mu_0, sigma_0) . 0.06062885772582072 . normal_probability_above(526.5, mu_0, sigma_0) . 0.04686839508859242 .",
            "url": "https://andrasnovoszath.com/2020/10/13/p-values.html",
            "relUrl": "/2020/10/13/p-values.html",
            "date": " • Oct 13, 2020"
        }
        
    
  
    
        ,"post29": {
            "title": "Hypothesis test power from scratch with Python",
            "content": "Calculating power of hypothesis tests. . The code is from the Data Science from Scratch book. . Libraries and helper functions . from typing import Tuple import math as m . def calc_normal_cdf(x: float, mu: float = 0, sigma: float = 1) -&gt; float: return (1 + m.erf((x - mu) / m.sqrt(2) / sigma)) / 2 normal_probability_below = calc_normal_cdf . def normal_probability_between(lo: float, hi: float, mu: float = 0, sigma: float = 1) -&gt; float: return normal_probability_below(hi, mu, sigma) - normal_probability_below(lo, mu, sigma) . def calc_normal_cdf(x: float, mu: float = 0, sigma: float = 1) -&gt; float: return (1 + m.erf((x - mu) / m.sqrt(2) / sigma)) / 2 . def calc_inverse_normal_cdf(p: float, mu:float = 0, sigma: float = 1, tolerance: float = 1E-5, show_steps=False) -&gt; float: if p == 0: return -np.inf if p == 1: return np.inf # In case it is not a standard normal distribution, calculate the standard normal first and then rescale if mu != 0 or sigma != 1: return mu + sigma * calc_inverse_normal_cdf(p, tolerance=tolerance) low_z = -10 hi_z = 10 if show_steps: print(f&quot;{&#39;&#39;:&lt;19}&quot;.join([&#39;low_z&#39;, &#39;mid_z&#39;, &#39;hi_z&#39;]), &quot; n&quot;) while hi_z - low_z &gt; tolerance: mid_z = (low_z + hi_z) / 2 mid_p = calc_normal_cdf(mid_z) if mid_p &lt; p: low_z = mid_z else: hi_z = mid_z if show_steps: print(&quot; t&quot;.join(map(to_string, [low_z, mid_z, hi_z]))) return mid_z . def normal_upper_bound(probabilty: float, mu: float = 0, sigma: float = 1) -&gt; float: return calc_inverse_normal_cdf(probabilty, mu, sigma) . def normal_lower_bound(probabilty: float, mu: float = 0, sigma: float = 1) -&gt; float: return calc_inverse_normal_cdf(1 - probabilty, mu, sigma) . def normal_two_sided_bounds(probability: float, mu: float = 0, sigma: float = 1) -&gt; float: if probability == 0: return 0, 0 tail_probability = (1 - probability) / 2 lower_bound = normal_upper_bound(tail_probability, mu, sigma) upper_bound = normal_lower_bound(tail_probability, mu, sigma) return lower_bound, upper_bound . def normal_approximation_to_binomial(n: int, p: float) -&gt; Tuple[float, float]: mu = p * n sigma = m.sqrt(p * (1 - p) * n) return mu, sigma . Type 1 Error and Tolerance . Let&#39;s make our null hypothesis ($H_0$) that the probability of head is 0.5 . mu_0, sigma_0 = normal_approximation_to_binomial(1000, 0.5) mu_0, sigma_0 . (500.0, 15.811388300841896) . We define our tolerance at 5%. That is, we accept our model to produce &#39;type 1&#39; errors (false positive) in 5% of the time. With the coin flipping example, we expect to receive 5% of the results to fall outsied of our defined interval. . lo, hi = normal_two_sided_bounds(0.95, mu_0, sigma_0) lo, hi . (469.01026640487555, 530.9897335951244) . Type 2 Error and Power . At type 2 error we consider false negatives, that is, those cases where we fail to reject our null hypothesis even though we should. . Let&#39;s assume that contra $H_0$ the actual probability is 0.55. . mu_1, sigma_1 = normal_approximation_to_binomial(1000, 0.55) mu_1, sigma_1 . (550.0, 15.732132722552274) . In this case we get our Type 2 probability as the overlapping of the real distribution and the 95% probability region of $H_0$. In this particular case, in 11% of the cases we will wrongly fail to reject our null hypothesis. . type_2_probability = normal_probability_between(lo, hi, mu_1, sigma_1) type_2_probability . 0.1134519987046329 . The power of the test is then the probability of rightly rejecting the $H_0$ . power = 1 - type_2_probability power . 0.886548001295367 . Now, let&#39;s redefine our null hypothesis so that we expect the probability of head to be less than or equal to 0.5. . In this case we have a one-sided test. . hi = normal_upper_bound(0.95, mu_0, sigma_0) hi . 526.0073585242053 . Because this is a less strict hypothesis than our previus one, it has a smaller T2 probability and a greater power. . type_2_probability = normal_probability_below(hi, mu_1, sigma_1) type_2_probability . 0.06362051966928273 . power = 1 - type_2_probability power . 0.9363794803307173 .",
            "url": "https://andrasnovoszath.com/2020/10/12/hypothesis-test-power.html",
            "relUrl": "/2020/10/12/hypothesis-test-power.html",
            "date": " • Oct 12, 2020"
        }
        
    
  
    
        ,"post30": {
            "title": "Normal approximation of binomial distribution",
            "content": "from typing import Tuple import math as m . def normal_approximation_to_binomial(n: int, p: float) -&gt; Tuple[float, float]: mu = p * n sigma = m.sqrt(p * (1 - p) * n) return mu, sigma . The change of standard deviation with the incrase of number of trials. . for i in range(10): print(i, normal_approximation_to_binomial(10**i, 0.5)) . 0 (0.5, 0.5) 1 (5.0, 1.5811388300841898) 2 (50.0, 5.0) 3 (500.0, 15.811388300841896) 4 (5000.0, 50.0) 5 (50000.0, 158.11388300841898) 6 (500000.0, 500.0) 7 (5000000.0, 1581.1388300841897) 8 (50000000.0, 5000.0) 9 (500000000.0, 15811.388300841896) .",
            "url": "https://andrasnovoszath.com/2020/10/11/normal-approximation-binomial.html",
            "relUrl": "/2020/10/11/normal-approximation-binomial.html",
            "date": " • Oct 11, 2020"
        }
        
    
  
    
        ,"post31": {
            "title": "Probability interval boundaries with Python",
            "content": "In the previous post we calculated proabilities and z values with Python. Here we continue with finding the interval boundaries belonging to a particular tail or two-sided probabiltiy. . Libraries and helper functions . import math as m import numpy as np . def to_string(number: float, column_width: int = 20) -&gt; str: # return str(number).ljust(column_width) return f&quot;{str(number): &lt;{column_width}}&quot; . def calc_normal_cdf(x: float, mu: float = 0, sigma: float = 1) -&gt; float: return (1 + m.erf((x - mu) / m.sqrt(2) / sigma)) / 2 . def calc_inverse_normal_cdf(p: float, mu:float = 0, sigma: float = 1, tolerance: float = 1E-5, show_steps=False) -&gt; float: if p == 0: return -np.inf if p == 1: return np.inf # In case it is not a standard normal distribution, calculate the standard normal first and then rescale if mu != 0 or sigma != 1: return mu + sigma * calc_inverse_normal_cdf(p, tolerance=tolerance) low_z = -10 hi_z = 10 if show_steps: print(f&quot;{&#39;&#39;:&lt;19}&quot;.join([&#39;low_z&#39;, &#39;mid_z&#39;, &#39;hi_z&#39;]), &quot; n&quot;) while hi_z - low_z &gt; tolerance: mid_z = (low_z + hi_z) / 2 mid_p = calc_normal_cdf(mid_z) if mid_p &lt; p: low_z = mid_z else: hi_z = mid_z if show_steps: print(&quot; t&quot;.join(map(to_string, [low_z, mid_z, hi_z]))) return mid_z . Upper bound . def normal_upper_bound(probabilty: float, mu: float = 0, sigma: float = 1) -&gt; float: &quot;&quot;&quot;Return z for which P(Z &lt;= z) = probability&quot;&quot;&quot; return calc_inverse_normal_cdf(probabilty, mu, sigma) . def normal_lower_bound(probabilty: float, mu: float = 0, sigma: float = 1) -&gt; float: &quot;&quot;&quot;Return z for which P(Z &gt;= z) = probability&quot;&quot;&quot; return calc_inverse_normal_cdf(1 - probabilty, mu, sigma) . for i in np.arange(0, 1.1, .1): print(f&quot;{i:.1f}: {normal_upper_bound(i): &gt;10.4f}, {normal_lower_bound(i): &gt;10.4f}&quot;) . 0.0: -inf, inf 0.1: -1.2816, 1.2816 0.2: -0.8416, 0.8416 0.3: -0.5244, 0.5244 0.4: -0.2533, 0.2533 0.5: -0.0000, -0.0000 0.6: 0.2533, -0.2533 0.7: 0.5244, -0.5244 0.8: 0.8416, -0.8416 0.9: 1.2816, -1.2816 1.0: inf, -inf . Two sided bounds . Two variations . def normal_two_sided_bounds(probability: float, mu: float = 0, sigma: float = 1) -&gt; float: if probability == 0: return 0, 0 tail_probability = (1 - probability) / 2 lower_bound = normal_upper_bound(tail_probability, mu, sigma) upper_bound = normal_lower_bound(tail_probability, mu, sigma) return lower_bound, upper_bound . def normal_two_sided_bounds2(probability: float, mu: float = 0, sigma: float = 1) -&gt; float: if probability == 0: return 0, 0 if mu != 0: raise ValueError(&quot;Requires standard normal distribution&quot;) tail_probability = (1 - probability) / 2 z = normal_upper_bound(tail_probability, mu, sigma) return -z, z . By default they do not produce the same results at the extremes because of how the inverse cdf function defines the theoretical uppern and lower boundaries (as in these cases it should produce +/- infinity). . Accordingly, it is useful to define extreme values (i.e. probablities of 0 and 1) . for i in np.arange(0, 1.1, .1): print(f&quot;{i:.1f}: {normal_two_sided_bounds(i) == normal_two_sided_bounds2(i)}, {normal_two_sided_bounds(i)}, {normal_two_sided_bounds2(i)}&quot;) . 0.0: True, (0, 0), (0, 0) 0.1: False, (-0.12566566467285156, 0.12566566467285156), (0.12566566467285156, -0.12566566467285156) 0.2: False, (-0.2533435821533203, 0.2533435821533203), (0.2533435821533203, -0.2533435821533203) 0.3: False, (-0.3853130340576172, 0.3853130340576172), (0.3853130340576172, -0.3853130340576172) 0.4: False, (-0.5243968963623047, 0.5243968963623047), (0.5243968963623047, -0.5243968963623047) 0.5: False, (-0.6744861602783203, 0.6744861602783203), (0.6744861602783203, -0.6744861602783203) 0.6: False, (-0.8416271209716797, 0.8416271209716797), (0.8416271209716797, -0.8416271209716797) 0.7: False, (-1.0364246368408203, 1.0364246368408203), (1.0364246368408203, -1.0364246368408203) 0.8: False, (-1.2815570831298828, 1.2815570831298828), (1.2815570831298828, -1.2815570831298828) 0.9: False, (-1.6448497772216797, 1.6448497772216797), (1.6448497772216797, -1.6448497772216797) 1.0: False, (-inf, inf), (inf, -inf) .",
            "url": "https://andrasnovoszath.com/2020/10/10/probability-intervals-boundaries.html",
            "relUrl": "/2020/10/10/probability-intervals-boundaries.html",
            "date": " • Oct 10, 2020"
        }
        
    
  
    
        ,"post32": {
            "title": "Inverse normal cumulative distribution - finding $z$ values from probabilities with Python",
            "content": "Sometimes we would like to find the $z$ values belonging to a particular probability. For this we use an inverse cumulative distribution function. . Because the cumulative distribution function is strictly increasing and continuous, we can find the $z$ by narrowing down with binary bisecting the distance between high and low $z$ values. . The code is based upon the respective example from Data Science from Scratch. . Libraries, helper functions . import math as m import numpy as np . def calc_normal_cdf(x: float, mu: float = 0, sigma: float = 1) -&gt; float: return (1 + m.erf((x - mu) / m.sqrt(2) / sigma)) / 2 . def to_string(number: float, column_width: int = 20) -&gt; str: # return str(number).ljust(column_width) return f&quot;{str(number): &lt;{column_width}}&quot; . Inverse normal CDF . The inverse normal cumulative distribution function. . Takes a probability and optionaly a mean and standard deviation of the distribution, and a tolerance value. . If the distribution is not standard normal, it calculates $z$ for the standard normal one and then rescales the result . | Take a very low and a very high $z$ value . | While the distance between the high and low $z$ values are greater than our tolerance . a. Take their midpoint b. Calculate the probability belonging to that midpoint c. Based on the position of the probability belonging of the midpoint in respect to the probabilty for which we try to find out the $z$ value, assign the midpoint to the low or high $z$ value . | def calc_inverse_normal_cdf(p: float, mu:float = 0, sigma: float = 1, tolerance: float = 1E-5, show_steps=False) -&gt; float: # In case it is not a standard normal distribution, calculate the standard normal first and then rescale if mu != 0 or sigma != 1: return mu + sigma * calc_inverse_normal_cdf(p, tolerance=tolerance) low_z = -10 hi_z = 10 while hi_z - low_z &gt; tolerance: mid_z = (low_z + hi_z) / 2 mid_p = calc_normal_cdf(mid_z) if mid_p &lt; p: low_z = mid_z else: hi_z = mid_z if show_steps: print(&quot; t&quot;.join(map(to_string, [low_z, mid_z, hi_z]))) return mid_z . We can examine the steps as the $z$ closes down to its final value. . calc_inverse_normal_cdf(p=.3, show_steps=True) . -10 0.0 0.0 -5.0 -5.0 0.0 -2.5 -2.5 0.0 -1.25 -1.25 0.0 -0.625 -0.625 0.0 -0.625 -0.3125 -0.3125 -0.625 -0.46875 -0.46875 -0.546875 -0.546875 -0.46875 -0.546875 -0.5078125 -0.5078125 -0.52734375 -0.52734375 -0.5078125 -0.52734375 -0.517578125 -0.517578125 -0.52734375 -0.5224609375 -0.5224609375 -0.52490234375 -0.52490234375 -0.5224609375 -0.52490234375 -0.523681640625 -0.523681640625 -0.52490234375 -0.5242919921875 -0.5242919921875 -0.52459716796875 -0.52459716796875 -0.5242919921875 -0.524444580078125 -0.524444580078125 -0.5242919921875 -0.524444580078125 -0.5243682861328125 -0.5243682861328125 -0.5244064331054688 -0.5244064331054688 -0.5243682861328125 -0.5244064331054688 -0.5243873596191406 -0.5243873596191406 -0.5244064331054688 -0.5243968963623047 -0.5243968963623047 . -0.5243968963623047 . Finally, we get a number of reference values for some different probabilities . for i in np.arange(0, 1.1, .1): print(f&quot;{i:.1f}: {calc_inverse_normal_cdf(i) : &gt;20}&quot;) . 0.0: -9.999990463256836 0.1: -1.2815570831298828 0.2: -0.8416271209716797 0.3: -0.5243968963623047 0.4: -0.2533435821533203 0.5: -9.5367431640625e-06 0.6: 0.2533435821533203 0.7: 0.5243968963623047 0.8: 0.8416271209716797 0.9: 1.2815570831298828 1.0: 8.244009017944336 .",
            "url": "https://andrasnovoszath.com/2020/10/09/inverse-normal-cdf.html",
            "relUrl": "/2020/10/09/inverse-normal-cdf.html",
            "date": " • Oct 9, 2020"
        }
        
    
  
    
        ,"post33": {
            "title": "Loading MEG data with the `MNE` python package",
            "content": "I am interested in sleep data analysis, where one core informational source is neurophysiological data and in particular EEG data coming from polysomnigraphy. . MNE is a Python package designed particularly for this purpose. Below I outline how to install the package and how to load data with it. . Install . Installation is relatively simple . pip install mne . We can also test the status of the package with the info method. . import mne import os . mne.sys_info() . Platform: Linux-5.4.0-48-generic-x86_64-with-glibc2.10 Python: 3.8.5 (default, Sep 4 2020, 07:30:14) [GCC 7.3.0] Executable: /home/andras/anaconda3/envs/sleep_data/bin/python CPU: x86_64: 8 cores Memory: Unavailable (requires &#34;psutil&#34; package) mne: 0.21.0 numpy: 1.19.2 {blas=openblas, lapack=openblas} scipy: 1.5.2 matplotlib: 3.3.1 {backend=module://ipykernel.pylab.backend_inline} sklearn: Not found numba: Not found nibabel: Not found cupy: Not found pandas: 1.1.3 dipy: Not found mayavi: Not found pyvista: Not found vtk: Not found . Data Load . We can set the path to where we download the dataset. . mne.set_config(&#39;MNE_DATA&#39;, &#39;~/MNE_DATA&#39;) . Here we use the sample dataset which has its own module. . First we generate the path where we will download the sample dataset. . sample_data_folder = mne.datasets.sample.data_path() sample_data_folder . Archive exists (MNE-sample-data-processed.tar.gz), checking hash 12b75d1cb7df9dfb4ad73ed82f61094f. Decompressing the archive: /home/andras/mne_data/MNE-sample-data-processed.tar.gz (please be patient, this can take some time) Successfully extracted to: [&#39;/home/andras/mne_data/MNE-sample-data&#39;] . &#39;/home/andras/mne_data/MNE-sample-data&#39; . We specify the specific dataset we want to use. . sample_data_raw_file = os.path.join(sample_data_folder, &#39;MEG&#39;, &#39;sample&#39;,&#39;sample_audvis_filt-0-40_raw.fif&#39;) sample_data_raw_file . &#39;/home/andras/mne_data/MNE-sample-data/MEG/sample/sample_audvis_filt-0-40_raw.fif&#39; . Then, we download and open the data (here I have it already) . raw = mne.io.read_raw_fif(sample_data_raw_file) . Opening raw data file /home/andras/mne_data/MNE-sample-data/MEG/sample/sample_audvis_filt-0-40_raw.fif... Read a total of 4 projection items: PCA-v1 (1 x 102) idle PCA-v2 (1 x 102) idle PCA-v3 (1 x 102) idle Average EEG reference (1 x 60) idle Range : 6450 ... 48149 = 42.956 ... 320.665 secs Ready. . Data overview . Basic information . print(raw) . &lt;Raw | sample_audvis_filt-0-40_raw.fif, 376 x 41700 (277.7 s), ~3.6 MB, data not loaded&gt; . print(raw.info) . &lt;Info | 15 non-empty values bads: 2 items (MEG 2443, EEG 053) ch_names: MEG 0113, MEG 0112, MEG 0111, MEG 0122, MEG 0123, MEG 0121, MEG ... chs: 204 GRAD, 102 MAG, 9 STIM, 60 EEG, 1 EOG custom_ref_applied: False dev_head_t: MEG device -&gt; head transform dig: 146 items (3 Cardinal, 4 HPI, 61 EEG, 78 Extra) file_id: 4 items (dict) highpass: 0.1 Hz hpi_meas: 1 item (list) hpi_results: 1 item (list) lowpass: 40.0 Hz meas_date: 2002-12-03 19:01:10 UTC meas_id: 4 items (dict) nchan: 376 projs: PCA-v1: off, PCA-v2: off, PCA-v3: off, Average EEG reference: off sfreq: 150.2 Hz &gt; . And finally plotting the data . raw.plot_psd(fmax=50) raw.plot(duration=5, n_channels=30) . Effective window size : 13.639 (s) Effective window size : 13.639 (s) Effective window size : 13.639 (s) . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; 2020-10-08T19:00:12.708539 image/svg+xml Matplotlib v3.3.1, https://matplotlib.org/ &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; 2020-10-08T19:00:16.587806 image/svg+xml Matplotlib v3.3.1, https://matplotlib.org/ &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; 2020-10-08T19:00:19.027280 image/svg+xml Matplotlib v3.3.1, https://matplotlib.org/",
            "url": "https://andrasnovoszath.com/2020/10/08/mne-meg-eeg-data.html",
            "relUrl": "/2020/10/08/mne-meg-eeg-data.html",
            "date": " • Oct 8, 2020"
        }
        
    
  
    
        ,"post34": {
            "title": "Probability test",
            "content": "Libraries . from typing import Tuple import math as m import pandas as pd import numpy as np . Probability below a threshold . $ p = frac {1 + text{erf} z ( frac {x - mu} { sqrt{2} sigma} )} {2} $ . where . $ text{erf} z = frac {2} { sqrt{ pi}} int_0^z e^{-t^2} dt $ . is the error function. . def calc_normal_cdf(x: float, mu: float = 0, sigma: float = 1) -&gt; float: return (1 + m.erf((x - mu) / m.sqrt(2) / sigma)) / 2 . normal_probability_below = calc_normal_cdf . for i in [n / 10 for n in range(-10, 10 + 1, 1)]: print(&quot; t&quot;.join([str(i), f&quot;{normal_probability_below(i):.4f}&quot;])) . -1.0 0.1587 -0.9 0.1841 -0.8 0.2119 -0.7 0.2420 -0.6 0.2743 -0.5 0.3085 -0.4 0.3446 -0.3 0.3821 -0.2 0.4207 -0.1 0.4602 0.0 0.5000 0.1 0.5398 0.2 0.5793 0.3 0.6179 0.4 0.6554 0.5 0.6915 0.6 0.7257 0.7 0.7580 0.8 0.7881 0.9 0.8159 1.0 0.8413 . Probability above a threshold . def normal_probability_above(lo: float, mu: float = 0, sigma: float = 1) -&gt; float: return 1 - normal_probability_below(lo, mu, sigma) . for i in [n / 10 for n in range(-10, 10 + 1, 1)]: print(&quot; t&quot;.join([str(i), f&quot;{normal_probability_above(i):.4f}&quot;])) . -1.0 0.8413 -0.9 0.8159 -0.8 0.7881 -0.7 0.7580 -0.6 0.7257 -0.5 0.6915 -0.4 0.6554 -0.3 0.6179 -0.2 0.5793 -0.1 0.5398 0.0 0.5000 0.1 0.4602 0.2 0.4207 0.3 0.3821 0.4 0.3446 0.5 0.3085 0.6 0.2743 0.7 0.2420 0.8 0.2119 0.9 0.1841 1.0 0.1587 . Probability between thresholds . domain = np.arange(-10, 10 + 1, 2) / 10 domain . array([-1. , -0.8, -0.6, -0.4, -0.2, 0. , 0.2, 0.4, 0.6, 0.8, 1. ]) . def normal_probability_between(lo: float, hi: float, mu: float = 0, sigma: float = 1) -&gt; float: return normal_probability_below(hi, mu, sigma) - normal_probability_below(lo, mu, sigma) . probabilities_between = pd.DataFrame() for i in domain: for j in domain: probabilities_between.loc[i, j] = normal_probability_between(i, j) probabilities_between = pd.DataFrame(np.triu(probabilities_between), index=domain, columns=domain).replace(0, np.NaN) for i in domain: probabilities_between.loc[i, i] = 0 probabilities_between . -1.0 -0.8 -0.6 -0.4 -0.2 0.0 0.2 0.4 0.6 0.8 1.0 . -1.0 0.0 | 0.0532 | 0.115598 | 0.185923 | 0.262085 | 0.341345 | 0.420604 | 0.496766 | 0.567092 | 0.629489 | 0.682689 | . -0.8 NaN | 0.0000 | 0.062398 | 0.132723 | 0.208885 | 0.288145 | 0.367404 | 0.443566 | 0.513891 | 0.576289 | 0.629489 | . -0.6 NaN | NaN | 0.000000 | 0.070325 | 0.146487 | 0.225747 | 0.305007 | 0.381169 | 0.451494 | 0.513891 | 0.567092 | . -0.4 NaN | NaN | NaN | 0.000000 | 0.076162 | 0.155422 | 0.234681 | 0.310843 | 0.381169 | 0.443566 | 0.496766 | . -0.2 NaN | NaN | NaN | NaN | 0.000000 | 0.079260 | 0.158519 | 0.234681 | 0.305007 | 0.367404 | 0.420604 | . 0.0 NaN | NaN | NaN | NaN | NaN | 0.000000 | 0.079260 | 0.155422 | 0.225747 | 0.288145 | 0.341345 | . 0.2 NaN | NaN | NaN | NaN | NaN | NaN | 0.000000 | 0.076162 | 0.146487 | 0.208885 | 0.262085 | . 0.4 NaN | NaN | NaN | NaN | NaN | NaN | NaN | 0.000000 | 0.070325 | 0.132723 | 0.185923 | . 0.6 NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | 0.000000 | 0.062398 | 0.115598 | . 0.8 NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | 0.000000 | 0.053200 | . 1.0 NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | 0.000000 | . Probabilities outside a threshold range . def normal_probability_outside(lo: float, hi: float, mu: float = 0, sigma: float = 1) -&gt; float: return 1 - normal_probability_between(lo, hi, mu, sigma) . probabilities_outside = pd.DataFrame() for i in domain: for j in domain: probabilities_outside.loc[i, j] = normal_probability_outside(i, j) pd.DataFrame(np.triu(probabilities_outside), index=domain, columns=domain).replace(0, np.NaN) . -1.0 -0.8 -0.6 -0.4 -0.2 0.0 0.2 0.4 0.6 0.8 1.0 . -1.0 1.0 | 0.9468 | 0.884402 | 0.814077 | 0.737915 | 0.658655 | 0.579396 | 0.503234 | 0.432908 | 0.370511 | 0.317311 | . -0.8 NaN | 1.0000 | 0.937602 | 0.867277 | 0.791115 | 0.711855 | 0.632596 | 0.556434 | 0.486109 | 0.423711 | 0.370511 | . -0.6 NaN | NaN | 1.000000 | 0.929675 | 0.853513 | 0.774253 | 0.694993 | 0.618831 | 0.548506 | 0.486109 | 0.432908 | . -0.4 NaN | NaN | NaN | 1.000000 | 0.923838 | 0.844578 | 0.765319 | 0.689157 | 0.618831 | 0.556434 | 0.503234 | . -0.2 NaN | NaN | NaN | NaN | 1.000000 | 0.920740 | 0.841481 | 0.765319 | 0.694993 | 0.632596 | 0.579396 | . 0.0 NaN | NaN | NaN | NaN | NaN | 1.000000 | 0.920740 | 0.844578 | 0.774253 | 0.711855 | 0.658655 | . 0.2 NaN | NaN | NaN | NaN | NaN | NaN | 1.000000 | 0.923838 | 0.853513 | 0.791115 | 0.737915 | . 0.4 NaN | NaN | NaN | NaN | NaN | NaN | NaN | 1.000000 | 0.929675 | 0.867277 | 0.814077 | . 0.6 NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | 1.000000 | 0.937602 | 0.884402 | . 0.8 NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | 1.000000 | 0.946800 | . 1.0 NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | 1.000000 | .",
            "url": "https://andrasnovoszath.com/2020/10/07/probability-test.html",
            "relUrl": "/2020/10/07/probability-test.html",
            "date": " • Oct 7, 2020"
        }
        
    
  
    
        ,"post35": {
            "title": "Docker cheatsheet",
            "content": "Main ideas and docker file commands (based on this blog post). . Glossary . Image: blueprint to build | Container: instance of image | Dockerfile: recipe for image, a text file | Layer: modifications of the image | . Dockerfile commands . FROM ubuntu:18.04: build image upon ubuntu base image | LABEL maintainer=&quot;Firstname Lastname email: optioal metadata | ENV LANG=C.UTF-8 LC_ALL=C.UTF-8: Environment variables | RUN apt-get update &amp;&amp; apt-get install -y python-pip: shell commands to run | EXPOSE 7745: open a port (e.g. for jupyter). Publises only if used -p or -P with run | VOLUME /ds: mount externally mounted volumes (need to specify the mountpoint at container run/creation) | data inside this won&#39;t be saved, but outide of it will | . | WORKDIR /ds: starting point for relative file references | COPY hom* /mydir/: copies new files and add them to the container&#39;s filesystem at a destination path | CMD [&#39;/bin/bash&#39;]: run a bash shell to prevent closing the container | only the last CMD will be executed | . | . Build a Docker Image . docker build -t tutorial -f ./Dockerfile.gpu ./ . (This builds an image not a container) . -t tutorial: name or tag | -f ./Dockerfile.gpu: Dockerfile location | ./: context host directory, the relative path refrence point | . Build a Docker container from an Image . docker run -it --name container1 --net=host -v ~/docker_files/:/ds tutorial . -it: interactive run | --net=host: bind ports to host | -v /docker_files/:/ds: Mount host directory to you as a volume : the mount detination | tutorial: image name | . Container-related commands . Open a terminal iteractively in the container1 container. . docker exec -it container1 bash . Save current state of container as new image (username is e.g. for DockerHub) . docker commit container1 username/image2name:tagname . List running containers . docker ps -a -f status=running . List images . docker images . Push image to registry . docker push username/image2name:tagname .",
            "url": "https://andrasnovoszath.com/2020/10/06/docker-cheatsheet.html",
            "relUrl": "/2020/10/06/docker-cheatsheet.html",
            "date": " • Oct 6, 2020"
        }
        
    
  
    
        ,"post36": {
            "title": "A first view on Streamlit",
            "content": "I had a look into streamlit as anoter way to deploy a data science app. It seems really convenient to work with. . Unfortunately, I cannot use it on a static website, so I need to learn how to deploy it on a service with Docker. . Tutorial . I went through the getting started tutorial, below are the main steps. . We import streamlit as a separate package and simply run it in as script. . streamlit run first_app.py . This, by default, creates a local server where we can see the results. . import streamlit as st import numpy as np import pandas as pd import altair as alt . Streamlit tries to diplay everything, somehow similar how it happens in a jupyter notebook. . df = pd.DataFrame({&quot;first&quot;: [1, 2, 3, 4], &quot;second&quot;: [10, 20, 30, 40]}) df . first second . 0 1 | 10 | . 1 2 | 20 | . 2 3 | 30 | . 3 4 | 40 | . . chart_data = pd.DataFrame(np.random.randn(20, 3), columns=[&quot;a&quot;, &quot;b&quot;, &quot;c&quot;]) st.line_chart(chart_data) . &amp;lt;streamlit.delta_generator.DeltaGenerator at 0x7fd5941fdc70&amp;gt; . . map_data = pd.DataFrame( np.random.randn(1000, 2) / [50, 50] + [37.76, -122.4], columns=[&quot;lat&quot;, &quot;lon&quot;] ) st.map(map_data) . &amp;lt;streamlit.delta_generator.DeltaGenerator at 0x7fd5941fdc70&amp;gt; . . if st.checkbox(&#39;Show dataframe&#39;): chart_data = pd.DataFrame(np.random.randn(20, 3), columns = [&#39;a&#39;, &#39;b&#39;, &#39;c&#39;]) st.line_chart(chart_data) option = st.sidebar.selectbox(&quot;Which number do you like best?&quot;, df[&#39;first&#39;]) &#39;You selected &#39;, df.loc[df[&#39;first&#39;] == option, :] . (&amp;#39;You selected &amp;#39;, first second 0 1 10) . . import time &quot;Long computation...&quot; # Add a placeholder latest_iteration = st.empty() bar = st.progress(0) for i in range(100): # Update progress bar with each iteration latest_iteration.text(f&quot;Iteration {i + 1}&quot;) bar.progress(i + 1) time.sleep(0.1) &quot;...and done!&quot; . &amp;#39;...and done!&amp;#39; . .",
            "url": "https://andrasnovoszath.com/2020/10/05/streamlit_tutorial.html",
            "relUrl": "/2020/10/05/streamlit_tutorial.html",
            "date": " • Oct 5, 2020"
        }
        
    
  
    
        ,"post37": {
            "title": "Command line tips for quick navigation in the terminal",
            "content": "Things I wish I had knew earlier. . Most of the tips are from here . Basics . Navigation to any folder . &gt; cd /home/andras/Projects/data_science_blog/ /home/andras/Projects/data_science_blog . With the ~ we can jump to our home folder . &gt; cd ~ /home/andras . cd . We do not even need the ~ to navigate to our home folder. . &gt; cd / / &gt; cd /home/andras . Bash function . We can also create a bash function so we can spare even typing cd . Here is the function . # .bashrc function blog() { cd &quot;$HOME/Projects/data_science_blog&quot; } . And jump! . &gt; blog /home/andras/Projects/data_science_blog . Symbolic link . An another possible way is to create a symbolic link in our home folder. . &gt; ln -s ~/Projects/data_science_blog/ dsblog &gt; ls -l ~/dsblog lrwxrwxrwx 1 andras andras 40 okt 4 10:39 /home/andras/dsblog -&gt; /home/andras/Projects/data_science_blog/ . And jump! . &gt; cd ~/dsblog /home/andras/dsblog . However, because of the way symbolic links work, our ‘parent’ folder is still the home directory. . &gt; cd .. /home/andras . cd - . With - we can jump back to our previous folder . &gt; cd - /home/andras/Projects/data_science_blog . Path variable in .bashrc . A further option is to create a variable in bash . # .bashrc BLOG=/home/andras/Projects/data_science_blog . cd $BLOG /home/andras/Projects/data_science_blog . Create an alias . # .bashrc alias ds_blog=&quot;cd /home/andras/Projects/data_science_blog&quot; . &gt; ds_blog /home/andras/Projects/data_science_blog .",
            "url": "https://andrasnovoszath.com/terminal/navigation/tips/2020/10/04/terminal-navigation-tips.html",
            "relUrl": "/terminal/navigation/tips/2020/10/04/terminal-navigation-tips.html",
            "date": " • Oct 4, 2020"
        }
        
    
  
    
        ,"post38": {
            "title": "Central Limit Theorem on the binomial distribution from scratch with Python",
            "content": "Here I am playing with the Central Limit Theorem. . I am running bernoulli trials to generate binomial distributions. . I was interested in how the results approximate the normal distribution, so I labelled their status at particular iteration rounds and plotted the result. . The code is based upon the respective example from Data Science from Scratch. . Libraries . import random import pandas as pd import numpy as np import altair as alt import math as m from collections import Counter . alt.data_transformers.disable_max_rows() . DataTransformerRegistry.enable(&amp;#39;default&amp;#39;) . Parameters . p = .35 binomial_trials = 10000 total_rounds = 5000 step = 100 . Bernoulli Trials . def bernoulli_trial(p: float) -&gt; int: return 1 if random.random() &lt; p else 0 . Let&#39;s run the trials many times. . trials = [] for _ in range(binomial_trials): trials.append(bernoulli_trial(p)) print(&quot;First thousand trials:&quot;) print(&quot; &quot;.join([str(t) for t in trials[:1000]])) print(f&quot; nOnes: {sum([t == 1 for t in trials])}, Zeros: {sum([t == 0 for t in trials])}&quot;) print(&quot; nMean value of &#39;success&#39; (ones):&quot;, np.mean(trials)) . First thousand trials: 0 0 0 0 1 0 0 0 0 0 0 0 0 1 1 0 0 1 0 0 1 1 0 0 0 1 0 0 1 1 0 1 0 0 0 0 0 1 0 0 0 1 1 0 0 0 1 0 1 0 1 0 1 1 0 1 0 0 1 1 0 1 0 1 1 0 0 1 1 1 1 1 0 1 1 0 0 0 1 1 1 0 0 0 1 0 0 0 1 0 0 0 1 0 0 1 1 0 0 1 0 0 0 0 0 0 1 0 1 0 1 0 1 0 0 0 1 1 1 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 1 0 1 0 0 1 1 0 0 1 1 0 1 0 0 0 0 0 1 0 1 0 0 0 1 0 0 0 0 1 1 0 1 1 1 1 0 0 0 0 0 1 0 1 0 1 0 0 0 0 1 0 1 1 0 1 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 1 0 0 1 1 0 0 0 1 0 1 0 1 1 0 0 0 1 1 0 1 0 0 1 1 0 0 1 1 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 1 0 1 1 0 0 1 0 0 1 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 1 1 0 0 1 1 0 0 1 0 0 0 0 1 0 0 0 0 1 0 1 0 0 0 0 1 1 0 1 1 0 1 0 0 1 0 1 0 0 1 0 0 1 0 0 1 1 1 0 0 0 0 1 1 0 1 1 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 1 1 0 1 1 0 0 0 0 0 1 0 0 0 1 0 1 0 1 0 1 0 0 0 0 0 0 1 1 0 0 0 1 0 0 1 0 1 1 0 0 0 0 1 1 0 0 0 1 0 0 1 1 0 1 1 1 0 0 0 1 0 0 1 1 0 0 0 0 0 1 0 1 0 1 0 0 0 1 1 0 1 1 1 1 0 0 0 0 1 0 0 1 0 0 0 0 1 0 1 1 0 0 0 0 0 0 0 0 1 1 0 0 0 1 1 0 1 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 1 0 1 0 0 0 0 0 1 0 0 0 1 0 1 0 0 0 1 0 0 1 0 0 0 1 1 0 0 1 1 1 0 1 0 1 0 1 0 0 0 0 0 1 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 1 0 0 1 1 0 0 1 1 0 0 0 1 0 0 1 0 1 0 1 0 0 1 1 1 0 0 1 0 0 0 1 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 1 1 0 0 0 1 0 0 0 1 1 0 0 1 1 1 0 1 0 0 1 1 0 0 0 0 0 1 1 1 1 1 0 0 0 1 0 0 0 1 1 0 0 0 1 1 0 0 0 1 1 0 1 0 1 0 1 0 1 0 0 0 1 0 1 0 1 1 1 1 1 1 0 1 1 0 0 1 0 1 0 1 0 1 1 1 0 1 0 0 0 1 0 1 0 0 0 0 0 0 1 1 0 0 1 0 0 1 0 0 1 0 0 0 0 0 0 0 1 0 0 1 1 0 1 1 0 0 1 0 0 0 0 0 1 0 0 1 0 1 1 1 1 0 1 0 0 0 0 1 0 0 1 0 0 1 0 0 0 1 0 0 0 1 1 0 1 0 1 0 0 0 0 0 1 0 1 1 1 1 0 1 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 1 0 0 0 0 0 1 1 1 0 1 0 0 1 0 1 1 0 0 0 1 0 0 0 0 0 0 1 0 1 0 1 0 0 0 1 0 0 0 0 1 0 0 0 0 1 0 1 0 1 0 0 0 1 0 0 1 1 0 0 0 1 1 0 1 0 0 1 0 0 0 1 0 1 1 1 0 0 0 0 0 0 1 1 0 1 0 1 0 0 0 1 1 0 1 0 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 1 1 0 0 0 0 1 0 0 0 0 0 0 0 1 1 0 1 1 1 0 1 1 0 0 1 1 1 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 1 1 Ones: 3594, Zeros: 6406 Mean value of &amp;#39;success&amp;#39; (ones): 0.3594 . def binomial(n: int, p: float) -&gt; int: return sum(bernoulli_trial(p) for _ in range(n)) trials = binomial(binomial_trials, p) trials . 3441 . Iterating the binomial . def iterate_binomial(n, p, num_points): return [binomial(n, p) for _ in range(num_points)] simulations = iterate_binomial(binomial_trials, p, total_rounds) print(&quot;First hunded values:&quot;) print(&quot; &quot;.join([str(results) for results in simulations[:100]])) print(&quot; nMean:&quot;, np.mean(simulations), &quot;Standard deviation:&quot;, np.std(simulations)) . First hunded values: 3483 3596 3442 3470 3479 3570 3521 3512 3470 3499 3446 3495 3530 3527 3485 3537 3445 3538 3478 3542 3510 3444 3478 3384 3452 3566 3538 3548 3485 3425 3547 3456 3411 3515 3466 3454 3428 3485 3514 3524 3411 3530 3559 3401 3459 3513 3580 3533 3527 3502 3476 3465 3478 3475 3493 3537 3558 3499 3511 3496 3572 3555 3480 3541 3519 3477 3460 3496 3501 3524 3489 3499 3537 3527 3557 3508 3503 3530 3489 3497 3519 3509 3502 3484 3499 3488 3537 3567 3395 3469 3537 3440 3542 3493 3385 3539 3490 3504 3398 3555 Mean: 3500.871 Standard deviation: 47.232848304966744 . As we are interested in the intermediate states, we add new rounds iteratively and labeling them in the way. . def add_iterations(probability, trials, total_rounds, step): iteration_results = pd.DataFrame() for round in range(step, total_rounds + 1, step): simulations = iterate_binomial(trials, probability, round) counts = Counter(simulations) iteration_results = pd.concat( [ iteration_results, pd.DataFrame({&#39;count&#39;: counts.values(), &#39;value&#39;: counts.keys(), &#39;iterations&#39;: round}) ] ) return iteration_results iteration_results = add_iterations(p, binomial_trials, total_rounds, step) iteration_results . count value iterations . 0 1 | 3447 | 100 | . 1 1 | 3457 | 100 | . 2 1 | 3530 | 100 | . 3 1 | 3496 | 100 | . 4 1 | 3551 | 100 | . ... ... | ... | ... | . 265 2 | 3410 | 5000 | . 266 1 | 3663 | 5000 | . 267 1 | 3372 | 5000 | . 268 1 | 3644 | 5000 | . 269 1 | 3622 | 5000 | . 11949 rows × 3 columns . As we are interested in the total counts at each step we need to cumulatively add the value counts together over the iteration rounds. . def sum_counts(value_df): return value_df.sort_values(&#39;iterations&#39;)[&#39;count&#39;].cumsum() iteration_results = iteration_results.sort_values([&#39;value&#39;, &#39;iterations&#39;]) iteration_results[&#39;cumulative count&#39;] = iteration_results.groupby(&#39;value&#39;).apply(sum_counts).values iteration_results . count value iterations cumulative count . 233 1 | 3285 | 4400 | 1 | . 200 1 | 3286 | 4700 | 1 | . 161 1 | 3288 | 1900 | 1 | . 244 1 | 3313 | 2200 | 1 | . 156 1 | 3314 | 3200 | 1 | . ... ... | ... | ... | ... | . 130 1 | 3691 | 4700 | 1 | . 240 1 | 3692 | 4100 | 1 | . 144 1 | 3695 | 5000 | 1 | . 155 1 | 3700 | 2400 | 1 | . 271 1 | 3704 | 4500 | 1 | . 11949 rows × 4 columns . Normal distributions . We also generate a normal distribution for reference . def calc_normal_cdf(x: float, mu: float = 0, sigma: float = 1) -&gt; float: return (1 + m.erf((x - mu) / m.sqrt(2) / sigma)) /2 def calc_normal_pdf(x: float, mu: float = 0, sigma: float=1) -&gt; float: return m.exp(-(x-mu)**2 / (2 * sigma **2)) * 1/(m.sqrt(2 * m.pi) * sigma) . n = binomial_trials mu = p * n sigma = m.sqrt(n * p * (1 - p)) data = iteration_results.loc[iteration_results[&#39;iterations&#39;] == iteration_results[&#39;iterations&#39;].max(), &#39;value&#39;] xs = range(min(data), max(data) + 1) ys = [calc_normal_cdf(i + .5, mu, sigma) - calc_normal_cdf(i - .5, mu, sigma) for i in xs] normal_dist = pd.DataFrame({&#39;x&#39;: xs, &#39;y&#39;:ys}) normal_dist . x y . 0 3347 | 0.000049 | . 1 3348 | 0.000052 | . 2 3349 | 0.000056 | . 3 3350 | 0.000060 | . 4 3351 | 0.000064 | . ... ... | ... | . 344 3691 | 0.000003 | . 345 3692 | 0.000003 | . 346 3693 | 0.000002 | . 347 3694 | 0.000002 | . 348 3695 | 0.000002 | . 349 rows × 2 columns . Visualization . With the slider you can see the intermediate results at each iteration. . label = alt.selection_single( encodings=[&#39;x&#39;], on=&#39;mouseover&#39;, nearest=True, empty=&#39;none&#39; ) iteration_bounds = alt.binding_range(min=iteration_results[&#39;iterations&#39;].min(), max=iteration_results[&#39;iterations&#39;].max(), step=step) iteration_slider = alt.selection_single(bind=iteration_bounds, fields=[&#39;iterations&#39;], name=&#39;Iteration&#39;, init={&#39;iterations&#39;: iteration_results[&#39;iterations&#39;].median()}) bar_chart = alt.Chart(iteration_results).mark_bar().encode( alt.X(&#39;value:Q&#39;, scale=alt.Scale(domain=(iteration_results[&#39;value&#39;].min(), iteration_results[&#39;value&#39;].max()))), alt.Y(&#39;cumulative count:Q&#39;, scale=alt.Scale(domain=(iteration_results[&#39;cumulative count&#39;].min(), iteration_results[&#39;cumulative count&#39;].max()))), opacity=alt.value(0.75) ) line_chart = alt.Chart(normal_dist).mark_line().encode( alt.X(&#39;x&#39;), alt.Y(&#39;y&#39;,),color=alt.value(&#39;darkred&#39;) ) bar_chart = alt.layer( bar_chart.add_selection(iteration_slider).transform_filter(iteration_slider), bar_chart.mark_circle().encode(opacity=alt.condition(label, alt.value(1), alt.value(0))).add_selection(label).transform_filter(iteration_slider), bar_chart.mark_text(dx=5, dy=-5, align=&#39;left&#39;, strokeWidth=.5, stroke=&#39;black&#39;).encode(text=alt.Text(&#39;cumulative count:Q&#39;)).transform_filter(label).transform_filter(iteration_slider) ) alt.layer( bar_chart, line_chart, ).resolve_scale(y=&#39;independent&#39;).properties(width=800) .",
            "url": "https://andrasnovoszath.com/2020/10/03/central-limit-theorem.html",
            "relUrl": "/2020/10/03/central-limit-theorem.html",
            "date": " • Oct 3, 2020"
        }
        
    
  
    
        ,"post39": {
            "title": "Probabilty distributions in Python 'from scratch'",
            "content": "Here I write up some functions to generate probability univariate and normal probability distributions based on the book Data Science from Scratch . Libraries . import math as m import altair as alt import numpy as np import pandas as pd . alt.data_transformers.disable_max_rows() . DataTransformerRegistry.enable(&amp;#39;default&amp;#39;) . Uniform distribution . def uniform_pdf(x: float) -&gt; float: return 1 if 0 &lt;= x &lt; 1 else 0 . def uniform_cdf(x: float) -&gt; float: if x &lt; 0: return 0 elif x &lt; 1 : return x else: return 1 . Example values . print(&quot;x tpdf tcdf n&quot;) for x in [-2, 0, .2, .8, 1, 1.5]: print(f&quot;{x} t{uniform_pdf(x)} t{uniform_cdf(x)}&quot;) . x pdf cdf -2 0 0 0 1 0 0.2 1 0.2 0.8 1 0.8 1 0 1 1.5 0 1 . For plotting we generate both cdf and pdf values in a tidy format. . x = pd.Series(np.linspace(-1, 2, 1000)) uniform = pd.DataFrame( { &#39;x&#39;: x, &#39;pdf&#39;: x.apply(uniform_pdf), &#39;cdf&#39;: x.apply(uniform_cdf) } ).melt(id_vars=&#39;x&#39;) uniform . x variable value . 0 -1.000000 | pdf | 0.0 | . 1 -0.996997 | pdf | 0.0 | . 2 -0.993994 | pdf | 0.0 | . 3 -0.990991 | pdf | 0.0 | . 4 -0.987988 | pdf | 0.0 | . ... ... | ... | ... | . 1995 1.987988 | cdf | 1.0 | . 1996 1.990991 | cdf | 1.0 | . 1997 1.993994 | cdf | 1.0 | . 1998 1.996997 | cdf | 1.0 | . 1999 2.000000 | cdf | 1.0 | . 2000 rows × 3 columns . uniform.groupby(&#39;variable&#39;).describe().loc[:, (&#39;value&#39;, slice(None))].T . variable cdf pdf . value count 1000.000000 | 1000.000000 | . mean 0.500000 | 0.333000 | . std 0.441243 | 0.471522 | . min 0.000000 | 0.000000 | . 25% 0.000000 | 0.000000 | . 50% 0.500000 | 0.000000 | . 75% 1.000000 | 1.000000 | . max 1.000000 | 1.000000 | . chart = alt.Chart().mark_line().encode( alt.X(&#39;x:Q&#39;), alt.Y(&#39;value:Q&#39;), alt.Color(&#39;variable:N&#39;), ) label = alt.selection_single( encodings=[&#39;x&#39;], on=&#39;mouseover&#39;, nearest=True, empty=&#39;none&#39; ) alt.layer( chart, chart.mark_circle().encode(opacity=alt.condition(label, alt.value(1), alt.value(0))).add_selection(label), alt.Chart().mark_rule(color=&#39;darkgray&#39;).encode(alt.X(&#39;x:Q&#39;)).transform_filter(label), chart.mark_text(align=&#39;left&#39;, dx=5, dy=-5, strokeWidth=0.5).encode( text=alt.Text(&#39;value:Q&#39;, format=&#39;,.4f&#39;) ).transform_filter(label), data=uniform ).properties(width=600, title=&#39;Uniform PDF and CDF&#39;) . Normal PDF . def calc_normal_pdf(x: float, mu: float = 0, sigma: float=1) -&gt; float: return m.exp(-(x-mu)**2 / (2 * sigma **2)) * 1/(m.sqrt(2 * m.pi) * sigma) . x = pd.Series(np.linspace(-5, 5, 1000)) normal_pdf = pd.DataFrame( { &#39;x&#39;: x, &#39;mu=0, sigma=1&#39;: x.apply(calc_normal_pdf), &#39;mu=0, sigma=2&#39;: x.apply(lambda x: calc_normal_pdf(x, 0, 2)), &#39;mu=0, sigma=3&#39;: x.apply(lambda x: calc_normal_pdf(x, 0, 3)) } ).melt(id_vars=&#39;x&#39;) normal_pdf . x variable value . 0 -5.00000 | mu=0, sigma=1 | 0.000001 | . 1 -4.98999 | mu=0, sigma=1 | 0.000002 | . 2 -4.97998 | mu=0, sigma=1 | 0.000002 | . 3 -4.96997 | mu=0, sigma=1 | 0.000002 | . 4 -4.95996 | mu=0, sigma=1 | 0.000002 | . ... ... | ... | ... | . 2995 4.95996 | mu=0, sigma=3 | 0.033902 | . 2996 4.96997 | mu=0, sigma=3 | 0.033715 | . 2997 4.97998 | mu=0, sigma=3 | 0.033529 | . 2998 4.98999 | mu=0, sigma=3 | 0.033344 | . 2999 5.00000 | mu=0, sigma=3 | 0.033159 | . 3000 rows × 3 columns . normal_pdf.groupby(&#39;variable&#39;).describe()[&#39;value&#39;].T . variable mu=0, sigma=1 mu=0, sigma=2 mu=0, sigma=3 . count 1000.000000 | 1000.000000 | 1000.000000 | . mean 0.099900 | 0.098668 | 0.090385 | . std 0.134980 | 0.065984 | 0.032457 | . min 0.000001 | 0.008764 | 0.033159 | . 25% 0.000351 | 0.034353 | 0.060851 | . 50% 0.017420 | 0.091182 | 0.093905 | . 75% 0.181794 | 0.163888 | 0.121860 | . max 0.398937 | 0.199471 | 0.132981 | . label = alt.selection_single( encodings=[&#39;x&#39;], on=&#39;mouseover&#39;, nearest=True, empty=&#39;none&#39; ) chart = alt.Chart().mark_line().encode( alt.X(&#39;x:Q&#39;), alt.Y(&#39;value:Q&#39;), alt.Color(&#39;variable:N&#39;) ) alt.layer( chart, chart.mark_circle().encode(opacity=alt.condition(label, alt.value(1), alt.value(0))).add_selection(label), alt.Chart().mark_rule(color=&#39;darkgray&#39;).encode(alt.X(&#39;x:Q&#39;)).transform_filter(label), chart.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode(text=alt.Text(&#39;value:Q&#39;, format=&#39;,.6f&#39;)).transform_filter(label), # tooltip=alt.Tooltip(&#39;value:Q&#39;), data=normal_pdf ).properties(width=600, title=&quot;Normal PDF&quot;) . Normal CDF . Finally, we also calculate and plot the CDF or normal distribution. . def calc_normal_cdf(x: float, mu: float = 0, sigma: float = 1) -&gt; float: return (1 + m.erf((x - mu) / m.sqrt(2) / sigma)) /2 . normal_cdf = pd.DataFrame( { &#39;x&#39;: x, &#39;mu=0, sigma=1&#39;: x.apply(calc_normal_cdf), &#39;mu=0, sigma=2&#39;: x.apply(lambda x: calc_normal_cdf(x, 0, 2)), &#39;mu=0, sigma=3&#39;: x.apply(lambda x: calc_normal_cdf(x, 0, 3)), } ).melt(id_vars=&#39;x&#39;) normal_cdf . x variable value . 0 -5.00000 | mu=0, sigma=1 | 2.866516e-07 | . 1 -4.98999 | mu=0, sigma=1 | 3.019121e-07 | . 2 -4.97998 | mu=0, sigma=1 | 3.179543e-07 | . 3 -4.96997 | mu=0, sigma=1 | 3.348164e-07 | . 4 -4.95996 | mu=0, sigma=1 | 3.525386e-07 | . ... ... | ... | ... | . 2995 4.95996 | mu=0, sigma=3 | 9.508671e-01 | . 2996 4.96997 | mu=0, sigma=3 | 9.512055e-01 | . 2997 4.97998 | mu=0, sigma=3 | 9.515421e-01 | . 2998 4.98999 | mu=0, sigma=3 | 9.518768e-01 | . 2999 5.00000 | mu=0, sigma=3 | 9.522096e-01 | . 3000 rows × 3 columns . normal_cdf.describe()[&#39;value&#39;].T . count 3.000000e+03 mean 5.000000e-01 std 3.760737e-01 min 2.866516e-07 25% 1.055739e-01 50% 5.000000e-01 75% 8.944261e-01 max 9.999997e-01 Name: value, dtype: float64 . label = alt.selection_single( encodings=[&#39;x&#39;], on=&#39;mouseover&#39;, nearest=True, empty=&#39;none&#39; ) chart = alt.Chart().mark_line().encode( alt.X(&#39;x:Q&#39;), alt.Y(&#39;value:Q&#39;), alt.Color(&#39;variable:N&#39;) ) alt.layer( chart, chart.mark_circle().encode(opacity=alt.condition(label, alt.value(1), alt.value(0))).add_selection(label), alt.Chart().mark_rule(color=&#39;darkgray&#39;).encode(alt.X(&#39;x:Q&#39;)).transform_filter(label), chart.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode(text=alt.Text(&#39;value:Q&#39;, format=&#39;,.6f&#39;)).transform_filter(label), # tooltip=alt.Tooltip(&#39;value:Q&#39;), data=normal_cdf ).properties(width=600, title=&quot;Normal CDFs&quot;) .",
            "url": "https://andrasnovoszath.com/2020/10/02/probability-distributions.html",
            "relUrl": "/2020/10/02/probability-distributions.html",
            "date": " • Oct 2, 2020"
        }
        
    
  
    
        ,"post40": {
            "title": "Computational checking of the girl/boy probability problem",
            "content": "Here I replay the classical two child problem and code it up in Python. . In a nutshell, the solution changes depending on if and how we differentiate between the children (for example, whether we talk about &#39;older&#39; or &#39;younger&#39; child or just refering to them as &#39;either&#39;). . I also chart the solutions while I run the simulation to see how the probabilites tend to converge to their values. . import enum, random . class Kid(enum.Enum): Boy = 0 Girl = 1 . def random_kid() -&gt; Kid: return random.choice([Kid.Boy, Kid.Girl]) . random.seed(42) . both_girls = 0 older_girl = 0 either_girl = 0 results = [] for _ in range(1000): younger = random_kid() older = random_kid() if older == Kid.Girl: older_girl += 1 if older == Kid.Girl and younger == Kid.Girl: both_girls += 1 if older == Kid.Girl or younger == Kid.Girl: either_girl += 1 try: p_both_older = both_girls / older_girl except ZeroDivisionError: p_both_older = 0 try: p_both_either = both_girls / either_girl except ZeroDivisionError: p_both_either = 0 results.append([younger.name, older.name, both_girls, older_girl, either_girl, p_both_either, p_both_older]) . import altair as alt import pandas as pd . df_results = pd.DataFrame(results, columns=[&#39;younger&#39;, &#39;older&#39;, &#39;both girls&#39;, &#39;older girl&#39;, &#39;either girl&#39;, &#39;P(Both|Either)&#39;, &#39;P(Both|Older)&#39;]).reset_index() . df_results . index younger older both girls older girl either girl P(Both|Either) P(Both|Older) . 0 0 | Boy | Boy | 0 | 0 | 0 | 0.000000 | 0.000000 | . 1 1 | Girl | Boy | 0 | 0 | 1 | 0.000000 | 0.000000 | . 2 2 | Boy | Boy | 0 | 0 | 1 | 0.000000 | 0.000000 | . 3 3 | Boy | Boy | 0 | 0 | 1 | 0.000000 | 0.000000 | . 4 4 | Girl | Boy | 0 | 0 | 2 | 0.000000 | 0.000000 | . ... ... | ... | ... | ... | ... | ... | ... | ... | . 995 995 | Girl | Boy | 263 | 513 | 763 | 0.344692 | 0.512671 | . 996 996 | Girl | Girl | 264 | 514 | 764 | 0.345550 | 0.513619 | . 997 997 | Girl | Girl | 265 | 515 | 765 | 0.346405 | 0.514563 | . 998 998 | Girl | Boy | 265 | 515 | 766 | 0.345953 | 0.514563 | . 999 999 | Girl | Girl | 266 | 516 | 767 | 0.346806 | 0.515504 | . 1000 rows × 8 columns . to_plot = df_results.melt(id_vars=&#39;index&#39;) to_plot.loc[to_plot[&#39;variable&#39;].isin([&#39;both girls&#39;, &#39;older girl&#39;, &#39;either girl&#39;]), &#39;type&#39;] = &#39;count&#39; to_plot.loc[to_plot[&#39;variable&#39;].isin([&#39;P(Both|Either)&#39;, &#39;P(Both|Older)&#39;]), &#39;type&#39;] = &#39;probability&#39; to_plot . index variable value type . 0 0 | younger | Boy | NaN | . 1 1 | younger | Girl | NaN | . 2 2 | younger | Boy | NaN | . 3 3 | younger | Boy | NaN | . 4 4 | younger | Girl | NaN | . ... ... | ... | ... | ... | . 6995 995 | P(Both|Older) | 0.512671 | probability | . 6996 996 | P(Both|Older) | 0.513619 | probability | . 6997 997 | P(Both|Older) | 0.514563 | probability | . 6998 998 | P(Both|Older) | 0.514563 | probability | . 6999 999 | P(Both|Older) | 0.515504 | probability | . 7000 rows × 4 columns . chart = alt.Chart(to_plot).encode(alt.X(&#39;index:Q&#39;)) label = alt.selection_single(encodings=[&#39;x&#39;], on=&#39;mouseover&#39;, nearest=True, empty=&#39;none&#39;) count_chart = alt.Chart(data=to_plot[to_plot[&#39;type&#39;] == &#39;count&#39;]).mark_line().encode( alt.X(&#39;index:Q&#39;), alt.Y(&#39;value:Q&#39;), color=alt.Color(&#39;variable:N&#39;), ) probablity_chart = alt.Chart(to_plot[to_plot[&#39;type&#39;] == &#39;probability&#39;]).mark_line().encode( alt.X(&#39;index:Q&#39;), alt.Y(&#39;value:Q&#39;), color=alt.Color(&#39;variable:N&#39;) ) values_chart = alt.layer( probablity_chart.add_selection(label), probablity_chart.mark_rule(color=&#39;gray&#39;).encode(alt.X(&#39;index:Q&#39;)).transform_filter(label), count_chart, ).resolve_scale(y=&#39;independent&#39;).properties(width=600) values_chart .",
            "url": "https://andrasnovoszath.com/2020/10/01/boy-girl-probability.html",
            "relUrl": "/2020/10/01/boy-girl-probability.html",
            "date": " • Oct 1, 2020"
        }
        
    
  
    
        ,"post41": {
            "title": "Correlation and covariance from scratch",
            "content": "In this post we examine covariance and a correlation a bit closer. . We will use them to examine the relationship between Ethereum transaction value and gas price. . Again, most of the time, we break down the steps into standard Python data types and operations (i.e. we use numpy mostly for verification of our results). . Libraries and data load . We pull the data from Google&#39;s public datasets with BigQuery, use pandas and numpy to manipulate it, and altair to plot their relationship. . import os os.environ[&quot;GOOGLE_APPLICATION_CREDENTIALS&quot;]=os.path.expanduser(&quot;~/.credentials/Notebook bigquery-c422e406404b.json&quot;) from google.cloud import bigquery client = bigquery.Client() import altair as alt alt.data_transformers.disable_max_rows() import numpy as np import pandas as pd . query =&quot;&quot;&quot; SELECT EXTRACT(DATE FROM block_timestamp) AS date, AVG(value) AS value, AVG(gas_price) AS gas_price, FROM `bigquery-public-data.ethereum_blockchain.transactions` WHERE EXTRACT(YEAR FROM block_timestamp) = 2019 GROUP BY date ORDER BY date &quot;&quot;&quot; . transactions = client.query(query).to_dataframe(dtypes={&#39;value&#39;: float, &#39;gas_price&#39;: float}, date_as_object=False) transactions.head() . date value gas_price . 0 2019-01-01 | 3.719103e+18 | 1.431514e+10 | . 1 2019-01-02 | 4.649915e+18 | 1.349952e+10 | . 2 2019-01-03 | 4.188781e+18 | 1.269504e+10 | . 3 2019-01-04 | 6.958368e+18 | 1.418197e+10 | . 4 2019-01-05 | 8.167590e+18 | 2.410475e+10 | . There are a few days when the gas prices were outstandingly high so we remove values beyond three standard deviation from the mean. . Outliers . labelx = alt.selection_single( encodings=[&#39;x&#39;], on=&#39;mouseover&#39;, empty=&#39;none&#39; ) labely = alt.selection_single( encodings=[&#39;y&#39;], on=&#39;mouseover&#39;, empty=&#39;none&#39; ) ruler = alt.Chart().mark_rule(color=&#39;darkgray&#39;) chart = alt.Chart().mark_point().encode( alt.X(&#39;value&#39;, axis=alt.Axis(format=(&#39;,.2e&#39;))), alt.Y(&#39;gas_price&#39;, axis=alt.Axis(format=(&#39;,.2e&#39;))), alt.Tooltip([&#39;value&#39;, &#39;gas_price&#39;, &#39;date&#39;]) ).properties(width=600, height=400, title=&#39;Trasaction values and gas prices&#39;).add_selection(labelx).add_selection(labely) alt.layer( chart, ruler.encode(x=&#39;value:Q&#39;).transform_filter(labelx), ruler.encode(y=&#39;gas_price:Q&#39;).transform_filter(labely), data=transactions ).interactive() . transactions = transactions[~(transactions[&#39;gas_price&#39;] &gt;= transactions[&#39;gas_price&#39;].mean() + 3 * transactions[&#39;gas_price&#39;].std())] . values = transactions[&#39;value&#39;] gas_prices = transactions[&#39;gas_price&#39;] . As we emphasize standard operations, we use a few helper functions in the steps leading to covariance and correlation. . Helper functions . from typing import Union, List Vector = List[float] . def dot(vector1: Vector, vector2: Vector) -&gt; float: assert len(vector1) == len(vector2) return sum(v1 * v2 for v1, v2 in zip(vector1, vector2)) assert dot([1, 2, 3], [4, 5, 6]) == 32 def mean(x: Vector) -&gt; float: return sum(x) / len(x) assert mean([1, 2, 3, 4]) == 2.5 def de_mean(xs: Vector) -&gt; Vector: x_mean = mean(xs) return [x - x_mean for x in xs] assert de_mean([4, 5, 6, 7, 8]) == [-2, -1, 0, 1, 2] def sum_of_squares(xs: Vector) -&gt; float: return dot(xs, xs) assert sum_of_squares([1, 2, 3]) == 14 def variance(xs: Vector) -&gt; float: return sum_of_squares(de_mean(xs)) / (len(xs) - 1) assert variance([1, 2, 3]) == 1 import math as m def standard_deviation(xs: Vector): return m.sqrt(variance(xs)) assert standard_deviation([4, 5, 6]) == 1 . Covariance looks at the degree two variables &#39;move together&#39;. . For this, first, it multiplies the variables&#39; deviation from their respective means. This produces a series of values which are very high for those observations where both variables deviate a lot. Furthermore, when the two variables deviate to the same direction these values are positive, otherwise they are negative. . Then, it calculates the mean of these multiplied deviation values. However, because we are calculating the sample covariance, we divide their sum by $n + 1$ (where $n$ is the number of observations) . $ text{Cov} = frac { sum_{i=1}^n (x- bar{x}) (y- bar{y})} {n - 1} $ . Covariance . def covariance(xs: Vector, ys: Vector) -&gt; float: assert len(xs) == len(ys) return dot(de_mean(xs), de_mean(ys)) / (len(xs) - 1) assert covariance([1, 2, 3], [4, 5, 6]) == 1 . There is also an alternate way to calculate covariance, using the variables&#39; expected values (which here are the means): . $ text{Cov} = E[ vec{x} vec{y}] - E[ vec{x}]E[ vec{y}] $ . This is a much simpler version. However, again, as we are dealing with sample data, so we need to adjust for that: . $ text{Cov}_s = frac {n} {n - 1} (E[ vec{x} vec{y}] - E[ vec{x}]E[ vec{y}]) $ . covariance(values, gas_prices) . 1.5856518696847875e+26 . def covariance_2(xs: Vector, ys: Vector) -&gt; float: xsys = [x * y for x, y in zip(xs, ys)] return (mean(xsys) - mean(xs) * mean(ys)) * len(xs) / (len(xs) - 1) assert np.isclose(covariance_2([1, 2, 3], [4, 5, 6]), 1) . We also verify our method with numpy. . np.cov(values, gas_prices)[0, 1] . 1.5856518696847882e+26 . covariance_2(values, gas_prices) . 1.585651869684888e+26 . Because the value of covariance really depends on the units of the variables, it is often hard to interpret and also to compare it with other covariences. . This is why correlation is an often preferred method as it adjusts the covariance by the variables&#39; standard deviation values. As a result, it bounds the end result into the $[-1, 1]$ domain making it comparable with other correlation values. . $ text{Corr(x, y)} = frac { text{Cov(x, y)} } { text{Std(x)} text{Std(y)}} $ . Correlation . correlation(values, gas_prices) . 0.035069533929694634 . Finally, we verify the result with numpy. . values.corr(gas_prices) . 0.03506953392969465 . def correlation(xs: Vector, ys: Vector) -&gt; float: return covariance(xs, ys) / (standard_deviation(xs) * standard_deviation(ys)) assert np.isclose(correlation([.1, .2, .3], [400, 500, 600]), 1) .",
            "url": "https://andrasnovoszath.com/2020/09/30/correlation.html",
            "relUrl": "/2020/09/30/correlation.html",
            "date": " • Sep 30, 2020"
        }
        
    
  
    
        ,"post42": {
            "title": "Matrix algebra 'from scratch' with type annotations",
            "content": "This is the continuation of yesterday&#39;s practice, but now with Matrix operations. . from typing import List, Tuple, Callable . Vector = List[float] Matrix = List[List[float]] . A = [[1, 2, 3], [4, 5, 6]] B = [[1, 2], [3, 4], [5, 6]] . Shape . def shape(A: Matrix) -&gt; Tuple[int, int]: number_of_rows = len(A) number_of_columns = len(A[0]) if A else 0 return number_of_rows, number_of_columns assert shape([[1, 2, 3],[4, 5, 6]]) == (2, 3) . Get rows and columns . def get_row(A: Matrix, i: int) -&gt; Vector: return A[i] def get_columns(A: Matrix, i: int) -&gt; Vector: return [row[i] for row in A] assert get_columns([[1, 2, 3],[4, 5, 6]], 1) == [2, 5] . Create a new matrix from a function of $(i, j)$ . def make_matrix( number_of_rows: int, number_of_columns: int, entry_function: Callable[[int, int], float] ) -&gt; Matrix: return [ [entry_function(i, j) for j in range(number_of_columns)] for i in range(number_of_rows) ] assert make_matrix(3, 2, lambda i, j: i + j) == [[0, 1], [1, 2], [2, 3]] . def identity_matrix(n: int) -&gt; Matrix: return make_matrix(n, n, lambda i, j: 1 if i == j else 0) assert identity_matrix(3) == [[1, 0, 0], [0, 1, 0], [0, 0, 1]] . identity_matrix(5) . [[1, 0, 0, 0, 0], [0, 1, 0, 0, 0], [0, 0, 1, 0, 0], [0, 0, 0, 1, 0], [0, 0, 0, 0, 1]] .",
            "url": "https://andrasnovoszath.com/2020/09/29/matrix-algebra.html",
            "relUrl": "/2020/09/29/matrix-algebra.html",
            "date": " • Sep 29, 2020"
        }
        
    
  
    
        ,"post43": {
            "title": "Vector Algebra 'from scratch" with type annotations",
            "content": "I looked into Joel Grus&#39; Data science from Scratch and found his approach really nice as he actively uses type annotations and assertions in his fuctions. . So, below I practice vector operation functions from the book. . from typing import List Vector = List[float] Vector . typing.List[float] . Vector addition and subtraction . $ vec{v} + vec{w} = begin{bmatrix} v_1 + w_1 ldots v_n + w_n end{bmatrix} $ . def add(vector1: Vector, vector2: Vector) -&gt; Vector: assert len(vector1) == len(vector2) return [v1 + v2 for v1, v2 in zip(vector1, vector2)] assert add([1, 2, 3], [4, 5, 6]) == [5, 7, 9] . $ vec{w} - vec{v} = begin{bmatrix} w_1 - v_1 ldots w_n - v_n end{bmatrix} $ . def subtract(vector1: Vector, vector2:Vector) -&gt; Vector: assert len(vector1) == len(vector2) return [v1 - v2 for v1, v2 in zip(vector1, vector2)] assert subtract([1, 2, 3], [4, 5, 6]) == [-3, -3, -3] . Vector sum . $ vec{v} + ldots + vec{w} = begin{bmatrix} v_1 + ldots + w_1 ldots v_n + ldots + w_n end{bmatrix} $ . def vector_sum(vectors: List[Vector]) -&gt; Vector: assert vectors vector_length = len(vectors[0]) assert all(len(v) == vector_length for v in vectors) sums = [0] * vector_length for vector in vectors: sums = add(sums, vector) return sums assert vector_sum([[1, 2, 3], [4, 5, 6], [7, 8, 9]]) == [12, 15, 18] . Scalar multiplication . $ c cdot vec{v} = begin{bmatrix} c cdot v_0 ldots c cdot v_n end{bmatrix} $ . def scalar_multiply(c: float, vector: Vector) -&gt; Vector: return [c * v for v in vector] assert scalar_multiply(4, [1, 2, 3]) == [4, 8, 12] . Vector mean . $ text{vector mean}( vec{w}, ldots, vec{v}) = begin{bmatrix} frac {v_1 + ldots + w_1}{n} ldots frac{v_n + ldots + w_n} {n} end{bmatrix} $ . def vector_mean(vectors: List[Vector]) -&gt; Vector: n = len(vectors) return scalar_multiply(1/n, vector_sum(vectors)) assert vector_mean([[1, 2], [3, 4], [5, 6]]) == [3, 4] . Dot product . $ vec{v} cdot vec{w} = sum_{i=1}^{n} v_1 w_1 + ldots + v_n w_n $ . def dot(vector1: Vector, vector2: Vector) -&gt; float: assert len(vector1) == len(vector2) return sum(v1 * v2 for v1, v2 in zip(vector1, vector2)) assert dot([1, 2, 3], [4, 5, 6]) == 32 . Vector sum of squares . $ vec{v} cdot vec{v} = sum_{i=1}^n v_1^2 + ldots + v_n^2 $ . def sum_of_squares(v: Vector) -&gt; Vector: return dot(v, v) assert sum_of_squares([1, 2, 3]) == 14 . Magnitude . $ || vec{v} || = sqrt {v_1^2 + ldots + v_n^2} $ . import math as m def magnitude(v: Vector) -&gt; Vector: return m.sqrt(sum_of_squares(v)) assert magnitude([1, 2, 3]) == m.sqrt(14) . Distance of two vectors . $ d = sqrt { (v_1 - w_1)^2 + ldots + (v_n - w_n)^2 } $ . def distance(vector1: Vector, vector2: Vector) -&gt; Vector: return magnitude(subtract(vector1, vector2)) .",
            "url": "https://andrasnovoszath.com/2020/09/28/vector-algebra.html",
            "relUrl": "/2020/09/28/vector-algebra.html",
            "date": " • Sep 28, 2020"
        }
        
    
  
    
        ,"post44": {
            "title": "Linear regression 'from scratch' to predict Ethereum gas prices from transaction values",
            "content": "In this post I go through the main steps of how to calculate a simple univariate linear regression model. . For the example I try to predict Ethereum daily average gas prices from daily average transaction values. I pull the data from the public Google data base with BigQuery. . Code and inspiration are based on Jason Brownlee&#39;s &quot;Machine Learning Algorithms from Scratch&quot; book . Libraries and data load . import os os.environ[&quot;GOOGLE_APPLICATION_CREDENTIALS&quot;]=os.path.expanduser(&quot;~/.credentials/Notebook bigquery-c422e406404b.json&quot;) . from google.cloud import bigquery client = bigquery.Client() . import altair as alt alt.data_transformers.disable_max_rows() . DataTransformerRegistry.enable(&#39;default&#39;) . query =&quot;&quot;&quot; SELECT EXTRACT(DATE FROM block_timestamp) AS date, AVG(value) AS value, AVG(gas_price) AS gas_price, FROM `bigquery-public-data.ethereum_blockchain.transactions` WHERE EXTRACT(YEAR FROM block_timestamp) = 2019 GROUP BY date ORDER BY date &quot;&quot;&quot; . values = client.query(query).to_dataframe(dtypes={&#39;value&#39;: float, &#39;gas_price&#39;: float}, date_as_object=False) values.head() . date value gas_price . 0 2019-01-01 | 3.719103e+18 | 1.431514e+10 | . 1 2019-01-02 | 4.649915e+18 | 1.349952e+10 | . 2 2019-01-03 | 4.188781e+18 | 1.269504e+10 | . 3 2019-01-04 | 6.958368e+18 | 1.418197e+10 | . 4 2019-01-05 | 8.167590e+18 | 2.410475e+10 | . Calculating linear regression . Linear regression makes predictions with the help of linear coefficient. For an univariate case, this is expressed as: . $ hat{y} = b_0 + b_1 x $ . Coefficients . We can estimate the $b_0$ and $b_1$ coeffients in the following ways: . $ b_1 = frac { sum_{i = 1}^{n} (x_i - bar{x} ) (y_i - bar{y} ) } { sum_{i=1}^{n} (x_i - bar{x})^2 } $ . $ b_0 = bar{y} - b_1 bar{x} $ . Where . $x$: the predictor variable | $y$: the variable to predict | $ bar{x} $ and $ bar{y}$ are their respective means | . Covariance and variance . We can also can get the $b_1$ coefficient from the variance and covariance: . Covariance: $ text{Cov}(x, y) = frac { sum_{i = 1}^{n} (x_i - bar{x} ) (y_i - bar{y} ) } { n } $ | Variance: $ delta^2 = frac { { sum_{i=1}^{n} (x_i - bar{x})^2 } } { n } $ | . From these, we can get . $ b_1 = frac { text{Cov} } { delta^2 } $ . Calculation steps . Accordingly, we need to calculate the following metrics: . Variable means for both $x$ and $y$ | Their deviations from the mean | Covariance of $x$ and $y$ | Variance of $x$ | The $b_1$ coeffcient | The $b_0$ coefficient | $ hat{y}$, that is, the predictions | Means . values.mean() . value 3.173648e+18 gas_price 1.616874e+10 dtype: float64 . Deviations from the mean . def deviation(array): return array - array.mean() . deviation(values[&#39;value&#39;]) . 0 5.454550e+17 1 1.476267e+18 2 1.015132e+18 3 3.784720e+18 4 4.993942e+18 ... 360 -1.004019e+18 361 -1.259703e+18 362 -1.050454e+18 363 -6.660476e+17 364 5.699485e+17 Name: value, Length: 365, dtype: float64 . Covariance . def covariance(arr1, arr2): return (deviation(arr1) * deviation(arr2)).sum() / len(arr1) . covariance(values[&#39;value&#39;], values[&#39;gas_price&#39;]) . 2.8148882775011114e+27 . Variance . def variance(arr): return (deviation(arr) ** 2).sum() / len(arr) . variance(values[&#39;value&#39;]) . 1.3829873312251886e+36 . The $b_1$ coefficient . def b1(arr1, arr2): return covariance(arr1, arr2) / variance(arr1) . b1(values[&#39;value&#39;], values[&#39;gas_price&#39;]) . 2.035368086132359e-09 . The $b_0$ coefficient . def b0(arr1, arr2): return arr2.mean() - b1(arr1, arr2) * arr1.mean() . b_0 = b0(values[&#39;value&#39;], values[&#39;gas_price&#39;]) b_0 . 9709198058.151459 . Predictions . values[&#39;predictions&#39;] = b_0 + values[&#39;value&#39;] * b_1 . values[&#39;predictions&#39;] . 0 1.727894e+10 1 1.917349e+10 2 1.823491e+10 3 2.387204e+10 4 2.633325e+10 ... 360 1.412519e+10 361 1.360478e+10 362 1.403068e+10 363 1.481309e+10 364 1.732880e+10 Name: predictions, Length: 365, dtype: float64 . values.head() . date value gas_price predictions . 0 2019-01-01 | 3.719103e+18 | 1.431514e+10 | 1.727894e+10 | . 1 2019-01-02 | 4.649915e+18 | 1.349952e+10 | 1.917349e+10 | . 2 2019-01-03 | 4.188781e+18 | 1.269504e+10 | 1.823491e+10 | . 3 2019-01-04 | 6.958368e+18 | 1.418197e+10 | 2.387204e+10 | . 4 2019-01-05 | 8.167590e+18 | 2.410475e+10 | 2.633325e+10 | . Plotting the results . First we transform the data into long format . to_plot = values.melt(id_vars=[&#39;date&#39;], value_vars=[&#39;gas_price&#39;, &#39;predictions&#39;], var_name=&#39;status&#39;) to_plot.head() . date status value . 0 2019-01-01 | gas_price | 1.431514e+10 | . 1 2019-01-02 | gas_price | 1.349952e+10 | . 2 2019-01-03 | gas_price | 1.269504e+10 | . 3 2019-01-04 | gas_price | 1.418197e+10 | . 4 2019-01-05 | gas_price | 2.410475e+10 | . Then, we plot the results. . chart = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;, axis=alt.Axis(format=(&quot;%x&quot;), labelAngle=270)), alt.Y(&#39;value:Q&#39;, axis=alt.Axis(format=&quot;,.2e&quot;), scale=alt.Scale(type=&#39;log&#39;)), color=alt.Color(&#39;status:N&#39;) ).properties(width=600) label = alt.selection_single( encodings=[&#39;x&#39;], on=&#39;mouseover&#39;, nearest=True, empty=&#39;none&#39; ) alt.layer( chart, chart.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), alt.Chart().mark_rule(color=&#39;darkgray&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), chart.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;black&#39;, strokeWidth=0.5).encode( text=alt.Text(&#39;value:Q&#39;, format=&#39;,.2e&#39;) ).transform_filter(label), chart.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=alt.Text(&#39;value:Q&#39;, format=&#39;,.2e&#39;) ).transform_filter(label), data=to_plot ).properties(title=&#39;Gas prices and their predictions (log scale)&#39;, width=600, height=400) . It is a bit hard to assess the performance of the results just by looking at them, but at least it seems to generate values within the same ballpark. We could generate metrics as RMSE for reference. . Obvious improvements . remove the outliers | include past values | .",
            "url": "https://andrasnovoszath.com/2020/09/27/linear-regression.html",
            "relUrl": "/2020/09/27/linear-regression.html",
            "date": " • Sep 27, 2020"
        }
        
    
  
    
        ,"post45": {
            "title": "Plotting Ethereum transaction value and gas prices with BigQuery and Altair",
            "content": "As part of getting a better handle on blockchain data, BigQuery, Altair, and Machine Learning, I pulled some Ethereum transaction data and plotted it. . import os os.environ[&quot;GOOGLE_APPLICATION_CREDENTIALS&quot;]=os.path.expanduser(&quot;~/.credentials/Notebook bigquery-c422e406404b.json&quot;) . import altair as alt alt.data_transformers.disable_max_rows() . DataTransformerRegistry.enable(&#39;default&#39;) . from google.cloud import bigquery client = bigquery.Client() . query =&quot;&quot;&quot; SELECT EXTRACT(DATE FROM block_timestamp) AS date, AVG(value) AS average_value, AVG(gas_price) AS average_gas_price, FROM `bigquery-public-data.ethereum_blockchain.transactions` WHERE EXTRACT(YEAR FROM block_timestamp) = 2019 GROUP BY date ORDER BY date &quot;&quot;&quot; . We calculate some basic statistics on raw transaction value data for each day over 2019. . schema = client.get_table(&quot;bigquery-public-data.ethereum_blockchain.transactions&quot;).schema schema . [SchemaField(&#39;hash&#39;, &#39;STRING&#39;, &#39;REQUIRED&#39;, &#39;Hash of the transaction&#39;, (), None), SchemaField(&#39;nonce&#39;, &#39;INTEGER&#39;, &#39;REQUIRED&#39;, &#39;The number of transactions made by the sender prior to this one&#39;, (), None), SchemaField(&#39;transaction_index&#39;, &#39;INTEGER&#39;, &#39;REQUIRED&#39;, &#39;Integer of the transactions index position in the block&#39;, (), None), SchemaField(&#39;from_address&#39;, &#39;STRING&#39;, &#39;REQUIRED&#39;, &#39;Address of the sender&#39;, (), None), SchemaField(&#39;to_address&#39;, &#39;STRING&#39;, &#39;NULLABLE&#39;, &#39;Address of the receiver. null when its a contract creation transaction&#39;, (), None), SchemaField(&#39;value&#39;, &#39;NUMERIC&#39;, &#39;NULLABLE&#39;, &#39;Value transferred in Wei&#39;, (), None), SchemaField(&#39;gas&#39;, &#39;INTEGER&#39;, &#39;NULLABLE&#39;, &#39;Gas provided by the sender&#39;, (), None), SchemaField(&#39;gas_price&#39;, &#39;INTEGER&#39;, &#39;NULLABLE&#39;, &#39;Gas price provided by the sender in Wei&#39;, (), None), SchemaField(&#39;input&#39;, &#39;STRING&#39;, &#39;NULLABLE&#39;, &#39;The data sent along with the transaction&#39;, (), None), SchemaField(&#39;receipt_cumulative_gas_used&#39;, &#39;INTEGER&#39;, &#39;NULLABLE&#39;, &#39;The total amount of gas used when this transaction was executed in the block&#39;, (), None), SchemaField(&#39;receipt_gas_used&#39;, &#39;INTEGER&#39;, &#39;NULLABLE&#39;, &#39;The amount of gas used by this specific transaction alone&#39;, (), None), SchemaField(&#39;receipt_contract_address&#39;, &#39;STRING&#39;, &#39;NULLABLE&#39;, &#39;The contract address created, if the transaction was a contract creation, otherwise null&#39;, (), None), SchemaField(&#39;receipt_root&#39;, &#39;STRING&#39;, &#39;NULLABLE&#39;, &#39;32 bytes of post-transaction stateroot (pre Byzantium)&#39;, (), None), SchemaField(&#39;receipt_status&#39;, &#39;INTEGER&#39;, &#39;NULLABLE&#39;, &#39;Either 1 (success) or 0 (failure) (post Byzantium)&#39;, (), None), SchemaField(&#39;block_timestamp&#39;, &#39;TIMESTAMP&#39;, &#39;REQUIRED&#39;, &#39;Timestamp of the block where this transaction was in&#39;, (), None), SchemaField(&#39;block_number&#39;, &#39;INTEGER&#39;, &#39;REQUIRED&#39;, &#39;Block number where this transaction was in&#39;, (), None), SchemaField(&#39;block_hash&#39;, &#39;STRING&#39;, &#39;REQUIRED&#39;, &#39;Hash of the block where this transaction was in&#39;, (), None)] . values = client.query(query).to_dataframe(dtypes={&#39;average_value&#39;: float, &#39;average_gas_price&#39;: float}, date_as_object=False) values.head() . date average_value average_gas_price . 0 2019-01-01 | 3.719103e+18 | 1.431514e+10 | . 1 2019-01-02 | 4.649915e+18 | 1.349952e+10 | . 2 2019-01-03 | 4.188781e+18 | 1.269504e+10 | . 3 2019-01-04 | 6.958368e+18 | 1.418197e+10 | . 4 2019-01-05 | 8.167590e+18 | 2.410475e+10 | . chart = alt.Chart(values).mark_line().encode( alt.X(&#39;date:T&#39;, axis=alt.Axis(format=(&quot;%x&quot;), labelAngle=270)) ).properties(width=600) alt.layer( chart.encode(alt.Y(&#39;average_value:Q&#39;, axis=alt.Axis(format=&quot;,.2e&quot;)), color=alt.value(&#39;darkred&#39;), opacity=alt.value(0.65)), chart.encode(alt.Y(&#39;average_gas_price&#39;, axis=alt.Axis(format=&quot;,.2e&quot;))) ).resolve_scale(y=&#39;independent&#39;) .",
            "url": "https://andrasnovoszath.com/2020/09/26/ethereum_value_gas_price.html",
            "relUrl": "/2020/09/26/ethereum_value_gas_price.html",
            "date": " • Sep 26, 2020"
        }
        
    
  
    
        ,"post46": {
            "title": "How to use BigQuery in a Jupyter notebook?",
            "content": "When we try to analyze huge datasets (like blockchain data) through BigQuery, it is useful to run the data load and conversion in the cloud and use our local environment mostly for the final transformations and visualization. For these cases, a jupyter notebook seems to be a fitting environment. . There are at least three main ways by which we can access BigQuery data from a notebook: . The most simple one is to use the pandas-gbq library (we already covered it in this previous post) | Using google&#39;s BigQuery notebook extension | The BigQuery python API | Here we will introduce the last two versions. . (This post is based on this official tutorial.) . Setting credentials . Compared to the pandas-gbq library, we need to define the credentials explicitly. . import os os.environ[&quot;GOOGLE_APPLICATION_CREDENTIALS&quot;] = os.path.expanduser(&quot;~/.credentials/Notebook bigquery-c422e406404b.json&quot;) . BigQuery extension . For the extension to work, we need to install the google-cloud-bigquery library. . conda install -c conda-forge google-cloud-bigquery . Then, we load the bigquery extension . %load_ext google.cloud.bigquery . By using the %%bigquery magic command, we immediately define the result of a query as a pandas dataframe. . Here, we pull the total Ethereum token spendings for each day during 2019. . %%bigquery token_transfers SELECT SUM(SAFE_CAST(value AS FLOAT64)) AS total_spent, SAFE_CAST(EXTRACT(DATE FROM block_timestamp) AS DATETIME) AS date FROM `bigquery-public-data.ethereum_blockchain.token_transfers` WHERE EXTRACT(YEAR FROM block_timestamp) = 2019 GROUP BY date ORDER BY date DESC . token_transfers . total_spent date . 0 2.331472e+79 | 2019-12-31 | . 1 4.078401e+79 | 2019-12-30 | . 2 1.773262e+79 | 2019-12-29 | . 3 3.553959e+79 | 2019-12-28 | . 4 2.751157e+79 | 2019-12-27 | . ... ... | ... | . 360 4.226642e+78 | 2019-01-05 | . 361 4.122376e+80 | 2019-01-04 | . 362 1.804472e+80 | 2019-01-03 | . 363 5.833122e+78 | 2019-01-02 | . 364 4.245749e+78 | 2019-01-01 | . 365 rows × 2 columns . import altair as alt alt.data_transformers.disable_max_rows() label = alt.selection_single( # encodings=[&#39;x&#39;], on=&#39;mouseover&#39;, nearest=True, empty=&#39;none&#39; ) chart = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;total_spent:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)) ) alt.layer( chart, chart.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), alt.Chart().mark_rule(color=&#39;darkgray&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), chart.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=alt.Text(&#39;total_spent:Q&#39;, format=&#39;,.2e&#39;) ).transform_filter(label), chart.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=alt.Text(&#39;total_spent:Q&#39;, format=&#39;,.2e&#39;) ).transform_filter(label), data=token_transfers ).properties(width=600, height=400, title=&#39;Daily token spending during 2019 (log scale)&#39;) . BigQuery module . from google.cloud import bigquery client = bigquery.Client() . query =&quot;&quot;&quot; SELECT from_address, SUM(SAFE_CAST(value AS FLOAT64)) AS total_spent, AVG(SAFE_CAST(value AS INT64)) AS average_spent, COUNT(1) AS times_spent FROM `bigquery-public-data.ethereum_blockchain.token_transfers` WHERE EXTRACT(YEAR FROM block_timestamp) = 2019 AND EXTRACT(MONTH FROM block_timestamp) = 09 AND EXTRACT(DAY FROM block_timestamp) = 24 AND EXTRACT(HOUR FROM block_timestamp) = 12 GROUP BY from_address ORDER BY times_spent DESC&quot;&quot;&quot; . For this example we calculate the number of spendings and their total and average values from addresses over a hour. . df = client.query(query).to_dataframe() df . from_address total_spent average_spent times_spent . 0 0x7ba732b1bb952155b720250b477ce154e19ad62f | 1.686990e+12 | 1.963900e+09 | 859 | . 1 0x262e4155e8c5519e668ec26f353d75dd9c18e78f | 3.132121e+23 | NaN | 365 | . 2 0xbd8da72e2f42f5c68b59ee02c2245599ccd702dc | 2.592921e+24 | NaN | 294 | . 3 0x0000000000000000000000000000000000000000 | 7.401857e+24 | 3.058231e+17 | 268 | . 4 0xa71c8bae673f99ac6c0f32c56efc89a8ddb9a501 | 3.896125e+12 | 1.504295e+10 | 259 | . ... ... | ... | ... | ... | . 5279 0xcd338611d74243844f3190b621eb781db53d20b4 | 1.630439e+23 | NaN | 1 | . 5280 0xa9d6b0ad82e46db1895a412ec96b00e18bf95b49 | 1.000000e+09 | 1.000000e+09 | 1 | . 5281 0x4aee792a88edda29932254099b9d1e06d537883f | 5.740766e+22 | NaN | 1 | . 5282 0x71e29ec9e13a39062269fc5c8cba155bb850b23a | 2.270000e+10 | 2.270000e+10 | 1 | . 5283 0x140d6fac06496b21efd086e107d5eca1a16592b3 | 4.335616e+08 | 4.335616e+08 | 1 | . 5284 rows × 4 columns . alt.Chart(df).mark_rect().encode( alt.X(&#39;times_spent:Q&#39;, bin=alt.BinParams(maxbins=20)), alt.Y(&#39;total_spent:Q&#39;, bin=alt.BinParams(maxbins=20)), alt.Color(&#39;count()&#39;), alt.Tooltip(&#39;count()&#39;) ).properties(title=&#39;Total spending value and spending frequency&#39;) . As expected, there are a few number of addresses responsible for the highest spending frequency and the highest spending value during that hour. .",
            "url": "https://andrasnovoszath.com/2020/09/25/bigquery-jupyter.html",
            "relUrl": "/2020/09/25/bigquery-jupyter.html",
            "date": " • Sep 25, 2020"
        }
        
    
  
    
        ,"post47": {
            "title": "Accessing Ethereum token transaction data with ``pandas-gbq``",
            "content": "One way to access blockchain bigquery data with Google BigQuery is to use the pandas-gbq library. . It makes it really easy to take a BigQuery SQL query and download its results as a pandas DataFrame. . Big Query authentication . In order to use this library, you need to authenticate a credential with BigQuery. I found creating a &#39;Service account&#39; to be the most meaningful. You can follow the steps here. . Installation . Installation with conda is straightforward. . $ conda install pandas-gbq --channel conda-forge . Basic usage . We can download a table by defining an SQL query and passing it to the read_gbq method. . import altair as alt import pandas_gbq . As Ethereum data is big, we constrain our query to a single hour. . sql = &quot;&quot;&quot; SELECT block_timestamp, value FROM `bigquery-public-data.ethereum_blockchain.token_transfers` WHERE EXTRACT(YEAR FROM block_timestamp) = 2019 AND EXTRACT(MONTH FROM block_timestamp) = 09 AND EXTRACT(DAY FROM block_timestamp) = 24 AND EXTRACT(HOUR FROM block_timestamp) = 12 &quot;&quot;&quot; . token = pandas_gbq.read_gbq(sql, project_id=&quot;pandas-gbq-test-290508&quot;) . Downloading: 100%|██████████| 15392/15392 [00:03&lt;00:00, 4846.48rows/s] . token . block_timestamp value . 0 2019-09-24 12:47:43+00:00 | 50058584428 | . 1 2019-09-24 12:47:43+00:00 | 30000000 | . 2 2019-09-24 12:47:43+00:00 | 14069000000 | . 3 2019-09-24 12:47:43+00:00 | 170000000 | . 4 2019-09-24 12:47:43+00:00 | 59478199 | . ... ... | ... | . 15387 2019-09-24 12:40:54+00:00 | 3577720000000000000000 | . 15388 2019-09-24 12:40:54+00:00 | 2314760000000000000000 | . 15389 2019-09-24 12:40:54+00:00 | 2399000000000000000000 | . 15390 2019-09-24 12:40:54+00:00 | 5701000000000000000000 | . 15391 2019-09-24 12:40:54+00:00 | 3608000000000000000000 | . 15392 rows × 2 columns . Data transformation . After loading the dataset, it requires some transformations. . First, we want to sort it by timestamps. . token.dtypes . block_timestamp datetime64[ns, UTC] value object dtype: object . token = token.sort_values(&#39;block_timestamp&#39;) . Then we want to convert the values column to float. . token[&#39;value&#39;] = token[&#39;value&#39;].astype(float) . Visualization . As altair does not allow visualization with more than 5000 rows, we need to manually set it possible. . alt.data_transformers.disable_max_rows() . DataTransformerRegistry.enable(&#39;default&#39;) . We plot the average transaction values by minute. As we there are a few number of very high value transactions, we use a log scale. . alt.Chart(token).mark_line().encode( alt.X(&#39;utchoursminutes(block_timestamp):T&#39;), alt.Y(&#39;average(value):Q&#39;, scale=alt.Scale(type=&#39;log&#39;)) ).properties(width=800, title=&#39;Average token transaction values (log scale)&#39;) .",
            "url": "https://andrasnovoszath.com/2020/09/24/pandas-gbq.html",
            "relUrl": "/2020/09/24/pandas-gbq.html",
            "date": " • Sep 24, 2020"
        }
        
    
  
    
        ,"post48": {
            "title": "Random prediction",
            "content": "When we try to build a prediction algorithm, it is a useful practice to first try out a baseline algorithm and see how they perform. Here we will describe a case of random prediction. . (Inspiration and examples are from Jason Brownlee&#39;s Machine Learning Algorithms from Scratch book.) . import pandas as pd import altair as alt import numpy as np from vega_datasets import data . np.random.seed(42) . For the examples we will use the vega &#39;volcano&#39; dataset. The width and height values are constant, so we work only with the values column. . volcano = data(&#39;volcano&#39;) volcano.head() . width height values . 0 87 | 61 | 103 | . 1 87 | 61 | 104 | . 2 87 | 61 | 104 | . 3 87 | 61 | 105 | . 4 87 | 61 | 105 | . The random prediction algorithm . takes a training and a test set | generates the prediction by selecting random elements from the training set | We split the dataset to training and test sets, by taking the first 2/3 and last 1/3 of the data respectively. . train, test = volcano.iloc[: volcano.shape[0] * 2 // 3, :], volcano.iloc[volcano.shape[0] * 2 // 3 :, :] . A small check that the split did not leave out an entry or did not result in an overlap. . assert train.index[-1] + 1 == test.index[0] . We plot the training and the test sets, respectively. . alt.Chart(train.reset_index()).mark_line().encode(alt.Y(&#39;values&#39;), alt.X(&#39;index:Q&#39;), alt.Tooltip(&#39;values&#39;)).interactive().properties(title=&#39;Train data&#39;) . alt.Chart(test.reset_index()).mark_line().encode(alt.Y(&#39;values&#39;), alt.X(&#39;index:Q&#39;), alt.Tooltip(&#39;values&#39;)).interactive().properties(title=&#39;Test data&#39;) . First, we generate the random predictions with replacement. That is, the same values can occur more than once. . predictions = np.random.choice(train[&#39;values&#39;], size=test.shape[0], replace=True) predictions . array([166, 179, 96, ..., 105, 157, 95]) . We calculate the error terms . errors = test[&#39;values&#39;] - predictions errors . 3538 -16 3539 -29 3540 54 3541 54 3542 56 .. 5302 -8 5303 2 5304 -7 5305 -60 5306 2 Name: values, Length: 1769, dtype: int64 . We calculate the root mean squared error of the predictions. . def calculate_rmse(observed, predicted): return np.sqrt(sum((observed - predicted) ** 2)/len(observed)) . The RMSE is around 34.59 which stands for about 1.34 STD. . rmse = calculate_rmse(test[&#39;values&#39;], predictions) rmse . 34.93380641982711 . For reference, if we would simply use the simple mean, we would get an RMSE of 18.42, . calculate_rmse(test[&#39;values&#39;], test[&#39;values&#39;].mean()) . 18.420942507684327 . Let&#39;s plot the results . to_compare = pd.concat( [ test[&#39;values&#39;].rename(&#39;observed&#39;).reset_index(drop=True), pd.Series(predictions).rename(&#39;predictions&#39;).reset_index(drop=True) ], axis=1 ).stack().reset_index().rename(columns={&#39;level_1&#39;: &#39;status&#39;, &#39;level_0&#39;: &#39;index&#39;, 0: &#39;values&#39;}) to_compare . index status values . 0 0 | observed | 150 | . 1 0 | predictions | 166 | . 2 1 | observed | 150 | . 3 1 | predictions | 179 | . 4 2 | observed | 150 | . ... ... | ... | ... | . 3533 1766 | predictions | 105 | . 3534 1767 | observed | 97 | . 3535 1767 | predictions | 157 | . 3536 1768 | observed | 97 | . 3537 1768 | predictions | 95 | . 3538 rows × 3 columns . alt.Chart(to_compare).mark_line().encode( alt.X(&#39;index:Q&#39;), alt.Y(&#39;values&#39;), alt.Color(&#39;status&#39;), alt.OpacityValue(0.7) ).properties(width=1200, title=&#39;Predictions with replacement&#39;) . We put the steps into a function and rerun the prediction without replacement. . def predict_randomly(train, test, replace=True): predictions = np.random.choice(train, size=test.shape[0], replace=True) errors = test - predictions rmse = calculate_rmse(test, predictions) return predictions, rmse . predictions, rmse = predict_randomly(train[&#39;values&#39;], test[&#39;values&#39;], replace=False) rmse . 35.90392934559341 . Finally, we also put the plotting steps into a function . def plot_predictions(observed, predicted): to_compare = pd.concat( [observed.rename(&#39;observed&#39;).reset_index(drop=True), pd.Series(predicted).rename(&#39;predictions&#39;).reset_index(drop=True)], axis=1 ).stack().reset_index().rename(columns={&#39;level_1&#39;: &#39;status&#39;, &#39;level_0&#39;: &#39;index&#39;, 0: &#39;values&#39;}) chart = alt.Chart(to_compare).mark_line().encode( alt.X(&#39;index:Q&#39;), alt.Y(&#39;values&#39;), alt.Color(&#39;status&#39;), alt.OpacityValue(0.7) ).properties(width=1200) chart.display() . The predictions without replacement . plot_predictions(test[&#39;values&#39;], predictions) .",
            "url": "https://andrasnovoszath.com/2020/09/23/random_predictions.html",
            "relUrl": "/2020/09/23/random_predictions.html",
            "date": " • Sep 23, 2020"
        }
        
    
  
    
        ,"post49": {
            "title": "How does data standardization work?",
            "content": "With standardization, we transform the data so its distribution&#39;s center will become 0 and its standard deviation will become 1. . import altair as alt from vega_datasets import data . For this demo, we use the sp500 price index. . sp500 = data(&#39;sp500&#39;) sp500.head() . date price . 0 2000-01-01 | 1394.46 | . 1 2000-02-01 | 1366.42 | . 2 2000-03-01 | 1498.58 | . 3 2000-04-01 | 1452.43 | . 4 2000-05-01 | 1420.60 | . alt.Chart(sp500).mark_line(point=True, strokeWidth=1).encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price&#39;), alt.Tooltip(&#39;price&#39;) ).configure_point(size=10) . The original distribution of prices. . alt.Chart(sp500).mark_bar().encode( x=alt.X(&#39;price:Q&#39;, bin=alt.BinParams(maxbins=20)), y=alt.Y(&#39;count()&#39;), tooltip=alt.Tooltip([&#39;count()&#39;]) ) . We get the standardized value by dividing the values&#39; difference from the mean by the standard deviation. . $ text{standardized value} = frac { text{value} - text{mean}} { text{standard deviation}} $ . The mean and standard deviation of the prices . print(f&quot;&quot;&quot; mean: {sp500[&#39;price&#39;].mean():.2f} std: {sp500[&#39;price&#39;].std():.2f} &quot;&quot;&quot;) . mean: 1184.43 std: 195.41 . We standardize the data set. . standardized_prices = sp500.copy() standardized_prices[&#39;price&#39;] = (sp500[&#39;price&#39;] - sp500[&#39;price&#39;].mean()) / sp500[&#39;price&#39;].std() standardized_prices . date price . 0 2000-01-01 | 1.074812 | . 1 2000-02-01 | 0.931317 | . 2 2000-03-01 | 1.607646 | . 3 2000-04-01 | 1.371473 | . 4 2000-05-01 | 1.208583 | . ... ... | ... | . 118 2009-11-01 | -0.454452 | . 119 2009-12-01 | -0.354814 | . 120 2010-01-01 | -0.565809 | . 121 2010-02-01 | -0.409111 | . 122 2010-03-01 | -0.225086 | . 123 rows × 2 columns . alt.Chart(standardized_prices).mark_bar().encode( x=alt.X(&#39;price:Q&#39;, bin=alt.BinParams(maxbins=20)), y=alt.Y(&#39;count()&#39;), tooltip=alt.Tooltip([&#39;count()&#39;]) ) . The new mean and standard deviation . print(f&quot;&quot;&quot; mean: {standardized_prices[&#39;price&#39;].mean():.2f} std: {standardized_prices[&#39;price&#39;].std():.2f} &quot;&quot;&quot;) . mean: -0.00 std: 1.00 . When plotted on the dates, it provides a similar graph as before, but now with a value domain around 0. . alt.Chart(standardized_prices).mark_line(point=True, strokeWidth=1).encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price&#39;), alt.Tooltip(&#39;price&#39;) ).configure_point(size=10) .",
            "url": "https://andrasnovoszath.com/2020/09/22/standardization.html",
            "relUrl": "/2020/09/22/standardization.html",
            "date": " • Sep 22, 2020"
        }
        
    
  
    
        ,"post50": {
            "title": "Data scale normalization",
            "content": "Normalization is a common technique used in machine learning to render the scales of different magnitudes to a common range between 0 and 1. . Here we demonstrate how this is done with pandas and altair. . Original inspiration: (Jason Brownlee: Machine Learning Algorithms from Scratch)[https://machinelearningmastery.com/machine-learning-algorithms-from-scratch/] . import altair as alt # alt.renderers.enable(&#39;default&#39;) alt.renderers . RendererRegistry(active=&#39;default&#39;, registered=[&#39;colab&#39;, &#39;default&#39;, &#39;html&#39;, &#39;json&#39;, &#39;jupyterlab&#39;, &#39;kaggle&#39;, &#39;mimetype&#39;, &#39;notebook&#39;, &#39;nteract&#39;, &#39;png&#39;, &#39;svg&#39;, &#39;zeppelin&#39;]) . from vega_datasets import data . We use the Gapminder health and income dataset . health_income = data(&#39;gapminder-health-income&#39;) health_income.head() . country income health population . 0 Afghanistan | 1925 | 57.63 | 32526562 | . 1 Albania | 10620 | 76.00 | 2896679 | . 2 Algeria | 13434 | 76.50 | 39666519 | . 3 Andorra | 46577 | 84.10 | 70473 | . 4 Angola | 7615 | 61.00 | 25021974 | . income_domain = [health_income[&#39;income&#39;].min(), health_income[&#39;income&#39;].max()] health_domain = [health_income[&#39;health&#39;].min(), health_income[&#39;health&#39;].max()] alt.Chart(health_income).mark_point().encode( alt.X(&#39;income:Q&#39;, scale=alt.Scale(domain=income_domain)), alt.Y(&#39;health:Q&#39;, scale=alt.Scale(domain=health_domain)), alt.Size(&#39;population:Q&#39;), alt.Tooltip(&#39;country:N&#39;) ).properties(height=600, width=800) . The process: . Take the values&#39; difference from the smallest one; | Take the value range, that is, the difference between the largest and smallest values; | Divide the reduced values with the range. | $ text {scaled value} = frac{value - min} {max - min} $ . The first step ensures that the smallest value will become 0. Dividing the reduced values by the range &#39;compresses&#39; the values so the new maximum becomes 1. . quantitative_columns = [&#39;income&#39;, &#39;health&#39;, &#39;population&#39;] . The original minimum and maximum values . health_income.loc[health_income[quantitative_columns].idxmin(), :] . country income health population . 32 Central African Republic | 599 | 53.8 | 4900274 | . 93 Lesotho | 2598 | 48.5 | 2135022 | . 105 Marshall Islands | 3661 | 65.1 | 52993 | . minimums = health_income[quantitative_columns].min() minimums . income 599.0 health 48.5 population 52993.0 dtype: float64 . health_income.loc[health_income[quantitative_columns].idxmax(), :] . country income health population . 134 Qatar | 132877 | 82.0 | 2235355 | . 3 Andorra | 46577 | 84.1 | 70473 | . 35 China | 13334 | 76.9 | 1376048943 | . maximums = health_income[quantitative_columns].max() maximums . income 1.328770e+05 health 8.410000e+01 population 1.376049e+09 dtype: float64 . Difference of values from the column minimum . health_income[quantitative_columns] - minimums . income health population . 0 1326.0 | 9.13 | 32473569.0 | . 1 10021.0 | 27.50 | 2843686.0 | . 2 12835.0 | 28.00 | 39613526.0 | . 3 45978.0 | 35.60 | 17480.0 | . 4 7016.0 | 12.50 | 24968981.0 | . ... ... | ... | ... | . 182 5024.0 | 28.00 | 93394608.0 | . 183 3720.0 | 26.70 | 4615473.0 | . 184 3288.0 | 19.10 | 26779222.0 | . 185 3435.0 | 10.46 | 16158774.0 | . 186 1202.0 | 11.51 | 15549758.0 | . 187 rows × 3 columns . Value ranges: the difference between the maximum and the minimum . maximums - minimums . income 1.322780e+05 health 3.560000e+01 population 1.375996e+09 dtype: float64 . Let&#39;s normalize the dataset . def normalize_dataset(dataset, quantitative_columns): dataset = dataset.copy() minimums = dataset[quantitative_columns].min() maximums = dataset[quantitative_columns].max() dataset[quantitative_columns] = (dataset[quantitative_columns] - minimums) / (maximums - minimums) return dataset . normalized_health_income = normalize_dataset(health_income, quantitative_columns) normalized_health_income . country income health population . 0 Afghanistan | 0.010024 | 0.256461 | 0.023600 | . 1 Albania | 0.075757 | 0.772472 | 0.002067 | . 2 Algeria | 0.097030 | 0.786517 | 0.028789 | . 3 Andorra | 0.347586 | 1.000000 | 0.000013 | . 4 Angola | 0.053040 | 0.351124 | 0.018146 | . ... ... | ... | ... | ... | . 182 Vietnam | 0.037981 | 0.786517 | 0.067874 | . 183 West Bank and Gaza | 0.028123 | 0.750000 | 0.003354 | . 184 Yemen | 0.024857 | 0.536517 | 0.019462 | . 185 Zambia | 0.025968 | 0.293820 | 0.011743 | . 186 Zimbabwe | 0.009087 | 0.323315 | 0.011301 | . 187 rows × 4 columns . The new minimum and maximum values . normalized_health_income.loc[normalized_health_income[quantitative_columns].idxmin(), :] . country income health population . 32 Central African Republic | 0.000000 | 0.148876 | 0.003523 | . 93 Lesotho | 0.015112 | 0.000000 | 0.001513 | . 105 Marshall Islands | 0.023148 | 0.466292 | 0.000000 | . normalized_health_income.loc[normalized_health_income[quantitative_columns].idxmax(), :] . country income health population . 134 Qatar | 1.000000 | 0.941011 | 0.001586 | . 3 Andorra | 0.347586 | 1.000000 | 0.000013 | . 35 China | 0.096275 | 0.797753 | 1.000000 | . Plotting the normalized data, we got the same results, but with the income, health, and population scales all normalized to the [0, 1] range. . Maximum values . alt.Chart(normalized_health_income).mark_point().encode( alt.X(&#39;income:Q&#39;,), alt.Y(&#39;health:Q&#39;), alt.Size(&#39;population:Q&#39;), alt.Tooltip(&#39;country:N&#39;) ).properties(height=600, width=800) .",
            "url": "https://andrasnovoszath.com/2020/09/21/normalize-scale.html",
            "relUrl": "/2020/09/21/normalize-scale.html",
            "date": " • Sep 21, 2020"
        }
        
    
  
    
        ,"post51": {
            "title": "How to categorize your notes?",
            "content": "I take notes . I take lots of notes. Many of them are ideas to research, explore, and think about, while others are rather tasks or possible projects. . As I collect them continuously, their number grows. After a while, it becomes hard to look through them and prioritize for action. . The obvious way to do this, of course, is to categorize them under topics or themes. . Concept hierarchies . This produces a hierarchical tree-like structure. There a few problems with this: . There always be some idea or task which does not belong to a single category alone but under multiple ones. | As the number of notes and, therefore, the number of categories and sub-categories becomes high, you need to dig very deep to find some notes. If the hierarchies are obvious, this is not really a problem. However, as the number of hierarchy levels and branches grow, the previous issue (i.e., notes belonging under multiple categories) become even more prominent. | . Concept drift . There is also the problem of concept drift. While we refer to a particular thing or category with a specific term, later on, we start to refer to it with a related but different one. Similarly, we do the same with categories. . This shift can happen for multiple reasons, but the most obvious ones are terminological shifts or our changing ‘use’ of the original idea or category. . Concept drift, deep hierarchies, and the overlapping category boundaries can lead to the ‘forgetting’ of notes. They can also lead to making multiple, ‘almost similar’ but ‘slightly different’ versions of them at different places of the hierarchy. . And, even whether the relationship between two ideas should be hierarchical or not also can change from use case to use case. . Chaos everywhere . And note-taking is just the most obvious example for me. I had the same experience with software development (both as writing code and managing versions), technical writing, academic research, work organization, file organization, etc. .",
            "url": "https://andrasnovoszath.com/note-taking/categorization/hierarchies/concept%20drift/chaos%20&%20order/2020/09/20/note-categorization-pain.html",
            "relUrl": "/note-taking/categorization/hierarchies/concept%20drift/chaos%20&%20order/2020/09/20/note-categorization-pain.html",
            "date": " • Sep 20, 2020"
        }
        
    
  
    
        ,"post52": {
            "title": "The sum of stock trading",
            "content": "I thought further about my previous position on whether stock trading is a zero-sum game. Now I see a bigger space for it not to be one. . Trading growth stocks . The situation I am thinking about is an argument I found during my google search. . Very simply, one buys a stock at one point in time and sells it to someone else in another. . Let’s take the ‘classical’ position on stocks and consider it merely an investment asset. In this case, we can argue that stock buyers lent money to a company in exchange for a part of its profits. They sell the stocks when it wants to get their money back, say for their retirement. . Here the value comes for the original owner in that the price will be higher when they sell. . This is the same hope the sellers have for the time when they want to realize their investment. That is, when the original buyers sell the stock, the new buyers can get a ‘fair’ price if, in that given time, the stock is still promising some price growth. . This seems more positive-sum for me. However, this view works if we strip away the ‘speculative’ (?), arbitrage-seeking trading aspect of security trading. In this view, stock trading seems to resemble the trade of a good tool one does not need anymore. . Remaining questions . There are, of course, a number of issues still unclear for me. . The first is whether ‘speculation’ or ‘arbitrage seeking’ inevitably leads to a zero-sum outcome? Also, can we separate ‘straightforward’ investment from these activities? . The next is about the buying price: when is the price too high/low? . The buyer could always want to ask for a higher price. Aren’t selling a stock at a too high price basically strips away the seller’s possible gains? . On the other hand, one might argue that, if the price reflects the market price, there is less space for unfairness (or at least not directly between the two parties). . Third, there is the question of whether a company can grow infinitely or trading stock is always based on this false promise. . Finally, how do we calculate the outcomes when the ‘gain’, in big part, comes from luck and decisions made years before their realization. .",
            "url": "https://andrasnovoszath.com/stock%20trading/zero-sum/finance/valuation/2020/09/19/stock-trading-sum.html",
            "relUrl": "/stock%20trading/zero-sum/finance/valuation/2020/09/19/stock-trading-sum.html",
            "date": " • Sep 19, 2020"
        }
        
    
  
    
        ,"post53": {
            "title": "How to build up a writing habit?",
            "content": "If you tend to analyze a lot but want to build up a consistent writing habit, the following tips might help. . Take notes . Have a notepad or a very light-weight text editor/app that you can open at any time and dump your ideas effortlessly. . Find out what interests you . Review your notes, todo lists, emails, past writing, or any other traces about yourself. Look for frequent themes about which you feel strongly and, even better, about which you developed some depth in skill or knowledge. . Prioritize your themes . Take these themes and group or merge the closely related ones. Then, prioritize them based on their importance. . When you have them ranked, drop the last 80% and focus on the remaining 20%. If you have too too many themes remained (e.g., more than 10), drop even more. . Brainstorm post ideas . For each remaining theme, write as many post ideas as possible, but a minimum of ten. Do not bother with how good or viable they are. The important thing is to have a high number of them. . Rotate your themes . Now you have a list of topics organized under themes. Outline a rotating schedule where you order the themes into a sequence, so, in each day, you write about a topic belonging to the next theme. . Write . Each day, you take the topics belonging under that day’s theme and pick one randomly. Write about it impatiently, badly, and experimentally for a minimum 5 minutes. . When you find that you could write about multiple things but you do not know which to pick, pick the first one and move the second into your ideas under the same theme. . When you run out of time, you will find yourself wanting to write more about something. Move it as a new topic under the same theme. . Publish . Publish the writing to somewhere, where nobody sees is. For example, create an account on Medium or any blogging platform under some alias. . Repeat . When you arrive at the end of the sequence, start again. If you could fill out all your 5 minutes for each day for a week, add an extra 5 minutes for your writing time. .",
            "url": "https://andrasnovoszath.com/writing/habit/2020/09/18/writing-habit.html",
            "relUrl": "/writing/habit/2020/09/18/writing-habit.html",
            "date": " • Sep 18, 2020"
        }
        
    
  
    
        ,"post54": {
            "title": "Is finance a zero-sum activity?",
            "content": "I recently thought about finance and whether it is a purely zero-sum game or has some value-add to it. I would like to know this to see if it is worthy of support either as a personal wealth source or as a social institution. . I did not have time to read academic papers, so I simply did a google search (1-2 hours) and reviewed the top results. . What I found . Just based on this short research, I barely found any substantial arguments against the zero-sum claim. . What counterarguments I found were acknowledged that some forms, like forex or short-term trading, are zero-sum. They argued that, compared to the former types, ‘real’ stock trading can bring about benefits for both parties. Most of these were conflating terminology and concepts (like ‘luck’, or ‘value growth’). . There are probably more substantial arguments in academic literature. Also, probably, there is the additional question of the social usefulness of financial markets for overall capital allocation, but that’s a separate thing. . How I see it now . Of course, as I started to dive into the topic, I quickly realized that is is a bit more complex than I thought. . So let’s break up the question to the following cases: . Pure commercial relationship | Trading of goods | Finance | . Pure commercial transaction . When two people get into a commercial relationship, one is a ‘seller’, and the other is a ‘buyer’. Regarding the price, they have opposite aims. One’s income is another’s cost. . So, in this sense, every commercial transaction is zero-sum. . Trade of ‘comparatively advantageous’ goods . When the seller gives away something they cannot use, but the buyer can, the exchange might benefit both. There is still the issue of the price for which they exchange the good, but the buyer’s benefit from possessing the good might surpass its price. . So, here we can imagine a non-zero-sum relationship. . Finance . Most of the people who trade and invest mostly want to get the most payoff for their buck. So this primarily seems zero-sum. . The mutual benefit might come about when their relationship to their asset is more complex than just trying to get the highest price difference. . Conclusion . Again, I can see that I could think and write about this topic for a long time. Some ideas . How do we measure and compare ‘comparatively advantages’? | What are examples of not purely ‘buy low, sell high’ uses of finance? | How to measure the utility ‘ratio’ between price gain/loss and usability of the result of a buy event? | Is the social benefit of efficient capital distribution via financial markets a fluke, or has empirical evidence behind it? | Can we replace financial markets with an AI and delegate human creativity to more productive things? | .",
            "url": "https://andrasnovoszath.com/finance/zero-sum/valuation/2020/09/17/finance-zero-sum.html",
            "relUrl": "/finance/zero-sum/valuation/2020/09/17/finance-zero-sum.html",
            "date": " • Sep 17, 2020"
        }
        
    
  
    
        ,"post55": {
            "title": "The specific you",
            "content": "When you are thinking about what business area to pursue, it is useful to focus on your specific situation and circumstance. . Why is it useful to find a specialization? . When you do not look at your own specificities, you basically do not use knowledge and skills that others could use but cannot. . You have your history, your dis/abilities, your interests, and your own conceptual framing. Most of it might be useless, especially regarding all the people in the world. . However, there are a few problems which maybe you are the only one who can solve it. . This is not about ‘finding your passion’ but rather to find out your skills, knowledge, approach, etc. These are actually quite tangible. . How to know your specificities? . To see yourself better, write down your characteristics in as many dimensions as possible. . Here are some dimensions which you can look for: . Knowledge, | Skills, | Experience, | Interest. | . One thing you can do is to create a spreadsheet and write as many of these things as possible. Then, value them on a scale (e.g., 1-5) showing how specific these traits and dimensions are to you, especially compared to others. .",
            "url": "https://andrasnovoszath.com/self-knowledge/specialization/2020/09/16/specific-you.html",
            "relUrl": "/self-knowledge/specialization/2020/09/16/specific-you.html",
            "date": " • Sep 16, 2020"
        }
        
    
  
    
        ,"post56": {
            "title": "Valuation technology experimentation",
            "content": "It might be useful and promising to experiment with technologies of valuation. . How does valuation happen now, and how does technology play a role in it? . Current technologies of valuation are in place because of different reasons. However, implementers rarely take into account the way the given technology plays a role in valuing things. . Examples of the use of technology in valuation and some trends . Money is arguably the most prominent example. Probably this is not independent of that most of us still do not really understand all its effects. . Other examples are in accountancy, trade, work management, economic metrics, environmental management. . An obvious note is that, of course, other, not strictly technological components, like language, moral values, physical factors, etc., also have critial roles. . Think about and experiment with different technologies to generate new valuation situations . Accordingly, it might be beneficial to try out technologies with explicit valuation functions in mind. . For the most part, and especially in early stages, this is concept-driven (i.e., we are looking for a particular effect to emerge). However, an uncertain but promising benefit would be discovering ‘unthought’ features due to things’ specific and idiosyncratic character. . The process of valuation technology experiments and its main challenges . Most of the time, we cannot implement large scale infrastructures for the sake of experimentation. . Instead, we can try other paths: . Implementation of a small-scale but very focused version | Scenario building | Simulation | Game development | The empirical analysis of similar processes | The empirical analysis of current technologies’ edge cases | Speculation | Incremental implementation | . Current action possibilities and possible future directions . As, currently, I this all is very high level and conceptual, and I am also mostly interested in discovery activities: . Collecting ideas | Conceptual exploration of background ideas | Case study analysis | Code experimentations | . In the future, I might be able to focus it down and branch out toward more serious development work. .",
            "url": "https://andrasnovoszath.com/technology/valuation/experimentation/2020/09/15/technnology_valuation_experimentation.html",
            "relUrl": "/technology/valuation/experimentation/2020/09/15/technnology_valuation_experimentation.html",
            "date": " • Sep 15, 2020"
        }
        
    
  
    
        ,"post57": {
            "title": "Note-taking obsessions",
            "content": "I have an obsession with the process of note-taking. . I often write down my ideas in the form of notes or tasks. I do this through multiple channels. Then I spend a considerable amount of time organizing them. . One common frustration with organizing notes is that I can do this only along one hierarchical dimension. . Perhaps one reason I like to code is that you can implement hierarchies along multiple dimensions more easily, although not always. . Because of the single hierarchy, some notes tend to become forgotten as I put them in some deep end part of a category branch. This then requires me to constantly re- and review my notes and to recategorize and reorganize them. . I have looked into multiple note-taking software, but there is not really a perfect solution for this. . One thing I found helpful is to require only a single function from a particular tool. So, for instance, I would use one particular tool or process for note-taking, then another for categorization, and perhaps a further one for review or note management. . This, however, needs that the different tools speak to each other relatively well. Ready-made solutions often do not allow it, so I found myself moving toward text-based tools. However, you need to utilize some basic scripting to make them work. .",
            "url": "https://andrasnovoszath.com/note-taking/2020/09/14/note-taking-obsessions.html",
            "relUrl": "/note-taking/2020/09/14/note-taking-obsessions.html",
            "date": " • Sep 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About",
          "content": "Online mental dump to put things out. Currently, I practice writing posts within 30 minutes. Still getting there… .",
          "url": "https://andrasnovoszath.com/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  

  

  

  
  

  
      ,"page13": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://andrasnovoszath.com/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}