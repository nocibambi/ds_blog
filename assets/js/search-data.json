{
  
    
        "post0": {
            "title": "Correlation and covariance from scratch",
            "content": "In this post we examine covariance and a correlation a bit closer. . We will use them to examine the relationship between Ethereum transaction value and gas price. . Again, most of the time, we break down the steps into standard Python data types and operations (i.e. we use numpy mostly for verification of our results). . Libraries and data load . We pull the data from Google&#39;s public datasets with BigQuery, use pandas and numpy to manipulate it, and altair to plot their relationship. . import os os.environ[&quot;GOOGLE_APPLICATION_CREDENTIALS&quot;]=os.path.expanduser(&quot;~/.credentials/Notebook bigquery-c422e406404b.json&quot;) from google.cloud import bigquery client = bigquery.Client() import altair as alt alt.data_transformers.disable_max_rows() import numpy as np import pandas as pd . query =&quot;&quot;&quot; SELECT EXTRACT(DATE FROM block_timestamp) AS date, AVG(value) AS value, AVG(gas_price) AS gas_price, FROM `bigquery-public-data.ethereum_blockchain.transactions` WHERE EXTRACT(YEAR FROM block_timestamp) = 2019 GROUP BY date ORDER BY date &quot;&quot;&quot; . transactions = client.query(query).to_dataframe(dtypes={&#39;value&#39;: float, &#39;gas_price&#39;: float}, date_as_object=False) transactions.head() . date value gas_price . 0 2019-01-01 | 3.719103e+18 | 1.431514e+10 | . 1 2019-01-02 | 4.649915e+18 | 1.349952e+10 | . 2 2019-01-03 | 4.188781e+18 | 1.269504e+10 | . 3 2019-01-04 | 6.958368e+18 | 1.418197e+10 | . 4 2019-01-05 | 8.167590e+18 | 2.410475e+10 | . There are a few days when the gas prices were outstandingly high so we remove values beyond three standard deviation from the mean. . Outliers . labelx = alt.selection_single( encodings=[&#39;x&#39;], on=&#39;mouseover&#39;, empty=&#39;none&#39; ) labely = alt.selection_single( encodings=[&#39;y&#39;], on=&#39;mouseover&#39;, empty=&#39;none&#39; ) ruler = alt.Chart().mark_rule(color=&#39;darkgray&#39;) chart = alt.Chart().mark_point().encode( alt.X(&#39;value&#39;, axis=alt.Axis(format=(&#39;,.2e&#39;))), alt.Y(&#39;gas_price&#39;, axis=alt.Axis(format=(&#39;,.2e&#39;))), alt.Tooltip([&#39;value&#39;, &#39;gas_price&#39;, &#39;date&#39;]) ).properties(width=600, height=400, title=&#39;Trasaction values and gas prices&#39;).add_selection(labelx).add_selection(labely) alt.layer( chart, ruler.encode(x=&#39;value:Q&#39;).transform_filter(labelx), ruler.encode(y=&#39;gas_price:Q&#39;).transform_filter(labely), data=transactions ).interactive() . transactions = transactions[~(transactions[&#39;gas_price&#39;] &gt;= transactions[&#39;gas_price&#39;].mean() + 3 * transactions[&#39;gas_price&#39;].std())] . values = transactions[&#39;value&#39;] gas_prices = transactions[&#39;gas_price&#39;] . As we emphasize standard operations, we use a few helper functions in the steps leading to covariance and correlation. . Helper functions . from typing import Union, List Vector = List[float] . def dot(vector1: Vector, vector2: Vector) -&gt; float: assert len(vector1) == len(vector2) return sum(v1 * v2 for v1, v2 in zip(vector1, vector2)) assert dot([1, 2, 3], [4, 5, 6]) == 32 def mean(x: Vector) -&gt; float: return sum(x) / len(x) assert mean([1, 2, 3, 4]) == 2.5 def de_mean(xs: Vector) -&gt; Vector: x_mean = mean(xs) return [x - x_mean for x in xs] assert de_mean([4, 5, 6, 7, 8]) == [-2, -1, 0, 1, 2] def sum_of_squares(xs: Vector) -&gt; float: return dot(xs, xs) assert sum_of_squares([1, 2, 3]) == 14 def variance(xs: Vector) -&gt; float: return sum_of_squares(de_mean(xs)) / (len(xs) - 1) assert variance([1, 2, 3]) == 1 import math as m def standard_deviation(xs: Vector): return m.sqrt(variance(xs)) assert standard_deviation([4, 5, 6]) == 1 . Covariance looks at the degree two variables &#39;move together&#39;. . For this, first, it multiplies the variables&#39; deviation from their respective means. This produces a series of values which are very high for those observations where both variables deviate a lot. Furthermore, when the two variables deviate to the same direction these values are positive, otherwise they are negative. . Then, it calculates the mean of these multiplied deviation values. However, because we are calculating the sample covariance, we divide their sum by $n + 1$ (where $n$ is the number of observations) . $ text{Cov} = frac { sum_{i=1}^n (x- bar{x}) (y- bar{y})} {n - 1} $ . Covariance . def covariance(xs: Vector, ys: Vector) -&gt; float: assert len(xs) == len(ys) return dot(de_mean(xs), de_mean(ys)) / (len(xs) - 1) assert covariance([1, 2, 3], [4, 5, 6]) == 1 . There is also an alternate way to calculate covariance, using the variables&#39; expected values (which here are the means): . $ text{Cov} = E[ vec{x} vec{y}] - E[ vec{x}]E[ vec{y}] $ . This is a much simpler version. However, again, as we are dealing with sample data, so we need to adjust for that: . $ text{Cov}_s = frac {n} {n - 1} (E[ vec{x} vec{y}] - E[ vec{x}]E[ vec{y}]) $ . covariance(values, gas_prices) . 1.5856518696847875e+26 . def covariance_2(xs: Vector, ys: Vector) -&gt; float: xsys = [x * y for x, y in zip(xs, ys)] return (mean(xsys) - mean(xs) * mean(ys)) * len(xs) / (len(xs) - 1) assert np.isclose(covariance_2([1, 2, 3], [4, 5, 6]), 1) . We also verify our method with numpy. . np.cov(values, gas_prices)[0, 1] . 1.5856518696847882e+26 . covariance_2(values, gas_prices) . 1.585651869684888e+26 . Because the value of covariance really depends on the units of the variables, it is often hard to interpret and also to compare it with other covariences. . This is why correlation is an often preferred method as it adjusts the covariance by the variables&#39; standard deviation values. As a result, it bounds the end result into the $[-1, 1]$ domain making it comparable with other correlation values. . $ text{Corr(x, y)} = frac { text{Cov(x, y)} } { text{Std(x)} text{Std(y)}} $ . Correlation . correlation(values, gas_prices) . 0.035069533929694634 . Finally, we verify the result with numpy. . values.corr(gas_prices) . 0.03506953392969465 . def correlation(xs: Vector, ys: Vector) -&gt; float: return covariance(xs, ys) / (standard_deviation(xs) * standard_deviation(ys)) assert np.isclose(correlation([.1, .2, .3], [400, 500, 600]), 1) .",
            "url": "andrasnovoszath.com/2020/09/30/correlation.html",
            "relUrl": "/2020/09/30/correlation.html",
            "date": " • Sep 30, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Matrix algebra 'from scratch' with type annotations",
            "content": "This is the continuation of yesterday&#39;s practice, but now with Matrix operations. . from typing import List, Tuple, Callable . Vector = List[float] Matrix = List[List[float]] . A = [[1, 2, 3], [4, 5, 6]] B = [[1, 2], [3, 4], [5, 6]] . Shape . def shape(A: Matrix) -&gt; Tuple[int, int]: number_of_rows = len(A) number_of_columns = len(A[0]) if A else 0 return number_of_rows, number_of_columns assert shape([[1, 2, 3],[4, 5, 6]]) == (2, 3) . Get rows and columns . def get_row(A: Matrix, i: int) -&gt; Vector: return A[i] def get_columns(A: Matrix, i: int) -&gt; Vector: return [row[i] for row in A] assert get_columns([[1, 2, 3],[4, 5, 6]], 1) == [2, 5] . Create a new matrix from a function of $(i, j)$ . def make_matrix( number_of_rows: int, number_of_columns: int, entry_function: Callable[[int, int], float] ) -&gt; Matrix: return [ [entry_function(i, j) for j in range(number_of_columns)] for i in range(number_of_rows) ] assert make_matrix(3, 2, lambda i, j: i + j) == [[0, 1], [1, 2], [2, 3]] . def identity_matrix(n: int) -&gt; Matrix: return make_matrix(n, n, lambda i, j: 1 if i == j else 0) assert identity_matrix(3) == [[1, 0, 0], [0, 1, 0], [0, 0, 1]] . identity_matrix(5) . [[1, 0, 0, 0, 0], [0, 1, 0, 0, 0], [0, 0, 1, 0, 0], [0, 0, 0, 1, 0], [0, 0, 0, 0, 1]] .",
            "url": "andrasnovoszath.com/2020/09/29/matrix-algebra.html",
            "relUrl": "/2020/09/29/matrix-algebra.html",
            "date": " • Sep 29, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Vector Algebra 'from scratch" with type annotations",
            "content": "I looked into Joel Grus&#39; Data science from Scratch and found his approach really nice as he actively uses type annotations and assertions in his fuctions. . So, below I practice vector operation functions from the book. . from typing import List Vector = List[float] Vector . typing.List[float] . Vector addition and subtraction . $ vec{v} + vec{w} = begin{bmatrix} v_1 + w_1 ldots v_n + w_n end{bmatrix} $ . def add(vector1: Vector, vector2: Vector) -&gt; Vector: assert len(vector1) == len(vector2) return [v1 + v2 for v1, v2 in zip(vector1, vector2)] assert add([1, 2, 3], [4, 5, 6]) == [5, 7, 9] . $ vec{w} - vec{v} = begin{bmatrix} w_1 - v_1 ldots w_n - v_n end{bmatrix} $ . def subtract(vector1: Vector, vector2:Vector) -&gt; Vector: assert len(vector1) == len(vector2) return [v1 - v2 for v1, v2 in zip(vector1, vector2)] assert subtract([1, 2, 3], [4, 5, 6]) == [-3, -3, -3] . Vector sum . $ vec{v} + ldots + vec{w} = begin{bmatrix} v_1 + ldots + w_1 ldots v_n + ldots + w_n end{bmatrix} $ . def vector_sum(vectors: List[Vector]) -&gt; Vector: assert vectors vector_length = len(vectors[0]) assert all(len(v) == vector_length for v in vectors) sums = [0] * vector_length for vector in vectors: sums = add(sums, vector) return sums assert vector_sum([[1, 2, 3], [4, 5, 6], [7, 8, 9]]) == [12, 15, 18] . Scalar multiplication . $ c cdot vec{v} = begin{bmatrix} c cdot v_0 ldots c cdot v_n end{bmatrix} $ . def scalar_multiply(c: float, vector: Vector) -&gt; Vector: return [c * v for v in vector] assert scalar_multiply(4, [1, 2, 3]) == [4, 8, 12] . Vector mean . $ text{vector mean}( vec{w}, ldots, vec{v}) = begin{bmatrix} frac {v_1 + ldots + w_1}{n} ldots frac{v_n + ldots + w_n} {n} end{bmatrix} $ . def vector_mean(vectors: List[Vector]) -&gt; Vector: n = len(vectors) return scalar_multiply(1/n, vector_sum(vectors)) assert vector_mean([[1, 2], [3, 4], [5, 6]]) == [3, 4] . Dot product . $ vec{v} cdot vec{w} = sum_{i=1}^{n} v_1 w_1 + ldots + v_n w_n $ . def dot(vector1: Vector, vector2: Vector) -&gt; float: assert len(vector1) == len(vector2) return sum(v1 * v2 for v1, v2 in zip(vector1, vector2)) assert dot([1, 2, 3], [4, 5, 6]) == 32 . Vector sum of squares . $ vec{v} cdot vec{v} = sum_{i=1}^n v_1^2 + ldots + v_n^2 $ . def sum_of_squares(v: Vector) -&gt; Vector: return dot(v, v) assert sum_of_squares([1, 2, 3]) == 14 . Magnitude . $ || vec{v} || = sqrt {v_1^2 + ldots + v_n^2} $ . import math as m def magnitude(v: Vector) -&gt; Vector: return m.sqrt(sum_of_squares(v)) assert magnitude([1, 2, 3]) == m.sqrt(14) . Distance of two vectors . $ d = sqrt { (v_1 - w_1)^2 + ldots + (v_n - w_n)^2 } $ . def distance(vector1: Vector, vector2: Vector) -&gt; Vector: return magnitude(subtract(vector1, vector2)) .",
            "url": "andrasnovoszath.com/2020/09/28/vector-algebra.html",
            "relUrl": "/2020/09/28/vector-algebra.html",
            "date": " • Sep 28, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "Linear regression 'from scratch' to predict Ethereum gas prices from transaction values",
            "content": "In this post I go through the main steps of how to calculate a simple univariate linear regression model. . For the example I try to predict Ethereum daily average gas prices from daily average transaction values. I pull the data from the public Google data base with BigQuery. . Code and inspiration are based on Jason Brownlee&#39;s &quot;Machine Learning Algorithms from Scratch&quot; book . Libraries and data load . import os os.environ[&quot;GOOGLE_APPLICATION_CREDENTIALS&quot;]=os.path.expanduser(&quot;~/.credentials/Notebook bigquery-c422e406404b.json&quot;) . from google.cloud import bigquery client = bigquery.Client() . import altair as alt alt.data_transformers.disable_max_rows() . DataTransformerRegistry.enable(&#39;default&#39;) . query =&quot;&quot;&quot; SELECT EXTRACT(DATE FROM block_timestamp) AS date, AVG(value) AS value, AVG(gas_price) AS gas_price, FROM `bigquery-public-data.ethereum_blockchain.transactions` WHERE EXTRACT(YEAR FROM block_timestamp) = 2019 GROUP BY date ORDER BY date &quot;&quot;&quot; . values = client.query(query).to_dataframe(dtypes={&#39;value&#39;: float, &#39;gas_price&#39;: float}, date_as_object=False) values.head() . date value gas_price . 0 2019-01-01 | 3.719103e+18 | 1.431514e+10 | . 1 2019-01-02 | 4.649915e+18 | 1.349952e+10 | . 2 2019-01-03 | 4.188781e+18 | 1.269504e+10 | . 3 2019-01-04 | 6.958368e+18 | 1.418197e+10 | . 4 2019-01-05 | 8.167590e+18 | 2.410475e+10 | . Calculating linear regression . Linear regression makes predictions with the help of linear coefficient. For an univariate case, this is expressed as: . $ hat{y} = b_0 + b_1 x $ . Coefficients . We can estimate the $b_0$ and $b_1$ coeffients in the following ways: . $ b_1 = frac { sum_{i = 1}^{n} (x_i - bar{x} ) (y_i - bar{y} ) } { sum_{i=1}^{n} (x_i - bar{x})^2 } $ . $ b_0 = bar{y} - b_1 bar{x} $ . Where . $x$: the predictor variable | $y$: the variable to predict | $ bar{x} $ and $ bar{y}$ are their respective means | . Covariance and variance . We can also can get the $b_1$ coefficient from the variance and covariance: . Covariance: $ text{Cov}(x, y) = frac { sum_{i = 1}^{n} (x_i - bar{x} ) (y_i - bar{y} ) } { n } $ | Variance: $ delta^2 = frac { { sum_{i=1}^{n} (x_i - bar{x})^2 } } { n } $ | . From these, we can get . $ b_1 = frac { text{Cov} } { delta^2 } $ . Calculation steps . Accordingly, we need to calculate the following metrics: . Variable means for both $x$ and $y$ | Their deviations from the mean | Covariance of $x$ and $y$ | Variance of $x$ | The $b_1$ coeffcient | The $b_0$ coefficient | $ hat{y}$, that is, the predictions | Means . values.mean() . value 3.173648e+18 gas_price 1.616874e+10 dtype: float64 . Deviations from the mean . def deviation(array): return array - array.mean() . deviation(values[&#39;value&#39;]) . 0 5.454550e+17 1 1.476267e+18 2 1.015132e+18 3 3.784720e+18 4 4.993942e+18 ... 360 -1.004019e+18 361 -1.259703e+18 362 -1.050454e+18 363 -6.660476e+17 364 5.699485e+17 Name: value, Length: 365, dtype: float64 . Covariance . def covariance(arr1, arr2): return (deviation(arr1) * deviation(arr2)).sum() / len(arr1) . covariance(values[&#39;value&#39;], values[&#39;gas_price&#39;]) . 2.8148882775011114e+27 . Variance . def variance(arr): return (deviation(arr) ** 2).sum() / len(arr) . variance(values[&#39;value&#39;]) . 1.3829873312251886e+36 . The $b_1$ coefficient . def b1(arr1, arr2): return covariance(arr1, arr2) / variance(arr1) . b1(values[&#39;value&#39;], values[&#39;gas_price&#39;]) . 2.035368086132359e-09 . The $b_0$ coefficient . def b0(arr1, arr2): return arr2.mean() - b1(arr1, arr2) * arr1.mean() . b_0 = b0(values[&#39;value&#39;], values[&#39;gas_price&#39;]) b_0 . 9709198058.151459 . Predictions . values[&#39;predictions&#39;] = b_0 + values[&#39;value&#39;] * b_1 . values[&#39;predictions&#39;] . 0 1.727894e+10 1 1.917349e+10 2 1.823491e+10 3 2.387204e+10 4 2.633325e+10 ... 360 1.412519e+10 361 1.360478e+10 362 1.403068e+10 363 1.481309e+10 364 1.732880e+10 Name: predictions, Length: 365, dtype: float64 . values.head() . date value gas_price predictions . 0 2019-01-01 | 3.719103e+18 | 1.431514e+10 | 1.727894e+10 | . 1 2019-01-02 | 4.649915e+18 | 1.349952e+10 | 1.917349e+10 | . 2 2019-01-03 | 4.188781e+18 | 1.269504e+10 | 1.823491e+10 | . 3 2019-01-04 | 6.958368e+18 | 1.418197e+10 | 2.387204e+10 | . 4 2019-01-05 | 8.167590e+18 | 2.410475e+10 | 2.633325e+10 | . Plotting the results . First we transform the data into long format . to_plot = values.melt(id_vars=[&#39;date&#39;], value_vars=[&#39;gas_price&#39;, &#39;predictions&#39;], var_name=&#39;status&#39;) to_plot.head() . date status value . 0 2019-01-01 | gas_price | 1.431514e+10 | . 1 2019-01-02 | gas_price | 1.349952e+10 | . 2 2019-01-03 | gas_price | 1.269504e+10 | . 3 2019-01-04 | gas_price | 1.418197e+10 | . 4 2019-01-05 | gas_price | 2.410475e+10 | . Then, we plot the results. . chart = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;, axis=alt.Axis(format=(&quot;%x&quot;), labelAngle=270)), alt.Y(&#39;value:Q&#39;, axis=alt.Axis(format=&quot;,.2e&quot;), scale=alt.Scale(type=&#39;log&#39;)), color=alt.Color(&#39;status:N&#39;) ).properties(width=600) label = alt.selection_single( encodings=[&#39;x&#39;], on=&#39;mouseover&#39;, nearest=True, empty=&#39;none&#39; ) alt.layer( chart, chart.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), alt.Chart().mark_rule(color=&#39;darkgray&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), chart.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;black&#39;, strokeWidth=0.5).encode( text=alt.Text(&#39;value:Q&#39;, format=&#39;,.2e&#39;) ).transform_filter(label), chart.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=alt.Text(&#39;value:Q&#39;, format=&#39;,.2e&#39;) ).transform_filter(label), data=to_plot ).properties(title=&#39;Gas prices and their predictions (log scale)&#39;, width=600, height=400) . It is a bit hard to assess the performance of the results just by looking at them, but at least it seems to generate values within the same ballpark. We could generate metrics as RMSE for reference. . Obvious improvements . remove the outliers | include past values | .",
            "url": "andrasnovoszath.com/2020/09/27/linear-regression.html",
            "relUrl": "/2020/09/27/linear-regression.html",
            "date": " • Sep 27, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "Plotting Ethereum transaction value and gas prices with BigQuery and Altair",
            "content": "As part of getting a better handle on blockchain data, BigQuery, Altair, and Machine Learning, I pulled some Ethereum transaction data and plotted it. . import os os.environ[&quot;GOOGLE_APPLICATION_CREDENTIALS&quot;]=os.path.expanduser(&quot;~/.credentials/Notebook bigquery-c422e406404b.json&quot;) . import altair as alt alt.data_transformers.disable_max_rows() . DataTransformerRegistry.enable(&#39;default&#39;) . from google.cloud import bigquery client = bigquery.Client() . query =&quot;&quot;&quot; SELECT EXTRACT(DATE FROM block_timestamp) AS date, AVG(value) AS average_value, AVG(gas_price) AS average_gas_price, FROM `bigquery-public-data.ethereum_blockchain.transactions` WHERE EXTRACT(YEAR FROM block_timestamp) = 2019 GROUP BY date ORDER BY date &quot;&quot;&quot; . We calculate some basic statistics on raw transaction value data for each day over 2019. . schema = client.get_table(&quot;bigquery-public-data.ethereum_blockchain.transactions&quot;).schema schema . [SchemaField(&#39;hash&#39;, &#39;STRING&#39;, &#39;REQUIRED&#39;, &#39;Hash of the transaction&#39;, (), None), SchemaField(&#39;nonce&#39;, &#39;INTEGER&#39;, &#39;REQUIRED&#39;, &#39;The number of transactions made by the sender prior to this one&#39;, (), None), SchemaField(&#39;transaction_index&#39;, &#39;INTEGER&#39;, &#39;REQUIRED&#39;, &#39;Integer of the transactions index position in the block&#39;, (), None), SchemaField(&#39;from_address&#39;, &#39;STRING&#39;, &#39;REQUIRED&#39;, &#39;Address of the sender&#39;, (), None), SchemaField(&#39;to_address&#39;, &#39;STRING&#39;, &#39;NULLABLE&#39;, &#39;Address of the receiver. null when its a contract creation transaction&#39;, (), None), SchemaField(&#39;value&#39;, &#39;NUMERIC&#39;, &#39;NULLABLE&#39;, &#39;Value transferred in Wei&#39;, (), None), SchemaField(&#39;gas&#39;, &#39;INTEGER&#39;, &#39;NULLABLE&#39;, &#39;Gas provided by the sender&#39;, (), None), SchemaField(&#39;gas_price&#39;, &#39;INTEGER&#39;, &#39;NULLABLE&#39;, &#39;Gas price provided by the sender in Wei&#39;, (), None), SchemaField(&#39;input&#39;, &#39;STRING&#39;, &#39;NULLABLE&#39;, &#39;The data sent along with the transaction&#39;, (), None), SchemaField(&#39;receipt_cumulative_gas_used&#39;, &#39;INTEGER&#39;, &#39;NULLABLE&#39;, &#39;The total amount of gas used when this transaction was executed in the block&#39;, (), None), SchemaField(&#39;receipt_gas_used&#39;, &#39;INTEGER&#39;, &#39;NULLABLE&#39;, &#39;The amount of gas used by this specific transaction alone&#39;, (), None), SchemaField(&#39;receipt_contract_address&#39;, &#39;STRING&#39;, &#39;NULLABLE&#39;, &#39;The contract address created, if the transaction was a contract creation, otherwise null&#39;, (), None), SchemaField(&#39;receipt_root&#39;, &#39;STRING&#39;, &#39;NULLABLE&#39;, &#39;32 bytes of post-transaction stateroot (pre Byzantium)&#39;, (), None), SchemaField(&#39;receipt_status&#39;, &#39;INTEGER&#39;, &#39;NULLABLE&#39;, &#39;Either 1 (success) or 0 (failure) (post Byzantium)&#39;, (), None), SchemaField(&#39;block_timestamp&#39;, &#39;TIMESTAMP&#39;, &#39;REQUIRED&#39;, &#39;Timestamp of the block where this transaction was in&#39;, (), None), SchemaField(&#39;block_number&#39;, &#39;INTEGER&#39;, &#39;REQUIRED&#39;, &#39;Block number where this transaction was in&#39;, (), None), SchemaField(&#39;block_hash&#39;, &#39;STRING&#39;, &#39;REQUIRED&#39;, &#39;Hash of the block where this transaction was in&#39;, (), None)] . values = client.query(query).to_dataframe(dtypes={&#39;average_value&#39;: float, &#39;average_gas_price&#39;: float}, date_as_object=False) values.head() . date average_value average_gas_price . 0 2019-01-01 | 3.719103e+18 | 1.431514e+10 | . 1 2019-01-02 | 4.649915e+18 | 1.349952e+10 | . 2 2019-01-03 | 4.188781e+18 | 1.269504e+10 | . 3 2019-01-04 | 6.958368e+18 | 1.418197e+10 | . 4 2019-01-05 | 8.167590e+18 | 2.410475e+10 | . chart = alt.Chart(values).mark_line().encode( alt.X(&#39;date:T&#39;, axis=alt.Axis(format=(&quot;%x&quot;), labelAngle=270)) ).properties(width=600) alt.layer( chart.encode(alt.Y(&#39;average_value:Q&#39;, axis=alt.Axis(format=&quot;,.2e&quot;)), color=alt.value(&#39;darkred&#39;), opacity=alt.value(0.65)), chart.encode(alt.Y(&#39;average_gas_price&#39;, axis=alt.Axis(format=&quot;,.2e&quot;))) ).resolve_scale(y=&#39;independent&#39;) .",
            "url": "andrasnovoszath.com/2020/09/26/ethereum_value_gas_price.html",
            "relUrl": "/2020/09/26/ethereum_value_gas_price.html",
            "date": " • Sep 26, 2020"
        }
        
    
  
    
        ,"post5": {
            "title": "How to use BigQuery in a Jupyter notebook?",
            "content": "When we try to analyze huge datasets (like blockchain data) through BigQuery, it is useful to run the data load and conversion in the cloud and use our local environment mostly for the final transformations and visualization. For these cases, a jupyter notebook seems to be a fitting environment. . There are at least three main ways by which we can access BigQuery data from a notebook: . The most simple one is to use the pandas-gbq library (we already covered it in this previous post) | Using google&#39;s BigQuery notebook extension | The BigQuery python API | Here we will introduce the last two versions. . (This post is based on this official tutorial.) . Setting credentials . Compared to the pandas-gbq library, we need to define the credentials explicitly. . import os os.environ[&quot;GOOGLE_APPLICATION_CREDENTIALS&quot;] = os.path.expanduser(&quot;~/.credentials/Notebook bigquery-c422e406404b.json&quot;) . BigQuery extension . For the extension to work, we need to install the google-cloud-bigquery library. . conda install -c conda-forge google-cloud-bigquery . Then, we load the bigquery extension . %load_ext google.cloud.bigquery . By using the %%bigquery magic command, we immediately define the result of a query as a pandas dataframe. . Here, we pull the total Ethereum token spendings for each day during 2019. . %%bigquery token_transfers SELECT SUM(SAFE_CAST(value AS FLOAT64)) AS total_spent, SAFE_CAST(EXTRACT(DATE FROM block_timestamp) AS DATETIME) AS date FROM `bigquery-public-data.ethereum_blockchain.token_transfers` WHERE EXTRACT(YEAR FROM block_timestamp) = 2019 GROUP BY date ORDER BY date DESC . token_transfers . total_spent date . 0 2.331472e+79 | 2019-12-31 | . 1 4.078401e+79 | 2019-12-30 | . 2 1.773262e+79 | 2019-12-29 | . 3 3.553959e+79 | 2019-12-28 | . 4 2.751157e+79 | 2019-12-27 | . ... ... | ... | . 360 4.226642e+78 | 2019-01-05 | . 361 4.122376e+80 | 2019-01-04 | . 362 1.804472e+80 | 2019-01-03 | . 363 5.833122e+78 | 2019-01-02 | . 364 4.245749e+78 | 2019-01-01 | . 365 rows × 2 columns . import altair as alt alt.data_transformers.disable_max_rows() label = alt.selection_single( # encodings=[&#39;x&#39;], on=&#39;mouseover&#39;, nearest=True, empty=&#39;none&#39; ) chart = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;total_spent:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)) ) alt.layer( chart, chart.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), alt.Chart().mark_rule(color=&#39;darkgray&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), chart.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=alt.Text(&#39;total_spent:Q&#39;, format=&#39;,.2e&#39;) ).transform_filter(label), chart.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=alt.Text(&#39;total_spent:Q&#39;, format=&#39;,.2e&#39;) ).transform_filter(label), data=token_transfers ).properties(width=600, height=400, title=&#39;Daily token spending during 2019 (log scale)&#39;) . BigQuery module . from google.cloud import bigquery client = bigquery.Client() . query =&quot;&quot;&quot; SELECT from_address, SUM(SAFE_CAST(value AS FLOAT64)) AS total_spent, AVG(SAFE_CAST(value AS INT64)) AS average_spent, COUNT(1) AS times_spent FROM `bigquery-public-data.ethereum_blockchain.token_transfers` WHERE EXTRACT(YEAR FROM block_timestamp) = 2019 AND EXTRACT(MONTH FROM block_timestamp) = 09 AND EXTRACT(DAY FROM block_timestamp) = 24 AND EXTRACT(HOUR FROM block_timestamp) = 12 GROUP BY from_address ORDER BY times_spent DESC&quot;&quot;&quot; . For this example we calculate the number of spendings and their total and average values from addresses over a hour. . df = client.query(query).to_dataframe() df . from_address total_spent average_spent times_spent . 0 0x7ba732b1bb952155b720250b477ce154e19ad62f | 1.686990e+12 | 1.963900e+09 | 859 | . 1 0x262e4155e8c5519e668ec26f353d75dd9c18e78f | 3.132121e+23 | NaN | 365 | . 2 0xbd8da72e2f42f5c68b59ee02c2245599ccd702dc | 2.592921e+24 | NaN | 294 | . 3 0x0000000000000000000000000000000000000000 | 7.401857e+24 | 3.058231e+17 | 268 | . 4 0xa71c8bae673f99ac6c0f32c56efc89a8ddb9a501 | 3.896125e+12 | 1.504295e+10 | 259 | . ... ... | ... | ... | ... | . 5279 0xcd338611d74243844f3190b621eb781db53d20b4 | 1.630439e+23 | NaN | 1 | . 5280 0xa9d6b0ad82e46db1895a412ec96b00e18bf95b49 | 1.000000e+09 | 1.000000e+09 | 1 | . 5281 0x4aee792a88edda29932254099b9d1e06d537883f | 5.740766e+22 | NaN | 1 | . 5282 0x71e29ec9e13a39062269fc5c8cba155bb850b23a | 2.270000e+10 | 2.270000e+10 | 1 | . 5283 0x140d6fac06496b21efd086e107d5eca1a16592b3 | 4.335616e+08 | 4.335616e+08 | 1 | . 5284 rows × 4 columns . alt.Chart(df).mark_rect().encode( alt.X(&#39;times_spent:Q&#39;, bin=alt.BinParams(maxbins=20)), alt.Y(&#39;total_spent:Q&#39;, bin=alt.BinParams(maxbins=20)), alt.Color(&#39;count()&#39;), alt.Tooltip(&#39;count()&#39;) ).properties(title=&#39;Total spending value and spending frequency&#39;) . As expected, there are a few number of addresses responsible for the highest spending frequency and the highest spending value during that hour. .",
            "url": "andrasnovoszath.com/2020/09/25/bigquery-jupyter.html",
            "relUrl": "/2020/09/25/bigquery-jupyter.html",
            "date": " • Sep 25, 2020"
        }
        
    
  
    
        ,"post6": {
            "title": "Accessing Ethereum token transaction data with ``pandas-gbq``",
            "content": "One way to access blockchain bigquery data with Google BigQuery is to use the pandas-gbq library. . It makes it really easy to take a BigQuery SQL query and download its results as a pandas DataFrame. . Big Query authentication . In order to use this library, you need to authenticate a credential with BigQuery. I found creating a &#39;Service account&#39; to be the most meaningful. You can follow the steps here. . Installation . Installation with conda is straightforward. . $ conda install pandas-gbq --channel conda-forge . Basic usage . We can download a table by defining an SQL query and passing it to the read_gbq method. . import altair as alt import pandas_gbq . As Ethereum data is big, we constrain our query to a single hour. . sql = &quot;&quot;&quot; SELECT block_timestamp, value FROM `bigquery-public-data.ethereum_blockchain.token_transfers` WHERE EXTRACT(YEAR FROM block_timestamp) = 2019 AND EXTRACT(MONTH FROM block_timestamp) = 09 AND EXTRACT(DAY FROM block_timestamp) = 24 AND EXTRACT(HOUR FROM block_timestamp) = 12 &quot;&quot;&quot; . token = pandas_gbq.read_gbq(sql, project_id=&quot;pandas-gbq-test-290508&quot;) . Downloading: 100%|██████████| 15392/15392 [00:03&lt;00:00, 4846.48rows/s] . token . block_timestamp value . 0 2019-09-24 12:47:43+00:00 | 50058584428 | . 1 2019-09-24 12:47:43+00:00 | 30000000 | . 2 2019-09-24 12:47:43+00:00 | 14069000000 | . 3 2019-09-24 12:47:43+00:00 | 170000000 | . 4 2019-09-24 12:47:43+00:00 | 59478199 | . ... ... | ... | . 15387 2019-09-24 12:40:54+00:00 | 3577720000000000000000 | . 15388 2019-09-24 12:40:54+00:00 | 2314760000000000000000 | . 15389 2019-09-24 12:40:54+00:00 | 2399000000000000000000 | . 15390 2019-09-24 12:40:54+00:00 | 5701000000000000000000 | . 15391 2019-09-24 12:40:54+00:00 | 3608000000000000000000 | . 15392 rows × 2 columns . Data transformation . After loading the dataset, it requires some transformations. . First, we want to sort it by timestamps. . token.dtypes . block_timestamp datetime64[ns, UTC] value object dtype: object . token = token.sort_values(&#39;block_timestamp&#39;) . Then we want to convert the values column to float. . token[&#39;value&#39;] = token[&#39;value&#39;].astype(float) . Visualization . As altair does not allow visualization with more than 5000 rows, we need to manually set it possible. . alt.data_transformers.disable_max_rows() . DataTransformerRegistry.enable(&#39;default&#39;) . We plot the average transaction values by minute. As we there are a few number of very high value transactions, we use a log scale. . alt.Chart(token).mark_line().encode( alt.X(&#39;utchoursminutes(block_timestamp):T&#39;), alt.Y(&#39;average(value):Q&#39;, scale=alt.Scale(type=&#39;log&#39;)) ).properties(width=800, title=&#39;Average token transaction values (log scale)&#39;) .",
            "url": "andrasnovoszath.com/2020/09/24/pandas-gbq.html",
            "relUrl": "/2020/09/24/pandas-gbq.html",
            "date": " • Sep 24, 2020"
        }
        
    
  
    
        ,"post7": {
            "title": "Random prediction",
            "content": "When we try to build a prediction algorithm, it is a useful practice to first try out a baseline algorithm and see how they perform. Here we will describe a case of random prediction. . (Inspiration and examples are from Jason Brownlee&#39;s Machine Learning Algorithms from Scratch book.) . import pandas as pd import altair as alt import numpy as np from vega_datasets import data . np.random.seed(42) . For the examples we will use the vega &#39;volcano&#39; dataset. The width and height values are constant, so we work only with the values column. . volcano = data(&#39;volcano&#39;) volcano.head() . width height values . 0 87 | 61 | 103 | . 1 87 | 61 | 104 | . 2 87 | 61 | 104 | . 3 87 | 61 | 105 | . 4 87 | 61 | 105 | . The random prediction algorithm . takes a training and a test set | generates the prediction by selecting random elements from the training set | We split the dataset to training and test sets, by taking the first 2/3 and last 1/3 of the data respectively. . train, test = volcano.iloc[: volcano.shape[0] * 2 // 3, :], volcano.iloc[volcano.shape[0] * 2 // 3 :, :] . A small check that the split did not leave out an entry or did not result in an overlap. . assert train.index[-1] + 1 == test.index[0] . We plot the training and the test sets, respectively. . alt.Chart(train.reset_index()).mark_line().encode(alt.Y(&#39;values&#39;), alt.X(&#39;index:Q&#39;), alt.Tooltip(&#39;values&#39;)).interactive().properties(title=&#39;Train data&#39;) . alt.Chart(test.reset_index()).mark_line().encode(alt.Y(&#39;values&#39;), alt.X(&#39;index:Q&#39;), alt.Tooltip(&#39;values&#39;)).interactive().properties(title=&#39;Test data&#39;) . First, we generate the random predictions with replacement. That is, the same values can occur more than once. . predictions = np.random.choice(train[&#39;values&#39;], size=test.shape[0], replace=True) predictions . array([166, 179, 96, ..., 105, 157, 95]) . We calculate the error terms . errors = test[&#39;values&#39;] - predictions errors . 3538 -16 3539 -29 3540 54 3541 54 3542 56 .. 5302 -8 5303 2 5304 -7 5305 -60 5306 2 Name: values, Length: 1769, dtype: int64 . We calculate the root mean squared error of the predictions. . def calculate_rmse(observed, predicted): return np.sqrt(sum((observed - predicted) ** 2)/len(observed)) . The RMSE is around 34.59 which stands for about 1.34 STD. . rmse = calculate_rmse(test[&#39;values&#39;], predictions) rmse . 34.93380641982711 . For reference, if we would simply use the simple mean, we would get an RMSE of 18.42, . calculate_rmse(test[&#39;values&#39;], test[&#39;values&#39;].mean()) . 18.420942507684327 . Let&#39;s plot the results . to_compare = pd.concat( [ test[&#39;values&#39;].rename(&#39;observed&#39;).reset_index(drop=True), pd.Series(predictions).rename(&#39;predictions&#39;).reset_index(drop=True) ], axis=1 ).stack().reset_index().rename(columns={&#39;level_1&#39;: &#39;status&#39;, &#39;level_0&#39;: &#39;index&#39;, 0: &#39;values&#39;}) to_compare . index status values . 0 0 | observed | 150 | . 1 0 | predictions | 166 | . 2 1 | observed | 150 | . 3 1 | predictions | 179 | . 4 2 | observed | 150 | . ... ... | ... | ... | . 3533 1766 | predictions | 105 | . 3534 1767 | observed | 97 | . 3535 1767 | predictions | 157 | . 3536 1768 | observed | 97 | . 3537 1768 | predictions | 95 | . 3538 rows × 3 columns . alt.Chart(to_compare).mark_line().encode( alt.X(&#39;index:Q&#39;), alt.Y(&#39;values&#39;), alt.Color(&#39;status&#39;), alt.OpacityValue(0.7) ).properties(width=1200, title=&#39;Predictions with replacement&#39;) . We put the steps into a function and rerun the prediction without replacement. . def predict_randomly(train, test, replace=True): predictions = np.random.choice(train, size=test.shape[0], replace=True) errors = test - predictions rmse = calculate_rmse(test, predictions) return predictions, rmse . predictions, rmse = predict_randomly(train[&#39;values&#39;], test[&#39;values&#39;], replace=False) rmse . 35.90392934559341 . Finally, we also put the plotting steps into a function . def plot_predictions(observed, predicted): to_compare = pd.concat( [observed.rename(&#39;observed&#39;).reset_index(drop=True), pd.Series(predicted).rename(&#39;predictions&#39;).reset_index(drop=True)], axis=1 ).stack().reset_index().rename(columns={&#39;level_1&#39;: &#39;status&#39;, &#39;level_0&#39;: &#39;index&#39;, 0: &#39;values&#39;}) chart = alt.Chart(to_compare).mark_line().encode( alt.X(&#39;index:Q&#39;), alt.Y(&#39;values&#39;), alt.Color(&#39;status&#39;), alt.OpacityValue(0.7) ).properties(width=1200) chart.display() . The predictions without replacement . plot_predictions(test[&#39;values&#39;], predictions) .",
            "url": "andrasnovoszath.com/2020/09/23/random_predictions.html",
            "relUrl": "/2020/09/23/random_predictions.html",
            "date": " • Sep 23, 2020"
        }
        
    
  
    
        ,"post8": {
            "title": "How does data standardization work?",
            "content": "With standardization, we transform the data so its distribution&#39;s center will become 0 and its standard deviation will become 1. . import altair as alt from vega_datasets import data . For this demo, we use the sp500 price index. . sp500 = data(&#39;sp500&#39;) sp500.head() . date price . 0 2000-01-01 | 1394.46 | . 1 2000-02-01 | 1366.42 | . 2 2000-03-01 | 1498.58 | . 3 2000-04-01 | 1452.43 | . 4 2000-05-01 | 1420.60 | . alt.Chart(sp500).mark_line(point=True, strokeWidth=1).encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price&#39;), alt.Tooltip(&#39;price&#39;) ).configure_point(size=10) . The original distribution of prices. . alt.Chart(sp500).mark_bar().encode( x=alt.X(&#39;price:Q&#39;, bin=alt.BinParams(maxbins=20)), y=alt.Y(&#39;count()&#39;), tooltip=alt.Tooltip([&#39;count()&#39;]) ) . We get the standardized value by dividing the values&#39; difference from the mean by the standard deviation. . $ text{standardized value} = frac { text{value} - text{mean}} { text{standard deviation}} $ . The mean and standard deviation of the prices . print(f&quot;&quot;&quot; mean: {sp500[&#39;price&#39;].mean():.2f} std: {sp500[&#39;price&#39;].std():.2f} &quot;&quot;&quot;) . mean: 1184.43 std: 195.41 . We standardize the data set. . standardized_prices = sp500.copy() standardized_prices[&#39;price&#39;] = (sp500[&#39;price&#39;] - sp500[&#39;price&#39;].mean()) / sp500[&#39;price&#39;].std() standardized_prices . date price . 0 2000-01-01 | 1.074812 | . 1 2000-02-01 | 0.931317 | . 2 2000-03-01 | 1.607646 | . 3 2000-04-01 | 1.371473 | . 4 2000-05-01 | 1.208583 | . ... ... | ... | . 118 2009-11-01 | -0.454452 | . 119 2009-12-01 | -0.354814 | . 120 2010-01-01 | -0.565809 | . 121 2010-02-01 | -0.409111 | . 122 2010-03-01 | -0.225086 | . 123 rows × 2 columns . alt.Chart(standardized_prices).mark_bar().encode( x=alt.X(&#39;price:Q&#39;, bin=alt.BinParams(maxbins=20)), y=alt.Y(&#39;count()&#39;), tooltip=alt.Tooltip([&#39;count()&#39;]) ) . The new mean and standard deviation . print(f&quot;&quot;&quot; mean: {standardized_prices[&#39;price&#39;].mean():.2f} std: {standardized_prices[&#39;price&#39;].std():.2f} &quot;&quot;&quot;) . mean: -0.00 std: 1.00 . When plotted on the dates, it provides a similar graph as before, but now with a value domain around 0. . alt.Chart(standardized_prices).mark_line(point=True, strokeWidth=1).encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price&#39;), alt.Tooltip(&#39;price&#39;) ).configure_point(size=10) .",
            "url": "andrasnovoszath.com/2020/09/22/standardization.html",
            "relUrl": "/2020/09/22/standardization.html",
            "date": " • Sep 22, 2020"
        }
        
    
  
    
        ,"post9": {
            "title": "Data scale normalization",
            "content": "Normalization is a common technique used in machine learning to render the scales of different magnitudes to a common range between 0 and 1. . Here we demonstrate how this is done with pandas and altair. . Original inspiration: (Jason Brownlee: Machine Learning Algorithms from Scratch)[https://machinelearningmastery.com/machine-learning-algorithms-from-scratch/] . import altair as alt # alt.renderers.enable(&#39;default&#39;) alt.renderers . RendererRegistry(active=&#39;default&#39;, registered=[&#39;colab&#39;, &#39;default&#39;, &#39;html&#39;, &#39;json&#39;, &#39;jupyterlab&#39;, &#39;kaggle&#39;, &#39;mimetype&#39;, &#39;notebook&#39;, &#39;nteract&#39;, &#39;png&#39;, &#39;svg&#39;, &#39;zeppelin&#39;]) . from vega_datasets import data . We use the Gapminder health and income dataset . health_income = data(&#39;gapminder-health-income&#39;) health_income.head() . country income health population . 0 Afghanistan | 1925 | 57.63 | 32526562 | . 1 Albania | 10620 | 76.00 | 2896679 | . 2 Algeria | 13434 | 76.50 | 39666519 | . 3 Andorra | 46577 | 84.10 | 70473 | . 4 Angola | 7615 | 61.00 | 25021974 | . income_domain = [health_income[&#39;income&#39;].min(), health_income[&#39;income&#39;].max()] health_domain = [health_income[&#39;health&#39;].min(), health_income[&#39;health&#39;].max()] alt.Chart(health_income).mark_point().encode( alt.X(&#39;income:Q&#39;, scale=alt.Scale(domain=income_domain)), alt.Y(&#39;health:Q&#39;, scale=alt.Scale(domain=health_domain)), alt.Size(&#39;population:Q&#39;), alt.Tooltip(&#39;country:N&#39;) ).properties(height=600, width=800) . The process: . Take the values&#39; difference from the smallest one; | Take the value range, that is, the difference between the largest and smallest values; | Divide the reduced values with the range. | $ text {scaled value} = frac{value - min} {max - min} $ . The first step ensures that the smallest value will become 0. Dividing the reduced values by the range &#39;compresses&#39; the values so the new maximum becomes 1. . quantitative_columns = [&#39;income&#39;, &#39;health&#39;, &#39;population&#39;] . The original minimum and maximum values . health_income.loc[health_income[quantitative_columns].idxmin(), :] . country income health population . 32 Central African Republic | 599 | 53.8 | 4900274 | . 93 Lesotho | 2598 | 48.5 | 2135022 | . 105 Marshall Islands | 3661 | 65.1 | 52993 | . minimums = health_income[quantitative_columns].min() minimums . income 599.0 health 48.5 population 52993.0 dtype: float64 . health_income.loc[health_income[quantitative_columns].idxmax(), :] . country income health population . 134 Qatar | 132877 | 82.0 | 2235355 | . 3 Andorra | 46577 | 84.1 | 70473 | . 35 China | 13334 | 76.9 | 1376048943 | . maximums = health_income[quantitative_columns].max() maximums . income 1.328770e+05 health 8.410000e+01 population 1.376049e+09 dtype: float64 . Difference of values from the column minimum . health_income[quantitative_columns] - minimums . income health population . 0 1326.0 | 9.13 | 32473569.0 | . 1 10021.0 | 27.50 | 2843686.0 | . 2 12835.0 | 28.00 | 39613526.0 | . 3 45978.0 | 35.60 | 17480.0 | . 4 7016.0 | 12.50 | 24968981.0 | . ... ... | ... | ... | . 182 5024.0 | 28.00 | 93394608.0 | . 183 3720.0 | 26.70 | 4615473.0 | . 184 3288.0 | 19.10 | 26779222.0 | . 185 3435.0 | 10.46 | 16158774.0 | . 186 1202.0 | 11.51 | 15549758.0 | . 187 rows × 3 columns . Value ranges: the difference between the maximum and the minimum . maximums - minimums . income 1.322780e+05 health 3.560000e+01 population 1.375996e+09 dtype: float64 . Let&#39;s normalize the dataset . def normalize_dataset(dataset, quantitative_columns): dataset = dataset.copy() minimums = dataset[quantitative_columns].min() maximums = dataset[quantitative_columns].max() dataset[quantitative_columns] = (dataset[quantitative_columns] - minimums) / (maximums - minimums) return dataset . normalized_health_income = normalize_dataset(health_income, quantitative_columns) normalized_health_income . country income health population . 0 Afghanistan | 0.010024 | 0.256461 | 0.023600 | . 1 Albania | 0.075757 | 0.772472 | 0.002067 | . 2 Algeria | 0.097030 | 0.786517 | 0.028789 | . 3 Andorra | 0.347586 | 1.000000 | 0.000013 | . 4 Angola | 0.053040 | 0.351124 | 0.018146 | . ... ... | ... | ... | ... | . 182 Vietnam | 0.037981 | 0.786517 | 0.067874 | . 183 West Bank and Gaza | 0.028123 | 0.750000 | 0.003354 | . 184 Yemen | 0.024857 | 0.536517 | 0.019462 | . 185 Zambia | 0.025968 | 0.293820 | 0.011743 | . 186 Zimbabwe | 0.009087 | 0.323315 | 0.011301 | . 187 rows × 4 columns . The new minimum and maximum values . normalized_health_income.loc[normalized_health_income[quantitative_columns].idxmin(), :] . country income health population . 32 Central African Republic | 0.000000 | 0.148876 | 0.003523 | . 93 Lesotho | 0.015112 | 0.000000 | 0.001513 | . 105 Marshall Islands | 0.023148 | 0.466292 | 0.000000 | . normalized_health_income.loc[normalized_health_income[quantitative_columns].idxmax(), :] . country income health population . 134 Qatar | 1.000000 | 0.941011 | 0.001586 | . 3 Andorra | 0.347586 | 1.000000 | 0.000013 | . 35 China | 0.096275 | 0.797753 | 1.000000 | . Plotting the normalized data, we got the same results, but with the income, health, and population scales all normalized to the [0, 1] range. . Maximum values . alt.Chart(normalized_health_income).mark_point().encode( alt.X(&#39;income:Q&#39;,), alt.Y(&#39;health:Q&#39;), alt.Size(&#39;population:Q&#39;), alt.Tooltip(&#39;country:N&#39;) ).properties(height=600, width=800) .",
            "url": "andrasnovoszath.com/2020/09/21/normalize-scale.html",
            "relUrl": "/2020/09/21/normalize-scale.html",
            "date": " • Sep 21, 2020"
        }
        
    
  
    
        ,"post10": {
            "title": "How to categorize your notes?",
            "content": "I take notes . I take lots of notes. Many of them are ideas to research, explore, and think about, while others are rather tasks or possible projects. . As I collect them continuously, their number grows. After a while, it becomes hard to look through them and prioritize for action. . The obvious way to do this, of course, is to categorize them under topics or themes. . Concept hierarchies . This produces a hierarchical tree-like structure. There a few problems with this: . There always be some idea or task which does not belong to a single category alone but under multiple ones. | As the number of notes and, therefore, the number of categories and sub-categories becomes high, you need to dig very deep to find some notes. If the hierarchies are obvious, this is not really a problem. However, as the number of hierarchy levels and branches grow, the previous issue (i.e., notes belonging under multiple categories) become even more prominent. | . Concept drift . There is also the problem of concept drift. While we refer to a particular thing or category with a specific term, later on, we start to refer to it with a related but different one. Similarly, we do the same with categories. . This shift can happen for multiple reasons, but the most obvious ones are terminological shifts or our changing ‘use’ of the original idea or category. . Concept drift, deep hierarchies, and the overlapping category boundaries can lead to the ‘forgetting’ of notes. They can also lead to making multiple, ‘almost similar’ but ‘slightly different’ versions of them at different places of the hierarchy. . And, even whether the relationship between two ideas should be hierarchical or not also can change from use case to use case. . Chaos everywhere . And note-taking is just the most obvious example for me. I had the same experience with software development (both as writing code and managing versions), technical writing, academic research, work organization, file organization, etc. .",
            "url": "andrasnovoszath.com/note-taking/categorization/hierarchies/concept%20drift/chaos%20&%20order/2020/09/20/note-categorization-pain.html",
            "relUrl": "/note-taking/categorization/hierarchies/concept%20drift/chaos%20&%20order/2020/09/20/note-categorization-pain.html",
            "date": " • Sep 20, 2020"
        }
        
    
  
    
        ,"post11": {
            "title": "The sum of stock trading",
            "content": "I thought further about my previous position on whether stock trading is a zero-sum game. Now I see a bigger space for it not to be one. . Trading growth stocks . The situation I am thinking about is an argument I found during my google search. . Very simply, one buys a stock at one point in time and sells it to someone else in another. . Let’s take the ‘classical’ position on stocks and consider it merely an investment asset. In this case, we can argue that stock buyers lent money to a company in exchange for a part of its profits. They sell the stocks when it wants to get their money back, say for their retirement. . Here the value comes for the original owner in that the price will be higher when they sell. . This is the same hope the sellers have for the time when they want to realize their investment. That is, when the original buyers sell the stock, the new buyers can get a ‘fair’ price if, in that given time, the stock is still promising some price growth. . This seems more positive-sum for me. However, this view works if we strip away the ‘speculative’ (?), arbitrage-seeking trading aspect of security trading. In this view, stock trading seems to resemble the trade of a good tool one does not need anymore. . Remaining questions . There are, of course, a number of issues still unclear for me. . The first is whether ‘speculation’ or ‘arbitrage seeking’ inevitably leads to a zero-sum outcome? Also, can we separate ‘straightforward’ investment from these activities? . The next is about the buying price: when is the price too high/low? . The buyer could always want to ask for a higher price. Aren’t selling a stock at a too high price basically strips away the seller’s possible gains? . On the other hand, one might argue that, if the price reflects the market price, there is less space for unfairness (or at least not directly between the two parties). . Third, there is the question of whether a company can grow infinitely or trading stock is always based on this false promise. . Finally, how do we calculate the outcomes when the ‘gain’, in big part, comes from luck and decisions made years before their realization. .",
            "url": "andrasnovoszath.com/stock%20trading/zero-sum/finance/valuation/2020/09/19/stock-trading-sum.html",
            "relUrl": "/stock%20trading/zero-sum/finance/valuation/2020/09/19/stock-trading-sum.html",
            "date": " • Sep 19, 2020"
        }
        
    
  
    
        ,"post12": {
            "title": "How to build up a writing habit?",
            "content": "If you tend to analyze a lot but want to build up a consistent writing habit, the following tips might help. . Take notes . Have a notepad or a very light-weight text editor/app that you can open at any time and dump your ideas effortlessly. . Find out what interests you . Review your notes, todo lists, emails, past writing, or any other traces about yourself. Look for frequent themes about which you feel strongly and, even better, about which you developed some depth in skill or knowledge. . Prioritize your themes . Take these themes and group or merge the closely related ones. Then, prioritize them based on their importance. . When you have them ranked, drop the last 80% and focus on the remaining 20%. If you have too too many themes remained (e.g., more than 10), drop even more. . Brainstorm post ideas . For each remaining theme, write as many post ideas as possible, but a minimum of ten. Do not bother with how good or viable they are. The important thing is to have a high number of them. . Rotate your themes . Now you have a list of topics organized under themes. Outline a rotating schedule where you order the themes into a sequence, so, in each day, you write about a topic belonging to the next theme. . Write . Each day, you take the topics belonging under that day’s theme and pick one randomly. Write about it impatiently, badly, and experimentally for a minimum 5 minutes. . When you find that you could write about multiple things but you do not know which to pick, pick the first one and move the second into your ideas under the same theme. . When you run out of time, you will find yourself wanting to write more about something. Move it as a new topic under the same theme. . Publish . Publish the writing to somewhere, where nobody sees is. For example, create an account on Medium or any blogging platform under some alias. . Repeat . When you arrive at the end of the sequence, start again. If you could fill out all your 5 minutes for each day for a week, add an extra 5 minutes for your writing time. .",
            "url": "andrasnovoszath.com/writing/habit/2020/09/18/writing-habit.html",
            "relUrl": "/writing/habit/2020/09/18/writing-habit.html",
            "date": " • Sep 18, 2020"
        }
        
    
  
    
        ,"post13": {
            "title": "Is finance a zero-sum activity?",
            "content": "I recently thought about finance and whether it is a purely zero-sum game or has some value-add to it. I would like to know this to see if it is worthy of support either as a personal wealth source or as a social institution. . I did not have time to read academic papers, so I simply did a google search (1-2 hours) and reviewed the top results. . What I found . Just based on this short research, I barely found any substantial arguments against the zero-sum claim. . What counterarguments I found were acknowledged that some forms, like forex or short-term trading, are zero-sum. They argued that, compared to the former types, ‘real’ stock trading can bring about benefits for both parties. Most of these were conflating terminology and concepts (like ‘luck’, or ‘value growth’). . There are probably more substantial arguments in academic literature. Also, probably, there is the additional question of the social usefulness of financial markets for overall capital allocation, but that’s a separate thing. . How I see it now . Of course, as I started to dive into the topic, I quickly realized that is is a bit more complex than I thought. . So let’s break up the question to the following cases: . Pure commercial relationship | Trading of goods | Finance | . Pure commercial transaction . When two people get into a commercial relationship, one is a ‘seller’, and the other is a ‘buyer’. Regarding the price, they have opposite aims. One’s income is another’s cost. . So, in this sense, every commercial transaction is zero-sum. . Trade of ‘comparatively advantageous’ goods . When the seller gives away something they cannot use, but the buyer can, the exchange might benefit both. There is still the issue of the price for which they exchange the good, but the buyer’s benefit from possessing the good might surpass its price. . So, here we can imagine a non-zero-sum relationship. . Finance . Most of the people who trade and invest mostly want to get the most payoff for their buck. So this primarily seems zero-sum. . The mutual benefit might come about when their relationship to their asset is more complex than just trying to get the highest price difference. . Conclusion . Again, I can see that I could think and write about this topic for a long time. Some ideas . How do we measure and compare ‘comparatively advantages’? | What are examples of not purely ‘buy low, sell high’ uses of finance? | How to measure the utility ‘ratio’ between price gain/loss and usability of the result of a buy event? | Is the social benefit of efficient capital distribution via financial markets a fluke, or has empirical evidence behind it? | Can we replace financial markets with an AI and delegate human creativity to more productive things? | .",
            "url": "andrasnovoszath.com/finance/zero-sum/valuation/2020/09/17/finance-zero-sum.html",
            "relUrl": "/finance/zero-sum/valuation/2020/09/17/finance-zero-sum.html",
            "date": " • Sep 17, 2020"
        }
        
    
  
    
        ,"post14": {
            "title": "The specific you",
            "content": "When you are thinking about what business area to pursue, it is useful to focus on your specific situation and circumstance. . Why is it useful to find a specialization? . When you do not look at your own specificities, you basically do not use knowledge and skills that others could use but cannot. . You have your history, your dis/abilities, your interests, and your own conceptual framing. Most of it might be useless, especially regarding all the people in the world. . However, there are a few problems which maybe you are the only one who can solve it. . This is not about ‘finding your passion’ but rather to find out your skills, knowledge, approach, etc. These are actually quite tangible. . How to know your specificities? . To see yourself better, write down your characteristics in as many dimensions as possible. . Here are some dimensions which you can look for: . Knowledge, | Skills, | Experience, | Interest. | . One thing you can do is to create a spreadsheet and write as many of these things as possible. Then, value them on a scale (e.g., 1-5) showing how specific these traits and dimensions are to you, especially compared to others. .",
            "url": "andrasnovoszath.com/self-knowledge/specialization/2020/09/16/specific-you.html",
            "relUrl": "/self-knowledge/specialization/2020/09/16/specific-you.html",
            "date": " • Sep 16, 2020"
        }
        
    
  
    
        ,"post15": {
            "title": "Valuation technology experimentation",
            "content": "It might be useful and promising to experiment with technologies of valuation. . How does valuation happen now, and how does technology play a role in it? . Current technologies of valuation are in place because of different reasons. However, implementers rarely take into account the way the given technology plays a role in valuing things. . Examples of the use of technology in valuation and some trends . Money is arguably the most prominent example. Probably this is not independent of that most of us still do not really understand all its effects. . Other examples are in accountancy, trade, work management, economic metrics, environmental management. . An obvious note is that, of course, other, not strictly technological components, like language, moral values, physical factors, etc., also have critial roles. . Think about and experiment with different technologies to generate new valuation situations . Accordingly, it might be beneficial to try out technologies with explicit valuation functions in mind. . For the most part, and especially in early stages, this is concept-driven (i.e., we are looking for a particular effect to emerge). However, an uncertain but promising benefit would be discovering ‘unthought’ features due to things’ specific and idiosyncratic character. . The process of valuation technology experiments and its main challenges . Most of the time, we cannot implement large scale infrastructures for the sake of experimentation. . Instead, we can try other paths: . Implementation of a small-scale but very focused version | Scenario building | Simulation | Game development | The empirical analysis of similar processes | The empirical analysis of current technologies’ edge cases | Speculation | Incremental implementation | . Current action possibilities and possible future directions . As, currently, I this all is very high level and conceptual, and I am also mostly interested in discovery activities: . Collecting ideas | Conceptual exploration of background ideas | Case study analysis | Code experimentations | . In the future, I might be able to focus it down and branch out toward more serious development work. .",
            "url": "andrasnovoszath.com/technology/valuation/experimentation/2020/09/15/technnology_valuation_experimentation.html",
            "relUrl": "/technology/valuation/experimentation/2020/09/15/technnology_valuation_experimentation.html",
            "date": " • Sep 15, 2020"
        }
        
    
  
    
        ,"post16": {
            "title": "Note-taking obsessions",
            "content": "I have an obsession with the process of note-taking. . I often write down my ideas in the form of notes or tasks. I do this through multiple channels. Then I spend a considerable amount of time organizing them. . One common frustration with organizing notes is that I can do this only along one hierarchical dimension. . Perhaps one reason I like to code is that you can implement hierarchies along multiple dimensions more easily, although not always. . Because of the single hierarchy, some notes tend to become forgotten as I put them in some deep end part of a category branch. This then requires me to constantly re- and review my notes and to recategorize and reorganize them. . I have looked into multiple note-taking software, but there is not really a perfect solution for this. . One thing I found helpful is to require only a single function from a particular tool. So, for instance, I would use one particular tool or process for note-taking, then another for categorization, and perhaps a further one for review or note management. . This, however, needs that the different tools speak to each other relatively well. Ready-made solutions often do not allow it, so I found myself moving toward text-based tools. However, you need to utilize some basic scripting to make them work. .",
            "url": "andrasnovoszath.com/note-taking/2020/09/14/note-taking-obsessions.html",
            "relUrl": "/note-taking/2020/09/14/note-taking-obsessions.html",
            "date": " • Sep 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About",
          "content": "Online mental dump to put things out. Currently, I practice writing posts within 30 minutes. Still getting there… .",
          "url": "andrasnovoszath.com/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  

}