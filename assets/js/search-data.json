{
  
    
        "post0": {
            "title": "Difference quotients from scratch with Python",
            "content": "From the Data Science from Scratch book. . Libraries and helper functions . import pandas as pd import altair as alt . from typing import List Vector = List[float] Vector . typing.List[float] . def dot(vector1: Vector, vector2: Vector) -&gt; float: assert len(vector1) == len(vector2) return sum(v1 * v2 for v1, v2 in zip(vector1, vector2)) assert dot([1, 2, 3], [4, 5, 6]) == 32 . def sum_of_squares(v: Vector) -&gt; Vector: return dot(v, v) assert sum_of_squares([1, 2, 3]) == 14 . Difference quotient . from typing import Callable def difference_quotient( f: Callable[[float], float], x: float, h: float ) -&gt; float : return (f(x + h) - f(x)) / h . A simple estimate . def square(x: float) -&gt; float: return x * x . def derivative_x2(x: float) -&gt; float: return 2 * x . xs = range(-10, 11) actuals = [derivative_x2(x) for x in xs] actuals . [-20, -18, -16, -14, -12, -10, -8, -6, -4, -2, 0, 2, 4, 6, 8, 10, 12, 14, 16, 18, 20] . estimates = [difference_quotient(square, x, h=0.001) for x in xs] estimates . [-19.998999999984335, -17.998999999988996, -15.999000000007868, -13.999000000005424, -11.99900000000298, -9.999000000004088, -7.998999999999867, -5.998999999999199, -3.9989999999994197, -1.998999999999973, 0.001, 2.0009999999996975, 4.000999999999699, 6.000999999999479, 8.0010000000037, 10.001000000002591, 12.001000000005035, 14.00100000000748, 16.000999999988608, 18.000999999983947, 20.000999999993496] . df = pd.DataFrame({&#39;actuals&#39;: actuals, &#39;estimates&#39;: estimates}).reset_index() df = df.melt(id_vars=&#39;index&#39;) df.sample(10) . index variable value . 34 13 | estimates | 6.001 | . 12 12 | actuals | 4.000 | . 29 8 | estimates | -3.999 | . 37 16 | estimates | 12.001 | . 28 7 | estimates | -5.999 | . 41 20 | estimates | 20.001 | . 25 4 | estimates | -11.999 | . 22 1 | estimates | -17.999 | . 11 11 | actuals | 2.000 | . 32 11 | estimates | 2.001 | . alt.Chart(df).mark_circle(opacity=0.75).encode( alt.X(&#39;index:Q&#39;), alt.Y(&#39;value:Q&#39;), alt.Size(&#39;variable:N&#39;), alt.Color(&#39;variable:N&#39;) ).properties(title=&#39;Actual derivatives and estimated quotients&#39;) . Calculating an i-th difference quotient . def partial_difference_quotient( f: Callable[[Vector], float], v: Vector, i: int, h: float ) -&gt; float: &quot;&quot;&quot;Return i-th parital difference quotient of `f` at a`v`&quot;&quot;&quot; w = [ v_j + (h if j == i else 0) for j, v_j in enumerate(v) ] return (f(w) - f(v)) / h . def estimate_gradient( f: Callable[[Vector], float], v: Vector, h: float = 0.0001 ): return [ partial_difference_quotient(f, v, i, h) for i in range(len(v)) ] .",
            "url": "https://andrasnovoszath.com/2020/10/19/difference_quotient_estimate.html",
            "relUrl": "/2020/10/19/difference_quotient_estimate.html",
            "date": " • Oct 19, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Using the Beta distribution",
            "content": "We try to use beta distributions to estimate if our coin is fair. . From the Data Science from Scratch book. . Libraries and helper functions . import math as m import numpy as np import altair as alt import pandas as pd . def B(alpha: float, beta: float) -&gt; float: &quot;This scales the parameters between 0 and 1&quot; return m.gamma(alpha) * m.gamma(beta) / m.gamma(alpha + beta) def beta_pdf(x: float, alpha: float, beta:float) -&gt; float: if x &lt;= 0 or x &gt;=1: return 0 return x ** (alpha - 1) * (1 - x) ** (beta - 1) / B(alpha, beta) . Example 1 . We do not want to make assumptions beforehand, so we choose both $ alpha$ and $ beta$ to be 1: $B(1, 1)$ | We flip the coin 10 times and get 3 heads | Our new posterior distribution becomes $B(4, 8)$ centered around 0.33 | We have to assume that the observed probabilty is the real | Example 2 . We have a strong assumption that the coin is fair so we choose a $B(20, 20) | Again, we got 3 heads out of 10 | Our new Beta is $B(23, 27)$ centered around 0.46 | It suggest that the coin is slightly biased toward tails | Example 3 . We believe that the coin is biased toward head by 75% of the time, so we choose $B(30, 10)$ | Again, we got 3 heads out of 10 | Our poseterior distribution is $B(33, 17)$ centered around 0.66 | It suggest that the coin is biased toward the head, although less strongly as we believed | df = pd.DataFrame() Beta_combinations = [(1, 1), (4, 8), (20, 20), (23, 27), (30, 10), (33, 17)] for Beta in Beta_combinations: alpha, beta = Beta df_B = pd.DataFrame() df_B[&#39;x&#39;] = pd.Series(np.arange(0.01, 1, .01)) df_B[&#39;y&#39;] = df_B[&#39;x&#39;].apply(lambda x: beta_pdf(x, alpha, beta)) df_B[&#39;Beta&#39;] = f&#39;({alpha}, {beta})&#39; df = pd.concat([df, df_B]) . alt.Chart(df).mark_line().encode( alt.X(&#39;x:Q&#39;), alt.Y(&#39;y:Q&#39;), alt.Color(&#39;Beta&#39;), tooltip=[&#39;x&#39;, &#39;y&#39;, &#39;Beta&#39;], strokeDash=&#39;Beta&#39; ).properties(width=600, title=&#39;Beta distributions&#39;) .",
            "url": "https://andrasnovoszath.com/2020/10/18/beta-coin-examples.html",
            "relUrl": "/2020/10/18/beta-coin-examples.html",
            "date": " • Oct 18, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Beta distribution",
            "content": "When trying to to Bayesian interference to estimate probability, we can use Beta distribution as a prior. Below are some steps to calculate it. . From the Data Science from Scratch book. . import math as m . $ f(x, alpha, beta) = x^{ alpha - 1} (1 - x)^{ beta - 1} frac {1} { text{B}( alpha, beta)} $ . where . $ text{B} = Gamma( alpha) Gamma( beta) frac{1} { Gamma( alpha + beta)} $ . where . $ Gamma(n)$ is the Gamma function which, for positive integers is . $ Gamma(n) = (n - 1)!$ . def B(alpha: float, beta: float) -&gt; float: &quot;This scales the parameters between 0 and 1&quot; return m.gamma(alpha) * m.gamma(beta) / m.gamma(alpha + beta) . def beta_pdf(x: float, alpha: float, beta:float) -&gt; float: if x &lt;= 0 or x &gt;=1: return 0 return x ** (alpha - 1) * (1 - x) ** (beta - 1) / B(alpha, beta) . import numpy as np import pandas as pd import altair as alt . df = pd.DataFrame() Beta_combinations = [(1, 1), (10, 10), (4, 16), (16, 4), (30, 30)] for Beta in Beta_combinations: alpha, beta = Beta df_B = pd.DataFrame() df_B[&#39;x&#39;] = pd.Series(np.arange(0.01, 1, .01)) df_B[&#39;y&#39;] = df_B[&#39;x&#39;].apply(lambda x: beta_pdf(x, alpha, beta)) df_B[&#39;Beta&#39;] = f&#39;({alpha}, {beta})&#39; df = pd.concat([df, df_B]) . The distribution centers around $ alpha frac{1} { alpha + beta} $ . Beta(1, 1) is the uniform distribution in [1, 1] | When alpha is greater than beta the distribution is skewed to the left (and respectively, in the opposite case) | The greater alpha and beta are the &#39;tighter&#39; is the distribution | . alt.Chart(df).mark_line().encode(alt.X(&#39;x:Q&#39;), alt.Y(&#39;y:Q&#39;), alt.Color(&#39;Beta&#39;), tooltip=[&#39;x&#39;, &#39;y&#39;, &#39;Beta&#39;], strokeDash=&#39;Beta&#39;).properties(width=600) .",
            "url": "https://andrasnovoszath.com/2020/10/17/beta-distribution.html",
            "relUrl": "/2020/10/17/beta-distribution.html",
            "date": " • Oct 17, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "A/B testing with Python",
            "content": "Introduction to A/B testing with python. . From the Data Science from Scratch book. . Libraries and helper functions . import math as m from typing import Tuple . def normal_probability_below(x: float, mu: float = 0, sigma: float = 1) -&gt; float: return (1 + m.erf((x - mu) / m.sqrt(2) / sigma)) / 2 def normal_probability_above(lo: float, mu: float = 0, sigma: float = 1) -&gt; float: return 1 - normal_probability_below(lo, mu, sigma) . def two_sided_p_value(x: float, mu: float = 0, sigma: float = 1) -&gt; float: &quot;&quot;&quot;Return the probability of getting at least as extreme value as `x`, given that our values are from a normal distribution with `mu` mean and `sigma` std. &quot;&quot;&quot; # If x is greater than the mean return everything above x if x &gt;= mu: return 2 * normal_probability_above(x, mu, sigma) # If x is less than the mean than return everything below x else: return 2 * normal_probability_below(x, mu, sigma) . A/B test . def estimate_parameters(N: int, true: int) -&gt; Tuple[float, float]: p = true / N sigma = m.sqrt(p * (1 - p) / N) return p, sigma . $H_0$: $p_a$ and $p_b$ are the same . With simplification this means that $p_a - p_b = 0$ . def a_b_test_statistic(N_A: int, A: int, N_B: int, B:int) -&gt; float: p_A, sigma_A = estimate_parameters(N_A, A) p_B, simga_B = estimate_parameters(N_B, B) return (p_B - p_A) / m.sqrt(sigma_A ** 2 + simga_B ** 2) . Example 1 . z = a_b_test_statistic(1000, 200, 1000, 180) z, two_sided_p_value(z) . (-1.1403464899034472, 0.2541419765422359) . That is, the probability of at least such a big difference occurring assuming that the two probabilities are the same is ~0.25. . Example 2 . Let&#39;s decrease the occurance of $b$ even more. . z = a_b_test_statistic(1000, 200, 1000, 150) z, two_sided_p_value(z) . (-2.948839123097944, 0.003189699706216853) . That is the probability of at least this large difference to occur if the probabilities are the same is 0.03. .",
            "url": "https://andrasnovoszath.com/2020/10/16/ab-testing.html",
            "relUrl": "/2020/10/16/ab-testing.html",
            "date": " • Oct 16, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "Programmatic p-hacking from scratch with Python",
            "content": "Below is a short script to demonstrate the &#39;process of p-hacking&#39;. . From the Data Science from Scratch book. . import random from typing import List . First we define a usual experiment consisting of 1000 binomial trials with 0.5 probability. . def run_experiment(trials) -&gt; List[bool]: return [random.random() &lt; 0.5 for _ in range(trials)] experiment = run_experiment(1000) print(&quot;Proportion of heads:&quot;, sum(experiment) / len(experiment)) print(&quot;First 10 elements:&quot;, experiment[:10]) . Proportion of heads: 0.51 First 10 elements: [True, True, True, False, False, False, False, True, True, True] . Then we examine whether the outcome an experiment is beyond the 95% confidence levels around p = 0.5, that is, the hypothesis of having a fair coin. . def reject_fairness(experiment: List[bool]) -&gt; bool: num_heads = sum(experiment) return num_heads &lt; 469 or num_heads &gt; 531 reject_fairness(experiment) . False . We run 1000 independent experiments with the exact same parameters. . random.seed(42) experiments = [run_experiment(1000) for _ in range(1000)] . Now we can simply pick those experiments which fall outside the confidence level. . number_of_unfair = sum([reject_fairness(experiment) for experiment in experiments]) print(&quot;Number of experiments &#39;showing&#39; that the coin if unfair:&quot;, number_of_unfair) print(&quot; nProbabilities:&quot;) print(&quot; t&quot;.join([str(sum(experiment) / len(experiment)) for experiment in experiments if reject_fairness(experiment)])) . Number of experiments &#39;showing&#39; that the coin if unfair: 42 Probabilities: 0.532 0.539 0.535 0.461 0.466 0.539 0.467 0.468 0.54 0.458 0.468 0.463 0.467 0.46 0.461 0.463 0.541 0.464 0.538 0.542 0.461 0.465 0.468 0.538 0.466 0.46 0.468 0.534 0.535 0.468 0.537 0.468 0.535 0.538 0.451 0.537 0.463 0.466 0.46 0.536 0.466 0.467 .",
            "url": "https://andrasnovoszath.com/2020/10/15/p-hacking.html",
            "relUrl": "/2020/10/15/p-hacking.html",
            "date": " • Oct 15, 2020"
        }
        
    
  
    
        ,"post5": {
            "title": "Confidence intervals from scratch with Python",
            "content": "From the Data Science from Scratch book. . Libraries and helper functions . import math as m . def calc_normal_cdf(x: float, mu: float = 0, sigma: float = 1) -&gt; float: return (1 + m.erf((x - mu) / m.sqrt(2) / sigma)) / 2 . def calc_inverse_normal_cdf(p: float, mu:float = 0, sigma: float = 1, tolerance: float = 1E-5, show_steps=False) -&gt; float: if p == 0: return -np.inf if p == 1: return np.inf # In case it is not a standard normal distribution, calculate the standard normal first and then rescale if mu != 0 or sigma != 1: return mu + sigma * calc_inverse_normal_cdf(p, tolerance=tolerance) low_z = -10 hi_z = 10 if show_steps: print(f&quot;{&#39;&#39;:&lt;19}&quot;.join([&#39;low_z&#39;, &#39;mid_z&#39;, &#39;hi_z&#39;]), &quot; n&quot;) while hi_z - low_z &gt; tolerance: mid_z = (low_z + hi_z) / 2 mid_p = calc_normal_cdf(mid_z) if mid_p &lt; p: low_z = mid_z else: hi_z = mid_z if show_steps: print(&quot; t&quot;.join(map(to_string, [low_z, mid_z, hi_z]))) return mid_z . def normal_upper_bound(probabilty: float, mu: float = 0, sigma: float = 1) -&gt; float: &quot;&quot;&quot;Return z for which P(Z &lt;= z) = probability&quot;&quot;&quot; return calc_inverse_normal_cdf(probabilty, mu, sigma) def normal_lower_bound(probabilty: float, mu: float = 0, sigma: float = 1) -&gt; float: &quot;&quot;&quot;Return z for which P(Z &gt;= z) = probability&quot;&quot;&quot; return calc_inverse_normal_cdf(1 - probabilty, mu, sigma) . def normal_two_sided_bounds(probability: float, mu: float = 0, sigma: float = 1) -&gt; float: if probability == 0: return 0, 0 tail_probability = (1 - probability) / 2 lower_bound = normal_upper_bound(tail_probability, mu, sigma) upper_bound = normal_lower_bound(tail_probability, mu, sigma) return lower_bound, upper_bound . Example 1 . We would like to know if a coin is &#39;fair&#39; or not (i.e. p=0.5) . We flip the coin 1000 times and observe 525 heads. . As we do not know the &#39;real&#39; probability, we need to rely on our observed results. . p_hat = 525 / 1000 mu = p_hat sigma = m.sqrt(p_hat * (1 - p_hat) / 1000) p_hat, mu, sigma . (0.525, 0.525, 0.015791611697353755) . While we cannot tell if this is really the probabiliy of heads, we can calculate the interval within which we expect the real probability fall with our chosen confidence (here: 95%) . confidence = 0.95 normal_two_sided_bounds(confidence, mu, sigma) . (0.4940490278129096, 0.5559509721870904) . As 0.5 is within the interval, we cannot reject the hypothesis that the coin is fair. . Example 2 . In another example we observe 540 heads in 1000 flips. . p_hat = 540 / 1000 mu = p_hat sigma = m.sqrt(p_hat * (1 - p_hat) / 1000) p_hat, mu, sigma . (0.54, 0.54, 0.015760710643876435) . Going through the same steps as above, we get that 0.5 falls outside our 95% confidence interval and therefore we reject the hypothesis that the coin is &#39;fair&#39;. . normal_two_sided_bounds(confidence, mu, sigma) . (0.5091095927295919, 0.5708904072704082) .",
            "url": "https://andrasnovoszath.com/2020/10/14/confidence-intervals.html",
            "relUrl": "/2020/10/14/confidence-intervals.html",
            "date": " • Oct 14, 2020"
        }
        
    
  
    
        ,"post6": {
            "title": "p-values with Python from scratch",
            "content": "Calculating p-values with python. . Libraries and helper functions . from typing import Tuple import math as m . def normal_probability_below(x: float, mu: float = 0, sigma: float = 1) -&gt; float: return (1 + m.erf((x - mu) / m.sqrt(2) / sigma)) / 2 . def normal_probability_above(lo: float, mu: float = 0, sigma: float = 1) -&gt; float: return 1 - normal_probability_below(lo, mu, sigma) . def normal_approximation_to_binomial(n: int, p: float) -&gt; Tuple[float, float]: mu = p * n sigma = m.sqrt(p * (1 - p) * n) return mu, sigma . Two-sided p-values . def two_sided_p_value(x: float, mu: float = 0, sigma: float = 1) -&gt; float: &quot;&quot;&quot;Return the probability of getting at least as extreme value as `x`, given that our values are from a normal distribution with `mu` mean and `sigma` std. &quot;&quot;&quot; # If x is greater than the mean return everything above x if x &gt;= mu: return 2 * normal_probability_above(x, mu, sigma) # If x is less than the mean than return everything below x else: return 2 * normal_probability_below(x, mu, sigma) . E.g. if our normal distribution has a mean of 500 and standard deviation of 15, the probabilty to get 530 or above is ~5.78% . mu_0, sigma_0 = normal_approximation_to_binomial (1000 , 0.5 ) mu_0, sigma_0 . (500.0, 15.811388300841896) . two_sided_p_value(529.5, mu_0, sigma_0) . 0.06207721579598835 . As the p-value is greater than 5%, so we don&#39;t reject the null hypothesis. . However, if we look at values beyond 32 distance from the mean, however, the probability will be less than 5%. . two_sided_p_value(531.5, mu_0, sigma_0) . 0.046345287837786575 . Validating with simulation . import random # Run 1000 simulations with 1000 binomial trials extreme_value_count = 0 for _ in range(1000): num_heads = sum(1 if random.random() &lt; 0.5 else 0 for _ in range(1000)) # Count the &#39;extreme&#39; values (i.e. beyond 30 distance from the mean) if num_heads &gt;= 530 or num_heads &lt;= 470: extreme_value_count += 1 extreme_value_count / 1000 # assert 610 &lt; extreme_value_count &lt; 630, f&quot;{extreme_value_count}&quot; . 0.07 . One-sided p-values . normal_probability_above(524.5, mu_0, sigma_0) . 0.06062885772582072 . normal_probability_above(526.5, mu_0, sigma_0) . 0.04686839508859242 .",
            "url": "https://andrasnovoszath.com/2020/10/13/p-values.html",
            "relUrl": "/2020/10/13/p-values.html",
            "date": " • Oct 13, 2020"
        }
        
    
  
    
        ,"post7": {
            "title": "Hypothesis test power from scratch with Python",
            "content": "Calculating power of hypothesis tests. . The code is from the Data Science from Scratch book. . Libraries and helper functions . from typing import Tuple import math as m . def calc_normal_cdf(x: float, mu: float = 0, sigma: float = 1) -&gt; float: return (1 + m.erf((x - mu) / m.sqrt(2) / sigma)) / 2 normal_probability_below = calc_normal_cdf . def normal_probability_between(lo: float, hi: float, mu: float = 0, sigma: float = 1) -&gt; float: return normal_probability_below(hi, mu, sigma) - normal_probability_below(lo, mu, sigma) . def calc_normal_cdf(x: float, mu: float = 0, sigma: float = 1) -&gt; float: return (1 + m.erf((x - mu) / m.sqrt(2) / sigma)) / 2 . def calc_inverse_normal_cdf(p: float, mu:float = 0, sigma: float = 1, tolerance: float = 1E-5, show_steps=False) -&gt; float: if p == 0: return -np.inf if p == 1: return np.inf # In case it is not a standard normal distribution, calculate the standard normal first and then rescale if mu != 0 or sigma != 1: return mu + sigma * calc_inverse_normal_cdf(p, tolerance=tolerance) low_z = -10 hi_z = 10 if show_steps: print(f&quot;{&#39;&#39;:&lt;19}&quot;.join([&#39;low_z&#39;, &#39;mid_z&#39;, &#39;hi_z&#39;]), &quot; n&quot;) while hi_z - low_z &gt; tolerance: mid_z = (low_z + hi_z) / 2 mid_p = calc_normal_cdf(mid_z) if mid_p &lt; p: low_z = mid_z else: hi_z = mid_z if show_steps: print(&quot; t&quot;.join(map(to_string, [low_z, mid_z, hi_z]))) return mid_z . def normal_upper_bound(probabilty: float, mu: float = 0, sigma: float = 1) -&gt; float: return calc_inverse_normal_cdf(probabilty, mu, sigma) . def normal_lower_bound(probabilty: float, mu: float = 0, sigma: float = 1) -&gt; float: return calc_inverse_normal_cdf(1 - probabilty, mu, sigma) . def normal_two_sided_bounds(probability: float, mu: float = 0, sigma: float = 1) -&gt; float: if probability == 0: return 0, 0 tail_probability = (1 - probability) / 2 lower_bound = normal_upper_bound(tail_probability, mu, sigma) upper_bound = normal_lower_bound(tail_probability, mu, sigma) return lower_bound, upper_bound . def normal_approximation_to_binomial(n: int, p: float) -&gt; Tuple[float, float]: mu = p * n sigma = m.sqrt(p * (1 - p) * n) return mu, sigma . Type 1 Error and Tolerance . Let&#39;s make our null hypothesis ($H_0$) that the probability of head is 0.5 . mu_0, sigma_0 = normal_approximation_to_binomial(1000, 0.5) mu_0, sigma_0 . (500.0, 15.811388300841896) . We define our tolerance at 5%. That is, we accept our model to produce &#39;type 1&#39; errors (false positive) in 5% of the time. With the coin flipping example, we expect to receive 5% of the results to fall outsied of our defined interval. . lo, hi = normal_two_sided_bounds(0.95, mu_0, sigma_0) lo, hi . (469.01026640487555, 530.9897335951244) . Type 2 Error and Power . At type 2 error we consider false negatives, that is, those cases where we fail to reject our null hypothesis even though we should. . Let&#39;s assume that contra $H_0$ the actual probability is 0.55. . mu_1, sigma_1 = normal_approximation_to_binomial(1000, 0.55) mu_1, sigma_1 . (550.0, 15.732132722552274) . In this case we get our Type 2 probability as the overlapping of the real distribution and the 95% probability region of $H_0$. In this particular case, in 11% of the cases we will wrongly fail to reject our null hypothesis. . type_2_probability = normal_probability_between(lo, hi, mu_1, sigma_1) type_2_probability . 0.1134519987046329 . The power of the test is then the probability of rightly rejecting the $H_0$ . power = 1 - type_2_probability power . 0.886548001295367 . Now, let&#39;s redefine our null hypothesis so that we expect the probability of head to be less than or equal to 0.5. . In this case we have a one-sided test. . hi = normal_upper_bound(0.95, mu_0, sigma_0) hi . 526.0073585242053 . Because this is a less strict hypothesis than our previus one, it has a smaller T2 probability and a greater power. . type_2_probability = normal_probability_below(hi, mu_1, sigma_1) type_2_probability . 0.06362051966928273 . power = 1 - type_2_probability power . 0.9363794803307173 .",
            "url": "https://andrasnovoszath.com/2020/10/12/hypothesis-test-power.html",
            "relUrl": "/2020/10/12/hypothesis-test-power.html",
            "date": " • Oct 12, 2020"
        }
        
    
  
    
        ,"post8": {
            "title": "Normal approximation of binomial distribution",
            "content": "from typing import Tuple import math as m . def normal_approximation_to_binomial(n: int, p: float) -&gt; Tuple[float, float]: mu = p * n sigma = m.sqrt(p * (1 - p) * n) return mu, sigma . The change of standard deviation with the incrase of number of trials. . for i in range(10): print(i, normal_approximation_to_binomial(10**i, 0.5)) . 0 (0.5, 0.5) 1 (5.0, 1.5811388300841898) 2 (50.0, 5.0) 3 (500.0, 15.811388300841896) 4 (5000.0, 50.0) 5 (50000.0, 158.11388300841898) 6 (500000.0, 500.0) 7 (5000000.0, 1581.1388300841897) 8 (50000000.0, 5000.0) 9 (500000000.0, 15811.388300841896) .",
            "url": "https://andrasnovoszath.com/2020/10/11/normal-approximation-binomial.html",
            "relUrl": "/2020/10/11/normal-approximation-binomial.html",
            "date": " • Oct 11, 2020"
        }
        
    
  
    
        ,"post9": {
            "title": "Probability interval boundaries with Python",
            "content": "In the previous post we calculated proabilities and z values with Python. Here we continue with finding the interval boundaries belonging to a particular tail or two-sided probabiltiy. . Libraries and helper functions . import math as m import numpy as np . def to_string(number: float, column_width: int = 20) -&gt; str: # return str(number).ljust(column_width) return f&quot;{str(number): &lt;{column_width}}&quot; . def calc_normal_cdf(x: float, mu: float = 0, sigma: float = 1) -&gt; float: return (1 + m.erf((x - mu) / m.sqrt(2) / sigma)) / 2 . def calc_inverse_normal_cdf(p: float, mu:float = 0, sigma: float = 1, tolerance: float = 1E-5, show_steps=False) -&gt; float: if p == 0: return -np.inf if p == 1: return np.inf # In case it is not a standard normal distribution, calculate the standard normal first and then rescale if mu != 0 or sigma != 1: return mu + sigma * calc_inverse_normal_cdf(p, tolerance=tolerance) low_z = -10 hi_z = 10 if show_steps: print(f&quot;{&#39;&#39;:&lt;19}&quot;.join([&#39;low_z&#39;, &#39;mid_z&#39;, &#39;hi_z&#39;]), &quot; n&quot;) while hi_z - low_z &gt; tolerance: mid_z = (low_z + hi_z) / 2 mid_p = calc_normal_cdf(mid_z) if mid_p &lt; p: low_z = mid_z else: hi_z = mid_z if show_steps: print(&quot; t&quot;.join(map(to_string, [low_z, mid_z, hi_z]))) return mid_z . Upper bound . def normal_upper_bound(probabilty: float, mu: float = 0, sigma: float = 1) -&gt; float: &quot;&quot;&quot;Return z for which P(Z &lt;= z) = probability&quot;&quot;&quot; return calc_inverse_normal_cdf(probabilty, mu, sigma) . def normal_lower_bound(probabilty: float, mu: float = 0, sigma: float = 1) -&gt; float: &quot;&quot;&quot;Return z for which P(Z &gt;= z) = probability&quot;&quot;&quot; return calc_inverse_normal_cdf(1 - probabilty, mu, sigma) . for i in np.arange(0, 1.1, .1): print(f&quot;{i:.1f}: {normal_upper_bound(i): &gt;10.4f}, {normal_lower_bound(i): &gt;10.4f}&quot;) . 0.0: -inf, inf 0.1: -1.2816, 1.2816 0.2: -0.8416, 0.8416 0.3: -0.5244, 0.5244 0.4: -0.2533, 0.2533 0.5: -0.0000, -0.0000 0.6: 0.2533, -0.2533 0.7: 0.5244, -0.5244 0.8: 0.8416, -0.8416 0.9: 1.2816, -1.2816 1.0: inf, -inf . Two sided bounds . Two variations . def normal_two_sided_bounds(probability: float, mu: float = 0, sigma: float = 1) -&gt; float: if probability == 0: return 0, 0 tail_probability = (1 - probability) / 2 lower_bound = normal_upper_bound(tail_probability, mu, sigma) upper_bound = normal_lower_bound(tail_probability, mu, sigma) return lower_bound, upper_bound . def normal_two_sided_bounds2(probability: float, mu: float = 0, sigma: float = 1) -&gt; float: if probability == 0: return 0, 0 if mu != 0: raise ValueError(&quot;Requires standard normal distribution&quot;) tail_probability = (1 - probability) / 2 z = normal_upper_bound(tail_probability, mu, sigma) return -z, z . By default they do not produce the same results at the extremes because of how the inverse cdf function defines the theoretical uppern and lower boundaries (as in these cases it should produce +/- infinity). . Accordingly, it is useful to define extreme values (i.e. probablities of 0 and 1) . for i in np.arange(0, 1.1, .1): print(f&quot;{i:.1f}: {normal_two_sided_bounds(i) == normal_two_sided_bounds2(i)}, {normal_two_sided_bounds(i)}, {normal_two_sided_bounds2(i)}&quot;) . 0.0: True, (0, 0), (0, 0) 0.1: False, (-0.12566566467285156, 0.12566566467285156), (0.12566566467285156, -0.12566566467285156) 0.2: False, (-0.2533435821533203, 0.2533435821533203), (0.2533435821533203, -0.2533435821533203) 0.3: False, (-0.3853130340576172, 0.3853130340576172), (0.3853130340576172, -0.3853130340576172) 0.4: False, (-0.5243968963623047, 0.5243968963623047), (0.5243968963623047, -0.5243968963623047) 0.5: False, (-0.6744861602783203, 0.6744861602783203), (0.6744861602783203, -0.6744861602783203) 0.6: False, (-0.8416271209716797, 0.8416271209716797), (0.8416271209716797, -0.8416271209716797) 0.7: False, (-1.0364246368408203, 1.0364246368408203), (1.0364246368408203, -1.0364246368408203) 0.8: False, (-1.2815570831298828, 1.2815570831298828), (1.2815570831298828, -1.2815570831298828) 0.9: False, (-1.6448497772216797, 1.6448497772216797), (1.6448497772216797, -1.6448497772216797) 1.0: False, (-inf, inf), (inf, -inf) .",
            "url": "https://andrasnovoszath.com/2020/10/10/probability-intervals-boundaries.html",
            "relUrl": "/2020/10/10/probability-intervals-boundaries.html",
            "date": " • Oct 10, 2020"
        }
        
    
  
    
        ,"post10": {
            "title": "Inverse normal cumulative distribution - finding $z$ values from probabilities with Python",
            "content": "Sometimes we would like to find the $z$ values belonging to a particular probability. For this we use an inverse cumulative distribution function. . Because the cumulative distribution function is strictly increasing and continuous, we can find the $z$ by narrowing down with binary bisecting the distance between high and low $z$ values. . The code is based upon the respective example from Data Science from Scratch. . Libraries, helper functions . import math as m import numpy as np . def calc_normal_cdf(x: float, mu: float = 0, sigma: float = 1) -&gt; float: return (1 + m.erf((x - mu) / m.sqrt(2) / sigma)) / 2 . def to_string(number: float, column_width: int = 20) -&gt; str: # return str(number).ljust(column_width) return f&quot;{str(number): &lt;{column_width}}&quot; . Inverse normal CDF . The inverse normal cumulative distribution function. . Takes a probability and optionaly a mean and standard deviation of the distribution, and a tolerance value. . If the distribution is not standard normal, it calculates $z$ for the standard normal one and then rescales the result . | Take a very low and a very high $z$ value . | While the distance between the high and low $z$ values are greater than our tolerance . a. Take their midpoint b. Calculate the probability belonging to that midpoint c. Based on the position of the probability belonging of the midpoint in respect to the probabilty for which we try to find out the $z$ value, assign the midpoint to the low or high $z$ value . | def calc_inverse_normal_cdf(p: float, mu:float = 0, sigma: float = 1, tolerance: float = 1E-5, show_steps=False) -&gt; float: # In case it is not a standard normal distribution, calculate the standard normal first and then rescale if mu != 0 or sigma != 1: return mu + sigma * calc_inverse_normal_cdf(p, tolerance=tolerance) low_z = -10 hi_z = 10 while hi_z - low_z &gt; tolerance: mid_z = (low_z + hi_z) / 2 mid_p = calc_normal_cdf(mid_z) if mid_p &lt; p: low_z = mid_z else: hi_z = mid_z if show_steps: print(&quot; t&quot;.join(map(to_string, [low_z, mid_z, hi_z]))) return mid_z . We can examine the steps as the $z$ closes down to its final value. . calc_inverse_normal_cdf(p=.3, show_steps=True) . -10 0.0 0.0 -5.0 -5.0 0.0 -2.5 -2.5 0.0 -1.25 -1.25 0.0 -0.625 -0.625 0.0 -0.625 -0.3125 -0.3125 -0.625 -0.46875 -0.46875 -0.546875 -0.546875 -0.46875 -0.546875 -0.5078125 -0.5078125 -0.52734375 -0.52734375 -0.5078125 -0.52734375 -0.517578125 -0.517578125 -0.52734375 -0.5224609375 -0.5224609375 -0.52490234375 -0.52490234375 -0.5224609375 -0.52490234375 -0.523681640625 -0.523681640625 -0.52490234375 -0.5242919921875 -0.5242919921875 -0.52459716796875 -0.52459716796875 -0.5242919921875 -0.524444580078125 -0.524444580078125 -0.5242919921875 -0.524444580078125 -0.5243682861328125 -0.5243682861328125 -0.5244064331054688 -0.5244064331054688 -0.5243682861328125 -0.5244064331054688 -0.5243873596191406 -0.5243873596191406 -0.5244064331054688 -0.5243968963623047 -0.5243968963623047 . -0.5243968963623047 . Finally, we get a number of reference values for some different probabilities . for i in np.arange(0, 1.1, .1): print(f&quot;{i:.1f}: {calc_inverse_normal_cdf(i) : &gt;20}&quot;) . 0.0: -9.999990463256836 0.1: -1.2815570831298828 0.2: -0.8416271209716797 0.3: -0.5243968963623047 0.4: -0.2533435821533203 0.5: -9.5367431640625e-06 0.6: 0.2533435821533203 0.7: 0.5243968963623047 0.8: 0.8416271209716797 0.9: 1.2815570831298828 1.0: 8.244009017944336 .",
            "url": "https://andrasnovoszath.com/2020/10/09/inverse-normal-cdf.html",
            "relUrl": "/2020/10/09/inverse-normal-cdf.html",
            "date": " • Oct 9, 2020"
        }
        
    
  
    
        ,"post11": {
            "title": "Loading MEG data with the `MNE` python package",
            "content": "I am interested in sleep data analysis, where one core informational source is neurophysiological data and in particular EEG data coming from polysomnigraphy. . MNE is a Python package designed particularly for this purpose. Below I outline how to install the package and how to load data with it. . Install . Installation is relatively simple . pip install mne . We can also test the status of the package with the info method. . import mne import os . mne.sys_info() . Platform: Linux-5.4.0-48-generic-x86_64-with-glibc2.10 Python: 3.8.5 (default, Sep 4 2020, 07:30:14) [GCC 7.3.0] Executable: /home/andras/anaconda3/envs/sleep_data/bin/python CPU: x86_64: 8 cores Memory: Unavailable (requires &#34;psutil&#34; package) mne: 0.21.0 numpy: 1.19.2 {blas=openblas, lapack=openblas} scipy: 1.5.2 matplotlib: 3.3.1 {backend=module://ipykernel.pylab.backend_inline} sklearn: Not found numba: Not found nibabel: Not found cupy: Not found pandas: 1.1.3 dipy: Not found mayavi: Not found pyvista: Not found vtk: Not found . Data Load . We can set the path to where we download the dataset. . mne.set_config(&#39;MNE_DATA&#39;, &#39;~/MNE_DATA&#39;) . Here we use the sample dataset which has its own module. . First we generate the path where we will download the sample dataset. . sample_data_folder = mne.datasets.sample.data_path() sample_data_folder . Archive exists (MNE-sample-data-processed.tar.gz), checking hash 12b75d1cb7df9dfb4ad73ed82f61094f. Decompressing the archive: /home/andras/mne_data/MNE-sample-data-processed.tar.gz (please be patient, this can take some time) Successfully extracted to: [&#39;/home/andras/mne_data/MNE-sample-data&#39;] . &#39;/home/andras/mne_data/MNE-sample-data&#39; . We specify the specific dataset we want to use. . sample_data_raw_file = os.path.join(sample_data_folder, &#39;MEG&#39;, &#39;sample&#39;,&#39;sample_audvis_filt-0-40_raw.fif&#39;) sample_data_raw_file . &#39;/home/andras/mne_data/MNE-sample-data/MEG/sample/sample_audvis_filt-0-40_raw.fif&#39; . Then, we download and open the data (here I have it already) . raw = mne.io.read_raw_fif(sample_data_raw_file) . Opening raw data file /home/andras/mne_data/MNE-sample-data/MEG/sample/sample_audvis_filt-0-40_raw.fif... Read a total of 4 projection items: PCA-v1 (1 x 102) idle PCA-v2 (1 x 102) idle PCA-v3 (1 x 102) idle Average EEG reference (1 x 60) idle Range : 6450 ... 48149 = 42.956 ... 320.665 secs Ready. . Data overview . Basic information . print(raw) . &lt;Raw | sample_audvis_filt-0-40_raw.fif, 376 x 41700 (277.7 s), ~3.6 MB, data not loaded&gt; . print(raw.info) . &lt;Info | 15 non-empty values bads: 2 items (MEG 2443, EEG 053) ch_names: MEG 0113, MEG 0112, MEG 0111, MEG 0122, MEG 0123, MEG 0121, MEG ... chs: 204 GRAD, 102 MAG, 9 STIM, 60 EEG, 1 EOG custom_ref_applied: False dev_head_t: MEG device -&gt; head transform dig: 146 items (3 Cardinal, 4 HPI, 61 EEG, 78 Extra) file_id: 4 items (dict) highpass: 0.1 Hz hpi_meas: 1 item (list) hpi_results: 1 item (list) lowpass: 40.0 Hz meas_date: 2002-12-03 19:01:10 UTC meas_id: 4 items (dict) nchan: 376 projs: PCA-v1: off, PCA-v2: off, PCA-v3: off, Average EEG reference: off sfreq: 150.2 Hz &gt; . And finally plotting the data . raw.plot_psd(fmax=50) raw.plot(duration=5, n_channels=30) . Effective window size : 13.639 (s) Effective window size : 13.639 (s) Effective window size : 13.639 (s) . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; 2020-10-08T19:00:12.708539 image/svg+xml Matplotlib v3.3.1, https://matplotlib.org/ &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; 2020-10-08T19:00:16.587806 image/svg+xml Matplotlib v3.3.1, https://matplotlib.org/ &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; 2020-10-08T19:00:19.027280 image/svg+xml Matplotlib v3.3.1, https://matplotlib.org/",
            "url": "https://andrasnovoszath.com/2020/10/08/mne-meg-eeg-data.html",
            "relUrl": "/2020/10/08/mne-meg-eeg-data.html",
            "date": " • Oct 8, 2020"
        }
        
    
  
    
        ,"post12": {
            "title": "Probability test",
            "content": "Libraries . from typing import Tuple import math as m import pandas as pd import numpy as np . Probability below a threshold . $ p = frac {1 + text{erf} z ( frac {x - mu} { sqrt{2} sigma} )} {2} $ . where . $ text{erf} z = frac {2} { sqrt{ pi}} int_0^z e^{-t^2} dt $ . is the error function. . def calc_normal_cdf(x: float, mu: float = 0, sigma: float = 1) -&gt; float: return (1 + m.erf((x - mu) / m.sqrt(2) / sigma)) / 2 . normal_probability_below = calc_normal_cdf . for i in [n / 10 for n in range(-10, 10 + 1, 1)]: print(&quot; t&quot;.join([str(i), f&quot;{normal_probability_below(i):.4f}&quot;])) . -1.0 0.1587 -0.9 0.1841 -0.8 0.2119 -0.7 0.2420 -0.6 0.2743 -0.5 0.3085 -0.4 0.3446 -0.3 0.3821 -0.2 0.4207 -0.1 0.4602 0.0 0.5000 0.1 0.5398 0.2 0.5793 0.3 0.6179 0.4 0.6554 0.5 0.6915 0.6 0.7257 0.7 0.7580 0.8 0.7881 0.9 0.8159 1.0 0.8413 . Probability above a threshold . def normal_probability_above(lo: float, mu: float = 0, sigma: float = 1) -&gt; float: return 1 - normal_probability_below(lo, mu, sigma) . for i in [n / 10 for n in range(-10, 10 + 1, 1)]: print(&quot; t&quot;.join([str(i), f&quot;{normal_probability_above(i):.4f}&quot;])) . -1.0 0.8413 -0.9 0.8159 -0.8 0.7881 -0.7 0.7580 -0.6 0.7257 -0.5 0.6915 -0.4 0.6554 -0.3 0.6179 -0.2 0.5793 -0.1 0.5398 0.0 0.5000 0.1 0.4602 0.2 0.4207 0.3 0.3821 0.4 0.3446 0.5 0.3085 0.6 0.2743 0.7 0.2420 0.8 0.2119 0.9 0.1841 1.0 0.1587 . Probability between thresholds . domain = np.arange(-10, 10 + 1, 2) / 10 domain . array([-1. , -0.8, -0.6, -0.4, -0.2, 0. , 0.2, 0.4, 0.6, 0.8, 1. ]) . def normal_probability_between(lo: float, hi: float, mu: float = 0, sigma: float = 1) -&gt; float: return normal_probability_below(hi, mu, sigma) - normal_probability_below(lo, mu, sigma) . probabilities_between = pd.DataFrame() for i in domain: for j in domain: probabilities_between.loc[i, j] = normal_probability_between(i, j) probabilities_between = pd.DataFrame(np.triu(probabilities_between), index=domain, columns=domain).replace(0, np.NaN) for i in domain: probabilities_between.loc[i, i] = 0 probabilities_between . -1.0 -0.8 -0.6 -0.4 -0.2 0.0 0.2 0.4 0.6 0.8 1.0 . -1.0 0.0 | 0.0532 | 0.115598 | 0.185923 | 0.262085 | 0.341345 | 0.420604 | 0.496766 | 0.567092 | 0.629489 | 0.682689 | . -0.8 NaN | 0.0000 | 0.062398 | 0.132723 | 0.208885 | 0.288145 | 0.367404 | 0.443566 | 0.513891 | 0.576289 | 0.629489 | . -0.6 NaN | NaN | 0.000000 | 0.070325 | 0.146487 | 0.225747 | 0.305007 | 0.381169 | 0.451494 | 0.513891 | 0.567092 | . -0.4 NaN | NaN | NaN | 0.000000 | 0.076162 | 0.155422 | 0.234681 | 0.310843 | 0.381169 | 0.443566 | 0.496766 | . -0.2 NaN | NaN | NaN | NaN | 0.000000 | 0.079260 | 0.158519 | 0.234681 | 0.305007 | 0.367404 | 0.420604 | . 0.0 NaN | NaN | NaN | NaN | NaN | 0.000000 | 0.079260 | 0.155422 | 0.225747 | 0.288145 | 0.341345 | . 0.2 NaN | NaN | NaN | NaN | NaN | NaN | 0.000000 | 0.076162 | 0.146487 | 0.208885 | 0.262085 | . 0.4 NaN | NaN | NaN | NaN | NaN | NaN | NaN | 0.000000 | 0.070325 | 0.132723 | 0.185923 | . 0.6 NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | 0.000000 | 0.062398 | 0.115598 | . 0.8 NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | 0.000000 | 0.053200 | . 1.0 NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | 0.000000 | . Probabilities outside a threshold range . def normal_probability_outside(lo: float, hi: float, mu: float = 0, sigma: float = 1) -&gt; float: return 1 - normal_probability_between(lo, hi, mu, sigma) . probabilities_outside = pd.DataFrame() for i in domain: for j in domain: probabilities_outside.loc[i, j] = normal_probability_outside(i, j) pd.DataFrame(np.triu(probabilities_outside), index=domain, columns=domain).replace(0, np.NaN) . -1.0 -0.8 -0.6 -0.4 -0.2 0.0 0.2 0.4 0.6 0.8 1.0 . -1.0 1.0 | 0.9468 | 0.884402 | 0.814077 | 0.737915 | 0.658655 | 0.579396 | 0.503234 | 0.432908 | 0.370511 | 0.317311 | . -0.8 NaN | 1.0000 | 0.937602 | 0.867277 | 0.791115 | 0.711855 | 0.632596 | 0.556434 | 0.486109 | 0.423711 | 0.370511 | . -0.6 NaN | NaN | 1.000000 | 0.929675 | 0.853513 | 0.774253 | 0.694993 | 0.618831 | 0.548506 | 0.486109 | 0.432908 | . -0.4 NaN | NaN | NaN | 1.000000 | 0.923838 | 0.844578 | 0.765319 | 0.689157 | 0.618831 | 0.556434 | 0.503234 | . -0.2 NaN | NaN | NaN | NaN | 1.000000 | 0.920740 | 0.841481 | 0.765319 | 0.694993 | 0.632596 | 0.579396 | . 0.0 NaN | NaN | NaN | NaN | NaN | 1.000000 | 0.920740 | 0.844578 | 0.774253 | 0.711855 | 0.658655 | . 0.2 NaN | NaN | NaN | NaN | NaN | NaN | 1.000000 | 0.923838 | 0.853513 | 0.791115 | 0.737915 | . 0.4 NaN | NaN | NaN | NaN | NaN | NaN | NaN | 1.000000 | 0.929675 | 0.867277 | 0.814077 | . 0.6 NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | 1.000000 | 0.937602 | 0.884402 | . 0.8 NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | 1.000000 | 0.946800 | . 1.0 NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | 1.000000 | .",
            "url": "https://andrasnovoszath.com/2020/10/07/probability-test.html",
            "relUrl": "/2020/10/07/probability-test.html",
            "date": " • Oct 7, 2020"
        }
        
    
  
    
        ,"post13": {
            "title": "Docker cheatsheet",
            "content": "Main ideas and docker file commands (based on this blog post). . Glossary . Image: blueprint to build | Container: instance of image | Dockerfile: recipe for image, a text file | Layer: modifications of the image | . Dockerfile commands . FROM ubuntu:18.04: build image upon ubuntu base image | LABEL maintainer=&quot;Firstname Lastname email: optioal metadata | ENV LANG=C.UTF-8 LC_ALL=C.UTF-8: Environment variables | RUN apt-get update &amp;&amp; apt-get install -y python-pip: shell commands to run | EXPOSE 7745: open a port (e.g. for jupyter). Publises only if used -p or -P with run | VOLUME /ds: mount externally mounted volumes (need to specify the mountpoint at container run/creation) | data inside this won&#39;t be saved, but outide of it will | . | WORKDIR /ds: starting point for relative file references | COPY hom* /mydir/: copies new files and add them to the container&#39;s filesystem at a destination path | CMD [&#39;/bin/bash&#39;]: run a bash shell to prevent closing the container | only the last CMD will be executed | . | . Build a Docker Image . docker build -t tutorial -f ./Dockerfile.gpu ./ . (This builds an image not a container) . -t tutorial: name or tag | -f ./Dockerfile.gpu: Dockerfile location | ./: context host directory, the relative path refrence point | . Build a Docker container from an Image . docker run -it --name container1 --net=host -v ~/docker_files/:/ds tutorial . -it: interactive run | --net=host: bind ports to host | -v /docker_files/:/ds: Mount host directory to you as a volume : the mount detination | tutorial: image name | . Container-related commands . Open a terminal iteractively in the container1 container. . docker exec -it container1 bash . Save current state of container as new image (username is e.g. for DockerHub) . docker commit container1 username/image2name:tagname . List running containers . docker ps -a -f status=running . List images . docker images . Push image to registry . docker push username/image2name:tagname .",
            "url": "https://andrasnovoszath.com/2020/10/06/docker-cheatsheet.html",
            "relUrl": "/2020/10/06/docker-cheatsheet.html",
            "date": " • Oct 6, 2020"
        }
        
    
  
    
        ,"post14": {
            "title": "A first view on Streamlit",
            "content": "I had a look into streamlit as anoter way to deploy a data science app. It seems really convenient to work with. . Unfortunately, I cannot use it on a static website, so I need to learn how to deploy it on a service with Docker. . Tutorial . I went through the getting started tutorial, below are the main steps. . We import streamlit as a separate package and simply run it in as script. . streamlit run first_app.py . This, by default, creates a local server where we can see the results. . import streamlit as st import numpy as np import pandas as pd import altair as alt . Streamlit tries to diplay everything, somehow similar how it happens in a jupyter notebook. . df = pd.DataFrame({&quot;first&quot;: [1, 2, 3, 4], &quot;second&quot;: [10, 20, 30, 40]}) df . first second . 0 1 | 10 | . 1 2 | 20 | . 2 3 | 30 | . 3 4 | 40 | . . chart_data = pd.DataFrame(np.random.randn(20, 3), columns=[&quot;a&quot;, &quot;b&quot;, &quot;c&quot;]) st.line_chart(chart_data) . &amp;lt;streamlit.delta_generator.DeltaGenerator at 0x7fd5941fdc70&amp;gt; . . map_data = pd.DataFrame( np.random.randn(1000, 2) / [50, 50] + [37.76, -122.4], columns=[&quot;lat&quot;, &quot;lon&quot;] ) st.map(map_data) . &amp;lt;streamlit.delta_generator.DeltaGenerator at 0x7fd5941fdc70&amp;gt; . . if st.checkbox(&#39;Show dataframe&#39;): chart_data = pd.DataFrame(np.random.randn(20, 3), columns = [&#39;a&#39;, &#39;b&#39;, &#39;c&#39;]) st.line_chart(chart_data) option = st.sidebar.selectbox(&quot;Which number do you like best?&quot;, df[&#39;first&#39;]) &#39;You selected &#39;, df.loc[df[&#39;first&#39;] == option, :] . (&amp;#39;You selected &amp;#39;, first second 0 1 10) . . import time &quot;Long computation...&quot; # Add a placeholder latest_iteration = st.empty() bar = st.progress(0) for i in range(100): # Update progress bar with each iteration latest_iteration.text(f&quot;Iteration {i + 1}&quot;) bar.progress(i + 1) time.sleep(0.1) &quot;...and done!&quot; . &amp;#39;...and done!&amp;#39; . .",
            "url": "https://andrasnovoszath.com/2020/10/05/streamlit_tutorial.html",
            "relUrl": "/2020/10/05/streamlit_tutorial.html",
            "date": " • Oct 5, 2020"
        }
        
    
  
    
        ,"post15": {
            "title": "Command line tips for quick navigation in the terminal",
            "content": "Things I wish I had knew earlier. . Most of the tips are from here . Basics . Navigation to any folder . &gt; cd /home/andras/Projects/data_science_blog/ /home/andras/Projects/data_science_blog . With the ~ we can jump to our home folder . &gt; cd ~ /home/andras . cd . We do not even need the ~ to navigate to our home folder. . &gt; cd / / &gt; cd /home/andras . Bash function . We can also create a bash function so we can spare even typing cd . Here is the function . # .bashrc function blog() { cd &quot;$HOME/Projects/data_science_blog&quot; } . And jump! . &gt; blog /home/andras/Projects/data_science_blog . Symbolic link . An another possible way is to create a symbolic link in our home folder. . &gt; ln -s ~/Projects/data_science_blog/ dsblog &gt; ls -l ~/dsblog lrwxrwxrwx 1 andras andras 40 okt 4 10:39 /home/andras/dsblog -&gt; /home/andras/Projects/data_science_blog/ . And jump! . &gt; cd ~/dsblog /home/andras/dsblog . However, because of the way symbolic links work, our ‘parent’ folder is still the home directory. . &gt; cd .. /home/andras . cd - . With - we can jump back to our previous folder . &gt; cd - /home/andras/Projects/data_science_blog . Path variable in .bashrc . A further option is to create a variable in bash . # .bashrc BLOG=/home/andras/Projects/data_science_blog . cd $BLOG /home/andras/Projects/data_science_blog . Create an alias . # .bashrc alias ds_blog=&quot;cd /home/andras/Projects/data_science_blog&quot; . &gt; ds_blog /home/andras/Projects/data_science_blog .",
            "url": "https://andrasnovoszath.com/terminal/navigation/tips/2020/10/04/terminal-navigation-tips.html",
            "relUrl": "/terminal/navigation/tips/2020/10/04/terminal-navigation-tips.html",
            "date": " • Oct 4, 2020"
        }
        
    
  
    
        ,"post16": {
            "title": "Central Limit Theorem on the binomial distribution from scratch with Python",
            "content": "Here I am playing with the Central Limit Theorem. . I am running bernoulli trials to generate binomial distributions. . I was interested in how the results approximate the normal distribution, so I labelled their status at particular iteration rounds and plotted the result. . The code is based upon the respective example from Data Science from Scratch. . Libraries . import random import pandas as pd import numpy as np import altair as alt import math as m from collections import Counter . alt.data_transformers.disable_max_rows() . DataTransformerRegistry.enable(&amp;#39;default&amp;#39;) . Parameters . p = .35 binomial_trials = 10000 total_rounds = 5000 step = 100 . Bernoulli Trials . def bernoulli_trial(p: float) -&gt; int: return 1 if random.random() &lt; p else 0 . Let&#39;s run the trials many times. . trials = [] for _ in range(binomial_trials): trials.append(bernoulli_trial(p)) print(&quot;First thousand trials:&quot;) print(&quot; &quot;.join([str(t) for t in trials[:1000]])) print(f&quot; nOnes: {sum([t == 1 for t in trials])}, Zeros: {sum([t == 0 for t in trials])}&quot;) print(&quot; nMean value of &#39;success&#39; (ones):&quot;, np.mean(trials)) . First thousand trials: 0 0 0 0 1 0 0 0 0 0 0 0 0 1 1 0 0 1 0 0 1 1 0 0 0 1 0 0 1 1 0 1 0 0 0 0 0 1 0 0 0 1 1 0 0 0 1 0 1 0 1 0 1 1 0 1 0 0 1 1 0 1 0 1 1 0 0 1 1 1 1 1 0 1 1 0 0 0 1 1 1 0 0 0 1 0 0 0 1 0 0 0 1 0 0 1 1 0 0 1 0 0 0 0 0 0 1 0 1 0 1 0 1 0 0 0 1 1 1 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 1 0 1 0 0 1 1 0 0 1 1 0 1 0 0 0 0 0 1 0 1 0 0 0 1 0 0 0 0 1 1 0 1 1 1 1 0 0 0 0 0 1 0 1 0 1 0 0 0 0 1 0 1 1 0 1 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 1 0 0 1 1 0 0 0 1 0 1 0 1 1 0 0 0 1 1 0 1 0 0 1 1 0 0 1 1 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 1 0 1 1 0 0 1 0 0 1 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 1 1 0 0 1 1 0 0 1 0 0 0 0 1 0 0 0 0 1 0 1 0 0 0 0 1 1 0 1 1 0 1 0 0 1 0 1 0 0 1 0 0 1 0 0 1 1 1 0 0 0 0 1 1 0 1 1 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 1 1 0 1 1 0 0 0 0 0 1 0 0 0 1 0 1 0 1 0 1 0 0 0 0 0 0 1 1 0 0 0 1 0 0 1 0 1 1 0 0 0 0 1 1 0 0 0 1 0 0 1 1 0 1 1 1 0 0 0 1 0 0 1 1 0 0 0 0 0 1 0 1 0 1 0 0 0 1 1 0 1 1 1 1 0 0 0 0 1 0 0 1 0 0 0 0 1 0 1 1 0 0 0 0 0 0 0 0 1 1 0 0 0 1 1 0 1 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 1 0 1 0 0 0 0 0 1 0 0 0 1 0 1 0 0 0 1 0 0 1 0 0 0 1 1 0 0 1 1 1 0 1 0 1 0 1 0 0 0 0 0 1 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 1 0 0 1 1 0 0 1 1 0 0 0 1 0 0 1 0 1 0 1 0 0 1 1 1 0 0 1 0 0 0 1 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 1 1 0 0 0 1 0 0 0 1 1 0 0 1 1 1 0 1 0 0 1 1 0 0 0 0 0 1 1 1 1 1 0 0 0 1 0 0 0 1 1 0 0 0 1 1 0 0 0 1 1 0 1 0 1 0 1 0 1 0 0 0 1 0 1 0 1 1 1 1 1 1 0 1 1 0 0 1 0 1 0 1 0 1 1 1 0 1 0 0 0 1 0 1 0 0 0 0 0 0 1 1 0 0 1 0 0 1 0 0 1 0 0 0 0 0 0 0 1 0 0 1 1 0 1 1 0 0 1 0 0 0 0 0 1 0 0 1 0 1 1 1 1 0 1 0 0 0 0 1 0 0 1 0 0 1 0 0 0 1 0 0 0 1 1 0 1 0 1 0 0 0 0 0 1 0 1 1 1 1 0 1 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 1 0 0 0 0 0 1 1 1 0 1 0 0 1 0 1 1 0 0 0 1 0 0 0 0 0 0 1 0 1 0 1 0 0 0 1 0 0 0 0 1 0 0 0 0 1 0 1 0 1 0 0 0 1 0 0 1 1 0 0 0 1 1 0 1 0 0 1 0 0 0 1 0 1 1 1 0 0 0 0 0 0 1 1 0 1 0 1 0 0 0 1 1 0 1 0 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 1 1 0 0 0 0 1 0 0 0 0 0 0 0 1 1 0 1 1 1 0 1 1 0 0 1 1 1 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 1 1 Ones: 3594, Zeros: 6406 Mean value of &amp;#39;success&amp;#39; (ones): 0.3594 . def binomial(n: int, p: float) -&gt; int: return sum(bernoulli_trial(p) for _ in range(n)) trials = binomial(binomial_trials, p) trials . 3441 . Iterating the binomial . def iterate_binomial(n, p, num_points): return [binomial(n, p) for _ in range(num_points)] simulations = iterate_binomial(binomial_trials, p, total_rounds) print(&quot;First hunded values:&quot;) print(&quot; &quot;.join([str(results) for results in simulations[:100]])) print(&quot; nMean:&quot;, np.mean(simulations), &quot;Standard deviation:&quot;, np.std(simulations)) . First hunded values: 3483 3596 3442 3470 3479 3570 3521 3512 3470 3499 3446 3495 3530 3527 3485 3537 3445 3538 3478 3542 3510 3444 3478 3384 3452 3566 3538 3548 3485 3425 3547 3456 3411 3515 3466 3454 3428 3485 3514 3524 3411 3530 3559 3401 3459 3513 3580 3533 3527 3502 3476 3465 3478 3475 3493 3537 3558 3499 3511 3496 3572 3555 3480 3541 3519 3477 3460 3496 3501 3524 3489 3499 3537 3527 3557 3508 3503 3530 3489 3497 3519 3509 3502 3484 3499 3488 3537 3567 3395 3469 3537 3440 3542 3493 3385 3539 3490 3504 3398 3555 Mean: 3500.871 Standard deviation: 47.232848304966744 . As we are interested in the intermediate states, we add new rounds iteratively and labeling them in the way. . def add_iterations(probability, trials, total_rounds, step): iteration_results = pd.DataFrame() for round in range(step, total_rounds + 1, step): simulations = iterate_binomial(trials, probability, round) counts = Counter(simulations) iteration_results = pd.concat( [ iteration_results, pd.DataFrame({&#39;count&#39;: counts.values(), &#39;value&#39;: counts.keys(), &#39;iterations&#39;: round}) ] ) return iteration_results iteration_results = add_iterations(p, binomial_trials, total_rounds, step) iteration_results . count value iterations . 0 1 | 3447 | 100 | . 1 1 | 3457 | 100 | . 2 1 | 3530 | 100 | . 3 1 | 3496 | 100 | . 4 1 | 3551 | 100 | . ... ... | ... | ... | . 265 2 | 3410 | 5000 | . 266 1 | 3663 | 5000 | . 267 1 | 3372 | 5000 | . 268 1 | 3644 | 5000 | . 269 1 | 3622 | 5000 | . 11949 rows × 3 columns . As we are interested in the total counts at each step we need to cumulatively add the value counts together over the iteration rounds. . def sum_counts(value_df): return value_df.sort_values(&#39;iterations&#39;)[&#39;count&#39;].cumsum() iteration_results = iteration_results.sort_values([&#39;value&#39;, &#39;iterations&#39;]) iteration_results[&#39;cumulative count&#39;] = iteration_results.groupby(&#39;value&#39;).apply(sum_counts).values iteration_results . count value iterations cumulative count . 233 1 | 3285 | 4400 | 1 | . 200 1 | 3286 | 4700 | 1 | . 161 1 | 3288 | 1900 | 1 | . 244 1 | 3313 | 2200 | 1 | . 156 1 | 3314 | 3200 | 1 | . ... ... | ... | ... | ... | . 130 1 | 3691 | 4700 | 1 | . 240 1 | 3692 | 4100 | 1 | . 144 1 | 3695 | 5000 | 1 | . 155 1 | 3700 | 2400 | 1 | . 271 1 | 3704 | 4500 | 1 | . 11949 rows × 4 columns . Normal distributions . We also generate a normal distribution for reference . def calc_normal_cdf(x: float, mu: float = 0, sigma: float = 1) -&gt; float: return (1 + m.erf((x - mu) / m.sqrt(2) / sigma)) /2 def calc_normal_pdf(x: float, mu: float = 0, sigma: float=1) -&gt; float: return m.exp(-(x-mu)**2 / (2 * sigma **2)) * 1/(m.sqrt(2 * m.pi) * sigma) . n = binomial_trials mu = p * n sigma = m.sqrt(n * p * (1 - p)) data = iteration_results.loc[iteration_results[&#39;iterations&#39;] == iteration_results[&#39;iterations&#39;].max(), &#39;value&#39;] xs = range(min(data), max(data) + 1) ys = [calc_normal_cdf(i + .5, mu, sigma) - calc_normal_cdf(i - .5, mu, sigma) for i in xs] normal_dist = pd.DataFrame({&#39;x&#39;: xs, &#39;y&#39;:ys}) normal_dist . x y . 0 3347 | 0.000049 | . 1 3348 | 0.000052 | . 2 3349 | 0.000056 | . 3 3350 | 0.000060 | . 4 3351 | 0.000064 | . ... ... | ... | . 344 3691 | 0.000003 | . 345 3692 | 0.000003 | . 346 3693 | 0.000002 | . 347 3694 | 0.000002 | . 348 3695 | 0.000002 | . 349 rows × 2 columns . Visualization . With the slider you can see the intermediate results at each iteration. . label = alt.selection_single( encodings=[&#39;x&#39;], on=&#39;mouseover&#39;, nearest=True, empty=&#39;none&#39; ) iteration_bounds = alt.binding_range(min=iteration_results[&#39;iterations&#39;].min(), max=iteration_results[&#39;iterations&#39;].max(), step=step) iteration_slider = alt.selection_single(bind=iteration_bounds, fields=[&#39;iterations&#39;], name=&#39;Iteration&#39;, init={&#39;iterations&#39;: iteration_results[&#39;iterations&#39;].median()}) bar_chart = alt.Chart(iteration_results).mark_bar().encode( alt.X(&#39;value:Q&#39;, scale=alt.Scale(domain=(iteration_results[&#39;value&#39;].min(), iteration_results[&#39;value&#39;].max()))), alt.Y(&#39;cumulative count:Q&#39;, scale=alt.Scale(domain=(iteration_results[&#39;cumulative count&#39;].min(), iteration_results[&#39;cumulative count&#39;].max()))), opacity=alt.value(0.75) ) line_chart = alt.Chart(normal_dist).mark_line().encode( alt.X(&#39;x&#39;), alt.Y(&#39;y&#39;,),color=alt.value(&#39;darkred&#39;) ) bar_chart = alt.layer( bar_chart.add_selection(iteration_slider).transform_filter(iteration_slider), bar_chart.mark_circle().encode(opacity=alt.condition(label, alt.value(1), alt.value(0))).add_selection(label).transform_filter(iteration_slider), bar_chart.mark_text(dx=5, dy=-5, align=&#39;left&#39;, strokeWidth=.5, stroke=&#39;black&#39;).encode(text=alt.Text(&#39;cumulative count:Q&#39;)).transform_filter(label).transform_filter(iteration_slider) ) alt.layer( bar_chart, line_chart, ).resolve_scale(y=&#39;independent&#39;).properties(width=800) .",
            "url": "https://andrasnovoszath.com/2020/10/03/central-limit-theorem.html",
            "relUrl": "/2020/10/03/central-limit-theorem.html",
            "date": " • Oct 3, 2020"
        }
        
    
  
    
        ,"post17": {
            "title": "Probabilty distributions in Python 'from scratch'",
            "content": "Here I write up some functions to generate probability univariate and normal probability distributions based on the book Data Science from Scratch . Libraries . import math as m import altair as alt import numpy as np import pandas as pd . alt.data_transformers.disable_max_rows() . DataTransformerRegistry.enable(&amp;#39;default&amp;#39;) . Uniform distribution . def uniform_pdf(x: float) -&gt; float: return 1 if 0 &lt;= x &lt; 1 else 0 . def uniform_cdf(x: float) -&gt; float: if x &lt; 0: return 0 elif x &lt; 1 : return x else: return 1 . Example values . print(&quot;x tpdf tcdf n&quot;) for x in [-2, 0, .2, .8, 1, 1.5]: print(f&quot;{x} t{uniform_pdf(x)} t{uniform_cdf(x)}&quot;) . x pdf cdf -2 0 0 0 1 0 0.2 1 0.2 0.8 1 0.8 1 0 1 1.5 0 1 . For plotting we generate both cdf and pdf values in a tidy format. . x = pd.Series(np.linspace(-1, 2, 1000)) uniform = pd.DataFrame( { &#39;x&#39;: x, &#39;pdf&#39;: x.apply(uniform_pdf), &#39;cdf&#39;: x.apply(uniform_cdf) } ).melt(id_vars=&#39;x&#39;) uniform . x variable value . 0 -1.000000 | pdf | 0.0 | . 1 -0.996997 | pdf | 0.0 | . 2 -0.993994 | pdf | 0.0 | . 3 -0.990991 | pdf | 0.0 | . 4 -0.987988 | pdf | 0.0 | . ... ... | ... | ... | . 1995 1.987988 | cdf | 1.0 | . 1996 1.990991 | cdf | 1.0 | . 1997 1.993994 | cdf | 1.0 | . 1998 1.996997 | cdf | 1.0 | . 1999 2.000000 | cdf | 1.0 | . 2000 rows × 3 columns . uniform.groupby(&#39;variable&#39;).describe().loc[:, (&#39;value&#39;, slice(None))].T . variable cdf pdf . value count 1000.000000 | 1000.000000 | . mean 0.500000 | 0.333000 | . std 0.441243 | 0.471522 | . min 0.000000 | 0.000000 | . 25% 0.000000 | 0.000000 | . 50% 0.500000 | 0.000000 | . 75% 1.000000 | 1.000000 | . max 1.000000 | 1.000000 | . chart = alt.Chart().mark_line().encode( alt.X(&#39;x:Q&#39;), alt.Y(&#39;value:Q&#39;), alt.Color(&#39;variable:N&#39;), ) label = alt.selection_single( encodings=[&#39;x&#39;], on=&#39;mouseover&#39;, nearest=True, empty=&#39;none&#39; ) alt.layer( chart, chart.mark_circle().encode(opacity=alt.condition(label, alt.value(1), alt.value(0))).add_selection(label), alt.Chart().mark_rule(color=&#39;darkgray&#39;).encode(alt.X(&#39;x:Q&#39;)).transform_filter(label), chart.mark_text(align=&#39;left&#39;, dx=5, dy=-5, strokeWidth=0.5).encode( text=alt.Text(&#39;value:Q&#39;, format=&#39;,.4f&#39;) ).transform_filter(label), data=uniform ).properties(width=600, title=&#39;Uniform PDF and CDF&#39;) . Normal PDF . def calc_normal_pdf(x: float, mu: float = 0, sigma: float=1) -&gt; float: return m.exp(-(x-mu)**2 / (2 * sigma **2)) * 1/(m.sqrt(2 * m.pi) * sigma) . x = pd.Series(np.linspace(-5, 5, 1000)) normal_pdf = pd.DataFrame( { &#39;x&#39;: x, &#39;mu=0, sigma=1&#39;: x.apply(calc_normal_pdf), &#39;mu=0, sigma=2&#39;: x.apply(lambda x: calc_normal_pdf(x, 0, 2)), &#39;mu=0, sigma=3&#39;: x.apply(lambda x: calc_normal_pdf(x, 0, 3)) } ).melt(id_vars=&#39;x&#39;) normal_pdf . x variable value . 0 -5.00000 | mu=0, sigma=1 | 0.000001 | . 1 -4.98999 | mu=0, sigma=1 | 0.000002 | . 2 -4.97998 | mu=0, sigma=1 | 0.000002 | . 3 -4.96997 | mu=0, sigma=1 | 0.000002 | . 4 -4.95996 | mu=0, sigma=1 | 0.000002 | . ... ... | ... | ... | . 2995 4.95996 | mu=0, sigma=3 | 0.033902 | . 2996 4.96997 | mu=0, sigma=3 | 0.033715 | . 2997 4.97998 | mu=0, sigma=3 | 0.033529 | . 2998 4.98999 | mu=0, sigma=3 | 0.033344 | . 2999 5.00000 | mu=0, sigma=3 | 0.033159 | . 3000 rows × 3 columns . normal_pdf.groupby(&#39;variable&#39;).describe()[&#39;value&#39;].T . variable mu=0, sigma=1 mu=0, sigma=2 mu=0, sigma=3 . count 1000.000000 | 1000.000000 | 1000.000000 | . mean 0.099900 | 0.098668 | 0.090385 | . std 0.134980 | 0.065984 | 0.032457 | . min 0.000001 | 0.008764 | 0.033159 | . 25% 0.000351 | 0.034353 | 0.060851 | . 50% 0.017420 | 0.091182 | 0.093905 | . 75% 0.181794 | 0.163888 | 0.121860 | . max 0.398937 | 0.199471 | 0.132981 | . label = alt.selection_single( encodings=[&#39;x&#39;], on=&#39;mouseover&#39;, nearest=True, empty=&#39;none&#39; ) chart = alt.Chart().mark_line().encode( alt.X(&#39;x:Q&#39;), alt.Y(&#39;value:Q&#39;), alt.Color(&#39;variable:N&#39;) ) alt.layer( chart, chart.mark_circle().encode(opacity=alt.condition(label, alt.value(1), alt.value(0))).add_selection(label), alt.Chart().mark_rule(color=&#39;darkgray&#39;).encode(alt.X(&#39;x:Q&#39;)).transform_filter(label), chart.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode(text=alt.Text(&#39;value:Q&#39;, format=&#39;,.6f&#39;)).transform_filter(label), # tooltip=alt.Tooltip(&#39;value:Q&#39;), data=normal_pdf ).properties(width=600, title=&quot;Normal PDF&quot;) . Normal CDF . Finally, we also calculate and plot the CDF or normal distribution. . def calc_normal_cdf(x: float, mu: float = 0, sigma: float = 1) -&gt; float: return (1 + m.erf((x - mu) / m.sqrt(2) / sigma)) /2 . normal_cdf = pd.DataFrame( { &#39;x&#39;: x, &#39;mu=0, sigma=1&#39;: x.apply(calc_normal_cdf), &#39;mu=0, sigma=2&#39;: x.apply(lambda x: calc_normal_cdf(x, 0, 2)), &#39;mu=0, sigma=3&#39;: x.apply(lambda x: calc_normal_cdf(x, 0, 3)), } ).melt(id_vars=&#39;x&#39;) normal_cdf . x variable value . 0 -5.00000 | mu=0, sigma=1 | 2.866516e-07 | . 1 -4.98999 | mu=0, sigma=1 | 3.019121e-07 | . 2 -4.97998 | mu=0, sigma=1 | 3.179543e-07 | . 3 -4.96997 | mu=0, sigma=1 | 3.348164e-07 | . 4 -4.95996 | mu=0, sigma=1 | 3.525386e-07 | . ... ... | ... | ... | . 2995 4.95996 | mu=0, sigma=3 | 9.508671e-01 | . 2996 4.96997 | mu=0, sigma=3 | 9.512055e-01 | . 2997 4.97998 | mu=0, sigma=3 | 9.515421e-01 | . 2998 4.98999 | mu=0, sigma=3 | 9.518768e-01 | . 2999 5.00000 | mu=0, sigma=3 | 9.522096e-01 | . 3000 rows × 3 columns . normal_cdf.describe()[&#39;value&#39;].T . count 3.000000e+03 mean 5.000000e-01 std 3.760737e-01 min 2.866516e-07 25% 1.055739e-01 50% 5.000000e-01 75% 8.944261e-01 max 9.999997e-01 Name: value, dtype: float64 . label = alt.selection_single( encodings=[&#39;x&#39;], on=&#39;mouseover&#39;, nearest=True, empty=&#39;none&#39; ) chart = alt.Chart().mark_line().encode( alt.X(&#39;x:Q&#39;), alt.Y(&#39;value:Q&#39;), alt.Color(&#39;variable:N&#39;) ) alt.layer( chart, chart.mark_circle().encode(opacity=alt.condition(label, alt.value(1), alt.value(0))).add_selection(label), alt.Chart().mark_rule(color=&#39;darkgray&#39;).encode(alt.X(&#39;x:Q&#39;)).transform_filter(label), chart.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode(text=alt.Text(&#39;value:Q&#39;, format=&#39;,.6f&#39;)).transform_filter(label), # tooltip=alt.Tooltip(&#39;value:Q&#39;), data=normal_cdf ).properties(width=600, title=&quot;Normal CDFs&quot;) .",
            "url": "https://andrasnovoszath.com/2020/10/02/probability-distributions.html",
            "relUrl": "/2020/10/02/probability-distributions.html",
            "date": " • Oct 2, 2020"
        }
        
    
  
    
        ,"post18": {
            "title": "Computational checking of the girl/boy probability problem",
            "content": "Here I replay the classical two child problem and code it up in Python. . In a nutshell, the solution changes depending on if and how we differentiate between the children (for example, whether we talk about &#39;older&#39; or &#39;younger&#39; child or just refering to them as &#39;either&#39;). . I also chart the solutions while I run the simulation to see how the probabilites tend to converge to their values. . import enum, random . class Kid(enum.Enum): Boy = 0 Girl = 1 . def random_kid() -&gt; Kid: return random.choice([Kid.Boy, Kid.Girl]) . random.seed(42) . both_girls = 0 older_girl = 0 either_girl = 0 results = [] for _ in range(1000): younger = random_kid() older = random_kid() if older == Kid.Girl: older_girl += 1 if older == Kid.Girl and younger == Kid.Girl: both_girls += 1 if older == Kid.Girl or younger == Kid.Girl: either_girl += 1 try: p_both_older = both_girls / older_girl except ZeroDivisionError: p_both_older = 0 try: p_both_either = both_girls / either_girl except ZeroDivisionError: p_both_either = 0 results.append([younger.name, older.name, both_girls, older_girl, either_girl, p_both_either, p_both_older]) . import altair as alt import pandas as pd . df_results = pd.DataFrame(results, columns=[&#39;younger&#39;, &#39;older&#39;, &#39;both girls&#39;, &#39;older girl&#39;, &#39;either girl&#39;, &#39;P(Both|Either)&#39;, &#39;P(Both|Older)&#39;]).reset_index() . df_results . index younger older both girls older girl either girl P(Both|Either) P(Both|Older) . 0 0 | Boy | Boy | 0 | 0 | 0 | 0.000000 | 0.000000 | . 1 1 | Girl | Boy | 0 | 0 | 1 | 0.000000 | 0.000000 | . 2 2 | Boy | Boy | 0 | 0 | 1 | 0.000000 | 0.000000 | . 3 3 | Boy | Boy | 0 | 0 | 1 | 0.000000 | 0.000000 | . 4 4 | Girl | Boy | 0 | 0 | 2 | 0.000000 | 0.000000 | . ... ... | ... | ... | ... | ... | ... | ... | ... | . 995 995 | Girl | Boy | 263 | 513 | 763 | 0.344692 | 0.512671 | . 996 996 | Girl | Girl | 264 | 514 | 764 | 0.345550 | 0.513619 | . 997 997 | Girl | Girl | 265 | 515 | 765 | 0.346405 | 0.514563 | . 998 998 | Girl | Boy | 265 | 515 | 766 | 0.345953 | 0.514563 | . 999 999 | Girl | Girl | 266 | 516 | 767 | 0.346806 | 0.515504 | . 1000 rows × 8 columns . to_plot = df_results.melt(id_vars=&#39;index&#39;) to_plot.loc[to_plot[&#39;variable&#39;].isin([&#39;both girls&#39;, &#39;older girl&#39;, &#39;either girl&#39;]), &#39;type&#39;] = &#39;count&#39; to_plot.loc[to_plot[&#39;variable&#39;].isin([&#39;P(Both|Either)&#39;, &#39;P(Both|Older)&#39;]), &#39;type&#39;] = &#39;probability&#39; to_plot . index variable value type . 0 0 | younger | Boy | NaN | . 1 1 | younger | Girl | NaN | . 2 2 | younger | Boy | NaN | . 3 3 | younger | Boy | NaN | . 4 4 | younger | Girl | NaN | . ... ... | ... | ... | ... | . 6995 995 | P(Both|Older) | 0.512671 | probability | . 6996 996 | P(Both|Older) | 0.513619 | probability | . 6997 997 | P(Both|Older) | 0.514563 | probability | . 6998 998 | P(Both|Older) | 0.514563 | probability | . 6999 999 | P(Both|Older) | 0.515504 | probability | . 7000 rows × 4 columns . chart = alt.Chart(to_plot).encode(alt.X(&#39;index:Q&#39;)) label = alt.selection_single(encodings=[&#39;x&#39;], on=&#39;mouseover&#39;, nearest=True, empty=&#39;none&#39;) count_chart = alt.Chart(data=to_plot[to_plot[&#39;type&#39;] == &#39;count&#39;]).mark_line().encode( alt.X(&#39;index:Q&#39;), alt.Y(&#39;value:Q&#39;), color=alt.Color(&#39;variable:N&#39;), ) probablity_chart = alt.Chart(to_plot[to_plot[&#39;type&#39;] == &#39;probability&#39;]).mark_line().encode( alt.X(&#39;index:Q&#39;), alt.Y(&#39;value:Q&#39;), color=alt.Color(&#39;variable:N&#39;) ) values_chart = alt.layer( probablity_chart.add_selection(label), probablity_chart.mark_rule(color=&#39;gray&#39;).encode(alt.X(&#39;index:Q&#39;)).transform_filter(label), count_chart, ).resolve_scale(y=&#39;independent&#39;).properties(width=600) values_chart .",
            "url": "https://andrasnovoszath.com/2020/10/01/boy-girl-probability.html",
            "relUrl": "/2020/10/01/boy-girl-probability.html",
            "date": " • Oct 1, 2020"
        }
        
    
  
    
        ,"post19": {
            "title": "Correlation and covariance from scratch",
            "content": "In this post we examine covariance and a correlation a bit closer. . We will use them to examine the relationship between Ethereum transaction value and gas price. . Again, most of the time, we break down the steps into standard Python data types and operations (i.e. we use numpy mostly for verification of our results). . Libraries and data load . We pull the data from Google&#39;s public datasets with BigQuery, use pandas and numpy to manipulate it, and altair to plot their relationship. . import os os.environ[&quot;GOOGLE_APPLICATION_CREDENTIALS&quot;]=os.path.expanduser(&quot;~/.credentials/Notebook bigquery-c422e406404b.json&quot;) from google.cloud import bigquery client = bigquery.Client() import altair as alt alt.data_transformers.disable_max_rows() import numpy as np import pandas as pd . query =&quot;&quot;&quot; SELECT EXTRACT(DATE FROM block_timestamp) AS date, AVG(value) AS value, AVG(gas_price) AS gas_price, FROM `bigquery-public-data.ethereum_blockchain.transactions` WHERE EXTRACT(YEAR FROM block_timestamp) = 2019 GROUP BY date ORDER BY date &quot;&quot;&quot; . transactions = client.query(query).to_dataframe(dtypes={&#39;value&#39;: float, &#39;gas_price&#39;: float}, date_as_object=False) transactions.head() . date value gas_price . 0 2019-01-01 | 3.719103e+18 | 1.431514e+10 | . 1 2019-01-02 | 4.649915e+18 | 1.349952e+10 | . 2 2019-01-03 | 4.188781e+18 | 1.269504e+10 | . 3 2019-01-04 | 6.958368e+18 | 1.418197e+10 | . 4 2019-01-05 | 8.167590e+18 | 2.410475e+10 | . There are a few days when the gas prices were outstandingly high so we remove values beyond three standard deviation from the mean. . Outliers . labelx = alt.selection_single( encodings=[&#39;x&#39;], on=&#39;mouseover&#39;, empty=&#39;none&#39; ) labely = alt.selection_single( encodings=[&#39;y&#39;], on=&#39;mouseover&#39;, empty=&#39;none&#39; ) ruler = alt.Chart().mark_rule(color=&#39;darkgray&#39;) chart = alt.Chart().mark_point().encode( alt.X(&#39;value&#39;, axis=alt.Axis(format=(&#39;,.2e&#39;))), alt.Y(&#39;gas_price&#39;, axis=alt.Axis(format=(&#39;,.2e&#39;))), alt.Tooltip([&#39;value&#39;, &#39;gas_price&#39;, &#39;date&#39;]) ).properties(width=600, height=400, title=&#39;Trasaction values and gas prices&#39;).add_selection(labelx).add_selection(labely) alt.layer( chart, ruler.encode(x=&#39;value:Q&#39;).transform_filter(labelx), ruler.encode(y=&#39;gas_price:Q&#39;).transform_filter(labely), data=transactions ).interactive() . transactions = transactions[~(transactions[&#39;gas_price&#39;] &gt;= transactions[&#39;gas_price&#39;].mean() + 3 * transactions[&#39;gas_price&#39;].std())] . values = transactions[&#39;value&#39;] gas_prices = transactions[&#39;gas_price&#39;] . As we emphasize standard operations, we use a few helper functions in the steps leading to covariance and correlation. . Helper functions . from typing import Union, List Vector = List[float] . def dot(vector1: Vector, vector2: Vector) -&gt; float: assert len(vector1) == len(vector2) return sum(v1 * v2 for v1, v2 in zip(vector1, vector2)) assert dot([1, 2, 3], [4, 5, 6]) == 32 def mean(x: Vector) -&gt; float: return sum(x) / len(x) assert mean([1, 2, 3, 4]) == 2.5 def de_mean(xs: Vector) -&gt; Vector: x_mean = mean(xs) return [x - x_mean for x in xs] assert de_mean([4, 5, 6, 7, 8]) == [-2, -1, 0, 1, 2] def sum_of_squares(xs: Vector) -&gt; float: return dot(xs, xs) assert sum_of_squares([1, 2, 3]) == 14 def variance(xs: Vector) -&gt; float: return sum_of_squares(de_mean(xs)) / (len(xs) - 1) assert variance([1, 2, 3]) == 1 import math as m def standard_deviation(xs: Vector): return m.sqrt(variance(xs)) assert standard_deviation([4, 5, 6]) == 1 . Covariance looks at the degree two variables &#39;move together&#39;. . For this, first, it multiplies the variables&#39; deviation from their respective means. This produces a series of values which are very high for those observations where both variables deviate a lot. Furthermore, when the two variables deviate to the same direction these values are positive, otherwise they are negative. . Then, it calculates the mean of these multiplied deviation values. However, because we are calculating the sample covariance, we divide their sum by $n + 1$ (where $n$ is the number of observations) . $ text{Cov} = frac { sum_{i=1}^n (x- bar{x}) (y- bar{y})} {n - 1} $ . Covariance . def covariance(xs: Vector, ys: Vector) -&gt; float: assert len(xs) == len(ys) return dot(de_mean(xs), de_mean(ys)) / (len(xs) - 1) assert covariance([1, 2, 3], [4, 5, 6]) == 1 . There is also an alternate way to calculate covariance, using the variables&#39; expected values (which here are the means): . $ text{Cov} = E[ vec{x} vec{y}] - E[ vec{x}]E[ vec{y}] $ . This is a much simpler version. However, again, as we are dealing with sample data, so we need to adjust for that: . $ text{Cov}_s = frac {n} {n - 1} (E[ vec{x} vec{y}] - E[ vec{x}]E[ vec{y}]) $ . covariance(values, gas_prices) . 1.5856518696847875e+26 . def covariance_2(xs: Vector, ys: Vector) -&gt; float: xsys = [x * y for x, y in zip(xs, ys)] return (mean(xsys) - mean(xs) * mean(ys)) * len(xs) / (len(xs) - 1) assert np.isclose(covariance_2([1, 2, 3], [4, 5, 6]), 1) . We also verify our method with numpy. . np.cov(values, gas_prices)[0, 1] . 1.5856518696847882e+26 . covariance_2(values, gas_prices) . 1.585651869684888e+26 . Because the value of covariance really depends on the units of the variables, it is often hard to interpret and also to compare it with other covariences. . This is why correlation is an often preferred method as it adjusts the covariance by the variables&#39; standard deviation values. As a result, it bounds the end result into the $[-1, 1]$ domain making it comparable with other correlation values. . $ text{Corr(x, y)} = frac { text{Cov(x, y)} } { text{Std(x)} text{Std(y)}} $ . Correlation . correlation(values, gas_prices) . 0.035069533929694634 . Finally, we verify the result with numpy. . values.corr(gas_prices) . 0.03506953392969465 . def correlation(xs: Vector, ys: Vector) -&gt; float: return covariance(xs, ys) / (standard_deviation(xs) * standard_deviation(ys)) assert np.isclose(correlation([.1, .2, .3], [400, 500, 600]), 1) .",
            "url": "https://andrasnovoszath.com/2020/09/30/correlation.html",
            "relUrl": "/2020/09/30/correlation.html",
            "date": " • Sep 30, 2020"
        }
        
    
  
    
        ,"post20": {
            "title": "Matrix algebra 'from scratch' with type annotations",
            "content": "This is the continuation of yesterday&#39;s practice, but now with Matrix operations. . from typing import List, Tuple, Callable . Vector = List[float] Matrix = List[List[float]] . A = [[1, 2, 3], [4, 5, 6]] B = [[1, 2], [3, 4], [5, 6]] . Shape . def shape(A: Matrix) -&gt; Tuple[int, int]: number_of_rows = len(A) number_of_columns = len(A[0]) if A else 0 return number_of_rows, number_of_columns assert shape([[1, 2, 3],[4, 5, 6]]) == (2, 3) . Get rows and columns . def get_row(A: Matrix, i: int) -&gt; Vector: return A[i] def get_columns(A: Matrix, i: int) -&gt; Vector: return [row[i] for row in A] assert get_columns([[1, 2, 3],[4, 5, 6]], 1) == [2, 5] . Create a new matrix from a function of $(i, j)$ . def make_matrix( number_of_rows: int, number_of_columns: int, entry_function: Callable[[int, int], float] ) -&gt; Matrix: return [ [entry_function(i, j) for j in range(number_of_columns)] for i in range(number_of_rows) ] assert make_matrix(3, 2, lambda i, j: i + j) == [[0, 1], [1, 2], [2, 3]] . def identity_matrix(n: int) -&gt; Matrix: return make_matrix(n, n, lambda i, j: 1 if i == j else 0) assert identity_matrix(3) == [[1, 0, 0], [0, 1, 0], [0, 0, 1]] . identity_matrix(5) . [[1, 0, 0, 0, 0], [0, 1, 0, 0, 0], [0, 0, 1, 0, 0], [0, 0, 0, 1, 0], [0, 0, 0, 0, 1]] .",
            "url": "https://andrasnovoszath.com/2020/09/29/matrix-algebra.html",
            "relUrl": "/2020/09/29/matrix-algebra.html",
            "date": " • Sep 29, 2020"
        }
        
    
  
    
        ,"post21": {
            "title": "Vector Algebra 'from scratch" with type annotations",
            "content": "I looked into Joel Grus&#39; Data science from Scratch and found his approach really nice as he actively uses type annotations and assertions in his fuctions. . So, below I practice vector operation functions from the book. . from typing import List Vector = List[float] Vector . typing.List[float] . Vector addition and subtraction . $ vec{v} + vec{w} = begin{bmatrix} v_1 + w_1 ldots v_n + w_n end{bmatrix} $ . def add(vector1: Vector, vector2: Vector) -&gt; Vector: assert len(vector1) == len(vector2) return [v1 + v2 for v1, v2 in zip(vector1, vector2)] assert add([1, 2, 3], [4, 5, 6]) == [5, 7, 9] . $ vec{w} - vec{v} = begin{bmatrix} w_1 - v_1 ldots w_n - v_n end{bmatrix} $ . def subtract(vector1: Vector, vector2:Vector) -&gt; Vector: assert len(vector1) == len(vector2) return [v1 - v2 for v1, v2 in zip(vector1, vector2)] assert subtract([1, 2, 3], [4, 5, 6]) == [-3, -3, -3] . Vector sum . $ vec{v} + ldots + vec{w} = begin{bmatrix} v_1 + ldots + w_1 ldots v_n + ldots + w_n end{bmatrix} $ . def vector_sum(vectors: List[Vector]) -&gt; Vector: assert vectors vector_length = len(vectors[0]) assert all(len(v) == vector_length for v in vectors) sums = [0] * vector_length for vector in vectors: sums = add(sums, vector) return sums assert vector_sum([[1, 2, 3], [4, 5, 6], [7, 8, 9]]) == [12, 15, 18] . Scalar multiplication . $ c cdot vec{v} = begin{bmatrix} c cdot v_0 ldots c cdot v_n end{bmatrix} $ . def scalar_multiply(c: float, vector: Vector) -&gt; Vector: return [c * v for v in vector] assert scalar_multiply(4, [1, 2, 3]) == [4, 8, 12] . Vector mean . $ text{vector mean}( vec{w}, ldots, vec{v}) = begin{bmatrix} frac {v_1 + ldots + w_1}{n} ldots frac{v_n + ldots + w_n} {n} end{bmatrix} $ . def vector_mean(vectors: List[Vector]) -&gt; Vector: n = len(vectors) return scalar_multiply(1/n, vector_sum(vectors)) assert vector_mean([[1, 2], [3, 4], [5, 6]]) == [3, 4] . Dot product . $ vec{v} cdot vec{w} = sum_{i=1}^{n} v_1 w_1 + ldots + v_n w_n $ . def dot(vector1: Vector, vector2: Vector) -&gt; float: assert len(vector1) == len(vector2) return sum(v1 * v2 for v1, v2 in zip(vector1, vector2)) assert dot([1, 2, 3], [4, 5, 6]) == 32 . Vector sum of squares . $ vec{v} cdot vec{v} = sum_{i=1}^n v_1^2 + ldots + v_n^2 $ . def sum_of_squares(v: Vector) -&gt; Vector: return dot(v, v) assert sum_of_squares([1, 2, 3]) == 14 . Magnitude . $ || vec{v} || = sqrt {v_1^2 + ldots + v_n^2} $ . import math as m def magnitude(v: Vector) -&gt; Vector: return m.sqrt(sum_of_squares(v)) assert magnitude([1, 2, 3]) == m.sqrt(14) . Distance of two vectors . $ d = sqrt { (v_1 - w_1)^2 + ldots + (v_n - w_n)^2 } $ . def distance(vector1: Vector, vector2: Vector) -&gt; Vector: return magnitude(subtract(vector1, vector2)) .",
            "url": "https://andrasnovoszath.com/2020/09/28/vector-algebra.html",
            "relUrl": "/2020/09/28/vector-algebra.html",
            "date": " • Sep 28, 2020"
        }
        
    
  
    
        ,"post22": {
            "title": "Linear regression 'from scratch' to predict Ethereum gas prices from transaction values",
            "content": "In this post I go through the main steps of how to calculate a simple univariate linear regression model. . For the example I try to predict Ethereum daily average gas prices from daily average transaction values. I pull the data from the public Google data base with BigQuery. . Code and inspiration are based on Jason Brownlee&#39;s &quot;Machine Learning Algorithms from Scratch&quot; book . Libraries and data load . import os os.environ[&quot;GOOGLE_APPLICATION_CREDENTIALS&quot;]=os.path.expanduser(&quot;~/.credentials/Notebook bigquery-c422e406404b.json&quot;) . from google.cloud import bigquery client = bigquery.Client() . import altair as alt alt.data_transformers.disable_max_rows() . DataTransformerRegistry.enable(&#39;default&#39;) . query =&quot;&quot;&quot; SELECT EXTRACT(DATE FROM block_timestamp) AS date, AVG(value) AS value, AVG(gas_price) AS gas_price, FROM `bigquery-public-data.ethereum_blockchain.transactions` WHERE EXTRACT(YEAR FROM block_timestamp) = 2019 GROUP BY date ORDER BY date &quot;&quot;&quot; . values = client.query(query).to_dataframe(dtypes={&#39;value&#39;: float, &#39;gas_price&#39;: float}, date_as_object=False) values.head() . date value gas_price . 0 2019-01-01 | 3.719103e+18 | 1.431514e+10 | . 1 2019-01-02 | 4.649915e+18 | 1.349952e+10 | . 2 2019-01-03 | 4.188781e+18 | 1.269504e+10 | . 3 2019-01-04 | 6.958368e+18 | 1.418197e+10 | . 4 2019-01-05 | 8.167590e+18 | 2.410475e+10 | . Calculating linear regression . Linear regression makes predictions with the help of linear coefficient. For an univariate case, this is expressed as: . $ hat{y} = b_0 + b_1 x $ . Coefficients . We can estimate the $b_0$ and $b_1$ coeffients in the following ways: . $ b_1 = frac { sum_{i = 1}^{n} (x_i - bar{x} ) (y_i - bar{y} ) } { sum_{i=1}^{n} (x_i - bar{x})^2 } $ . $ b_0 = bar{y} - b_1 bar{x} $ . Where . $x$: the predictor variable | $y$: the variable to predict | $ bar{x} $ and $ bar{y}$ are their respective means | . Covariance and variance . We can also can get the $b_1$ coefficient from the variance and covariance: . Covariance: $ text{Cov}(x, y) = frac { sum_{i = 1}^{n} (x_i - bar{x} ) (y_i - bar{y} ) } { n } $ | Variance: $ delta^2 = frac { { sum_{i=1}^{n} (x_i - bar{x})^2 } } { n } $ | . From these, we can get . $ b_1 = frac { text{Cov} } { delta^2 } $ . Calculation steps . Accordingly, we need to calculate the following metrics: . Variable means for both $x$ and $y$ | Their deviations from the mean | Covariance of $x$ and $y$ | Variance of $x$ | The $b_1$ coeffcient | The $b_0$ coefficient | $ hat{y}$, that is, the predictions | Means . values.mean() . value 3.173648e+18 gas_price 1.616874e+10 dtype: float64 . Deviations from the mean . def deviation(array): return array - array.mean() . deviation(values[&#39;value&#39;]) . 0 5.454550e+17 1 1.476267e+18 2 1.015132e+18 3 3.784720e+18 4 4.993942e+18 ... 360 -1.004019e+18 361 -1.259703e+18 362 -1.050454e+18 363 -6.660476e+17 364 5.699485e+17 Name: value, Length: 365, dtype: float64 . Covariance . def covariance(arr1, arr2): return (deviation(arr1) * deviation(arr2)).sum() / len(arr1) . covariance(values[&#39;value&#39;], values[&#39;gas_price&#39;]) . 2.8148882775011114e+27 . Variance . def variance(arr): return (deviation(arr) ** 2).sum() / len(arr) . variance(values[&#39;value&#39;]) . 1.3829873312251886e+36 . The $b_1$ coefficient . def b1(arr1, arr2): return covariance(arr1, arr2) / variance(arr1) . b1(values[&#39;value&#39;], values[&#39;gas_price&#39;]) . 2.035368086132359e-09 . The $b_0$ coefficient . def b0(arr1, arr2): return arr2.mean() - b1(arr1, arr2) * arr1.mean() . b_0 = b0(values[&#39;value&#39;], values[&#39;gas_price&#39;]) b_0 . 9709198058.151459 . Predictions . values[&#39;predictions&#39;] = b_0 + values[&#39;value&#39;] * b_1 . values[&#39;predictions&#39;] . 0 1.727894e+10 1 1.917349e+10 2 1.823491e+10 3 2.387204e+10 4 2.633325e+10 ... 360 1.412519e+10 361 1.360478e+10 362 1.403068e+10 363 1.481309e+10 364 1.732880e+10 Name: predictions, Length: 365, dtype: float64 . values.head() . date value gas_price predictions . 0 2019-01-01 | 3.719103e+18 | 1.431514e+10 | 1.727894e+10 | . 1 2019-01-02 | 4.649915e+18 | 1.349952e+10 | 1.917349e+10 | . 2 2019-01-03 | 4.188781e+18 | 1.269504e+10 | 1.823491e+10 | . 3 2019-01-04 | 6.958368e+18 | 1.418197e+10 | 2.387204e+10 | . 4 2019-01-05 | 8.167590e+18 | 2.410475e+10 | 2.633325e+10 | . Plotting the results . First we transform the data into long format . to_plot = values.melt(id_vars=[&#39;date&#39;], value_vars=[&#39;gas_price&#39;, &#39;predictions&#39;], var_name=&#39;status&#39;) to_plot.head() . date status value . 0 2019-01-01 | gas_price | 1.431514e+10 | . 1 2019-01-02 | gas_price | 1.349952e+10 | . 2 2019-01-03 | gas_price | 1.269504e+10 | . 3 2019-01-04 | gas_price | 1.418197e+10 | . 4 2019-01-05 | gas_price | 2.410475e+10 | . Then, we plot the results. . chart = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;, axis=alt.Axis(format=(&quot;%x&quot;), labelAngle=270)), alt.Y(&#39;value:Q&#39;, axis=alt.Axis(format=&quot;,.2e&quot;), scale=alt.Scale(type=&#39;log&#39;)), color=alt.Color(&#39;status:N&#39;) ).properties(width=600) label = alt.selection_single( encodings=[&#39;x&#39;], on=&#39;mouseover&#39;, nearest=True, empty=&#39;none&#39; ) alt.layer( chart, chart.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), alt.Chart().mark_rule(color=&#39;darkgray&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), chart.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;black&#39;, strokeWidth=0.5).encode( text=alt.Text(&#39;value:Q&#39;, format=&#39;,.2e&#39;) ).transform_filter(label), chart.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=alt.Text(&#39;value:Q&#39;, format=&#39;,.2e&#39;) ).transform_filter(label), data=to_plot ).properties(title=&#39;Gas prices and their predictions (log scale)&#39;, width=600, height=400) . It is a bit hard to assess the performance of the results just by looking at them, but at least it seems to generate values within the same ballpark. We could generate metrics as RMSE for reference. . Obvious improvements . remove the outliers | include past values | .",
            "url": "https://andrasnovoszath.com/2020/09/27/linear-regression.html",
            "relUrl": "/2020/09/27/linear-regression.html",
            "date": " • Sep 27, 2020"
        }
        
    
  
    
        ,"post23": {
            "title": "Plotting Ethereum transaction value and gas prices with BigQuery and Altair",
            "content": "As part of getting a better handle on blockchain data, BigQuery, Altair, and Machine Learning, I pulled some Ethereum transaction data and plotted it. . import os os.environ[&quot;GOOGLE_APPLICATION_CREDENTIALS&quot;]=os.path.expanduser(&quot;~/.credentials/Notebook bigquery-c422e406404b.json&quot;) . import altair as alt alt.data_transformers.disable_max_rows() . DataTransformerRegistry.enable(&#39;default&#39;) . from google.cloud import bigquery client = bigquery.Client() . query =&quot;&quot;&quot; SELECT EXTRACT(DATE FROM block_timestamp) AS date, AVG(value) AS average_value, AVG(gas_price) AS average_gas_price, FROM `bigquery-public-data.ethereum_blockchain.transactions` WHERE EXTRACT(YEAR FROM block_timestamp) = 2019 GROUP BY date ORDER BY date &quot;&quot;&quot; . We calculate some basic statistics on raw transaction value data for each day over 2019. . schema = client.get_table(&quot;bigquery-public-data.ethereum_blockchain.transactions&quot;).schema schema . [SchemaField(&#39;hash&#39;, &#39;STRING&#39;, &#39;REQUIRED&#39;, &#39;Hash of the transaction&#39;, (), None), SchemaField(&#39;nonce&#39;, &#39;INTEGER&#39;, &#39;REQUIRED&#39;, &#39;The number of transactions made by the sender prior to this one&#39;, (), None), SchemaField(&#39;transaction_index&#39;, &#39;INTEGER&#39;, &#39;REQUIRED&#39;, &#39;Integer of the transactions index position in the block&#39;, (), None), SchemaField(&#39;from_address&#39;, &#39;STRING&#39;, &#39;REQUIRED&#39;, &#39;Address of the sender&#39;, (), None), SchemaField(&#39;to_address&#39;, &#39;STRING&#39;, &#39;NULLABLE&#39;, &#39;Address of the receiver. null when its a contract creation transaction&#39;, (), None), SchemaField(&#39;value&#39;, &#39;NUMERIC&#39;, &#39;NULLABLE&#39;, &#39;Value transferred in Wei&#39;, (), None), SchemaField(&#39;gas&#39;, &#39;INTEGER&#39;, &#39;NULLABLE&#39;, &#39;Gas provided by the sender&#39;, (), None), SchemaField(&#39;gas_price&#39;, &#39;INTEGER&#39;, &#39;NULLABLE&#39;, &#39;Gas price provided by the sender in Wei&#39;, (), None), SchemaField(&#39;input&#39;, &#39;STRING&#39;, &#39;NULLABLE&#39;, &#39;The data sent along with the transaction&#39;, (), None), SchemaField(&#39;receipt_cumulative_gas_used&#39;, &#39;INTEGER&#39;, &#39;NULLABLE&#39;, &#39;The total amount of gas used when this transaction was executed in the block&#39;, (), None), SchemaField(&#39;receipt_gas_used&#39;, &#39;INTEGER&#39;, &#39;NULLABLE&#39;, &#39;The amount of gas used by this specific transaction alone&#39;, (), None), SchemaField(&#39;receipt_contract_address&#39;, &#39;STRING&#39;, &#39;NULLABLE&#39;, &#39;The contract address created, if the transaction was a contract creation, otherwise null&#39;, (), None), SchemaField(&#39;receipt_root&#39;, &#39;STRING&#39;, &#39;NULLABLE&#39;, &#39;32 bytes of post-transaction stateroot (pre Byzantium)&#39;, (), None), SchemaField(&#39;receipt_status&#39;, &#39;INTEGER&#39;, &#39;NULLABLE&#39;, &#39;Either 1 (success) or 0 (failure) (post Byzantium)&#39;, (), None), SchemaField(&#39;block_timestamp&#39;, &#39;TIMESTAMP&#39;, &#39;REQUIRED&#39;, &#39;Timestamp of the block where this transaction was in&#39;, (), None), SchemaField(&#39;block_number&#39;, &#39;INTEGER&#39;, &#39;REQUIRED&#39;, &#39;Block number where this transaction was in&#39;, (), None), SchemaField(&#39;block_hash&#39;, &#39;STRING&#39;, &#39;REQUIRED&#39;, &#39;Hash of the block where this transaction was in&#39;, (), None)] . values = client.query(query).to_dataframe(dtypes={&#39;average_value&#39;: float, &#39;average_gas_price&#39;: float}, date_as_object=False) values.head() . date average_value average_gas_price . 0 2019-01-01 | 3.719103e+18 | 1.431514e+10 | . 1 2019-01-02 | 4.649915e+18 | 1.349952e+10 | . 2 2019-01-03 | 4.188781e+18 | 1.269504e+10 | . 3 2019-01-04 | 6.958368e+18 | 1.418197e+10 | . 4 2019-01-05 | 8.167590e+18 | 2.410475e+10 | . chart = alt.Chart(values).mark_line().encode( alt.X(&#39;date:T&#39;, axis=alt.Axis(format=(&quot;%x&quot;), labelAngle=270)) ).properties(width=600) alt.layer( chart.encode(alt.Y(&#39;average_value:Q&#39;, axis=alt.Axis(format=&quot;,.2e&quot;)), color=alt.value(&#39;darkred&#39;), opacity=alt.value(0.65)), chart.encode(alt.Y(&#39;average_gas_price&#39;, axis=alt.Axis(format=&quot;,.2e&quot;))) ).resolve_scale(y=&#39;independent&#39;) .",
            "url": "https://andrasnovoszath.com/2020/09/26/ethereum_value_gas_price.html",
            "relUrl": "/2020/09/26/ethereum_value_gas_price.html",
            "date": " • Sep 26, 2020"
        }
        
    
  
    
        ,"post24": {
            "title": "How to use BigQuery in a Jupyter notebook?",
            "content": "When we try to analyze huge datasets (like blockchain data) through BigQuery, it is useful to run the data load and conversion in the cloud and use our local environment mostly for the final transformations and visualization. For these cases, a jupyter notebook seems to be a fitting environment. . There are at least three main ways by which we can access BigQuery data from a notebook: . The most simple one is to use the pandas-gbq library (we already covered it in this previous post) | Using google&#39;s BigQuery notebook extension | The BigQuery python API | Here we will introduce the last two versions. . (This post is based on this official tutorial.) . Setting credentials . Compared to the pandas-gbq library, we need to define the credentials explicitly. . import os os.environ[&quot;GOOGLE_APPLICATION_CREDENTIALS&quot;] = os.path.expanduser(&quot;~/.credentials/Notebook bigquery-c422e406404b.json&quot;) . BigQuery extension . For the extension to work, we need to install the google-cloud-bigquery library. . conda install -c conda-forge google-cloud-bigquery . Then, we load the bigquery extension . %load_ext google.cloud.bigquery . By using the %%bigquery magic command, we immediately define the result of a query as a pandas dataframe. . Here, we pull the total Ethereum token spendings for each day during 2019. . %%bigquery token_transfers SELECT SUM(SAFE_CAST(value AS FLOAT64)) AS total_spent, SAFE_CAST(EXTRACT(DATE FROM block_timestamp) AS DATETIME) AS date FROM `bigquery-public-data.ethereum_blockchain.token_transfers` WHERE EXTRACT(YEAR FROM block_timestamp) = 2019 GROUP BY date ORDER BY date DESC . token_transfers . total_spent date . 0 2.331472e+79 | 2019-12-31 | . 1 4.078401e+79 | 2019-12-30 | . 2 1.773262e+79 | 2019-12-29 | . 3 3.553959e+79 | 2019-12-28 | . 4 2.751157e+79 | 2019-12-27 | . ... ... | ... | . 360 4.226642e+78 | 2019-01-05 | . 361 4.122376e+80 | 2019-01-04 | . 362 1.804472e+80 | 2019-01-03 | . 363 5.833122e+78 | 2019-01-02 | . 364 4.245749e+78 | 2019-01-01 | . 365 rows × 2 columns . import altair as alt alt.data_transformers.disable_max_rows() label = alt.selection_single( # encodings=[&#39;x&#39;], on=&#39;mouseover&#39;, nearest=True, empty=&#39;none&#39; ) chart = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;total_spent:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)) ) alt.layer( chart, chart.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), alt.Chart().mark_rule(color=&#39;darkgray&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), chart.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=alt.Text(&#39;total_spent:Q&#39;, format=&#39;,.2e&#39;) ).transform_filter(label), chart.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=alt.Text(&#39;total_spent:Q&#39;, format=&#39;,.2e&#39;) ).transform_filter(label), data=token_transfers ).properties(width=600, height=400, title=&#39;Daily token spending during 2019 (log scale)&#39;) . BigQuery module . from google.cloud import bigquery client = bigquery.Client() . query =&quot;&quot;&quot; SELECT from_address, SUM(SAFE_CAST(value AS FLOAT64)) AS total_spent, AVG(SAFE_CAST(value AS INT64)) AS average_spent, COUNT(1) AS times_spent FROM `bigquery-public-data.ethereum_blockchain.token_transfers` WHERE EXTRACT(YEAR FROM block_timestamp) = 2019 AND EXTRACT(MONTH FROM block_timestamp) = 09 AND EXTRACT(DAY FROM block_timestamp) = 24 AND EXTRACT(HOUR FROM block_timestamp) = 12 GROUP BY from_address ORDER BY times_spent DESC&quot;&quot;&quot; . For this example we calculate the number of spendings and their total and average values from addresses over a hour. . df = client.query(query).to_dataframe() df . from_address total_spent average_spent times_spent . 0 0x7ba732b1bb952155b720250b477ce154e19ad62f | 1.686990e+12 | 1.963900e+09 | 859 | . 1 0x262e4155e8c5519e668ec26f353d75dd9c18e78f | 3.132121e+23 | NaN | 365 | . 2 0xbd8da72e2f42f5c68b59ee02c2245599ccd702dc | 2.592921e+24 | NaN | 294 | . 3 0x0000000000000000000000000000000000000000 | 7.401857e+24 | 3.058231e+17 | 268 | . 4 0xa71c8bae673f99ac6c0f32c56efc89a8ddb9a501 | 3.896125e+12 | 1.504295e+10 | 259 | . ... ... | ... | ... | ... | . 5279 0xcd338611d74243844f3190b621eb781db53d20b4 | 1.630439e+23 | NaN | 1 | . 5280 0xa9d6b0ad82e46db1895a412ec96b00e18bf95b49 | 1.000000e+09 | 1.000000e+09 | 1 | . 5281 0x4aee792a88edda29932254099b9d1e06d537883f | 5.740766e+22 | NaN | 1 | . 5282 0x71e29ec9e13a39062269fc5c8cba155bb850b23a | 2.270000e+10 | 2.270000e+10 | 1 | . 5283 0x140d6fac06496b21efd086e107d5eca1a16592b3 | 4.335616e+08 | 4.335616e+08 | 1 | . 5284 rows × 4 columns . alt.Chart(df).mark_rect().encode( alt.X(&#39;times_spent:Q&#39;, bin=alt.BinParams(maxbins=20)), alt.Y(&#39;total_spent:Q&#39;, bin=alt.BinParams(maxbins=20)), alt.Color(&#39;count()&#39;), alt.Tooltip(&#39;count()&#39;) ).properties(title=&#39;Total spending value and spending frequency&#39;) . As expected, there are a few number of addresses responsible for the highest spending frequency and the highest spending value during that hour. .",
            "url": "https://andrasnovoszath.com/2020/09/25/bigquery-jupyter.html",
            "relUrl": "/2020/09/25/bigquery-jupyter.html",
            "date": " • Sep 25, 2020"
        }
        
    
  
    
        ,"post25": {
            "title": "Accessing Ethereum token transaction data with ``pandas-gbq``",
            "content": "One way to access blockchain bigquery data with Google BigQuery is to use the pandas-gbq library. . It makes it really easy to take a BigQuery SQL query and download its results as a pandas DataFrame. . Big Query authentication . In order to use this library, you need to authenticate a credential with BigQuery. I found creating a &#39;Service account&#39; to be the most meaningful. You can follow the steps here. . Installation . Installation with conda is straightforward. . $ conda install pandas-gbq --channel conda-forge . Basic usage . We can download a table by defining an SQL query and passing it to the read_gbq method. . import altair as alt import pandas_gbq . As Ethereum data is big, we constrain our query to a single hour. . sql = &quot;&quot;&quot; SELECT block_timestamp, value FROM `bigquery-public-data.ethereum_blockchain.token_transfers` WHERE EXTRACT(YEAR FROM block_timestamp) = 2019 AND EXTRACT(MONTH FROM block_timestamp) = 09 AND EXTRACT(DAY FROM block_timestamp) = 24 AND EXTRACT(HOUR FROM block_timestamp) = 12 &quot;&quot;&quot; . token = pandas_gbq.read_gbq(sql, project_id=&quot;pandas-gbq-test-290508&quot;) . Downloading: 100%|██████████| 15392/15392 [00:03&lt;00:00, 4846.48rows/s] . token . block_timestamp value . 0 2019-09-24 12:47:43+00:00 | 50058584428 | . 1 2019-09-24 12:47:43+00:00 | 30000000 | . 2 2019-09-24 12:47:43+00:00 | 14069000000 | . 3 2019-09-24 12:47:43+00:00 | 170000000 | . 4 2019-09-24 12:47:43+00:00 | 59478199 | . ... ... | ... | . 15387 2019-09-24 12:40:54+00:00 | 3577720000000000000000 | . 15388 2019-09-24 12:40:54+00:00 | 2314760000000000000000 | . 15389 2019-09-24 12:40:54+00:00 | 2399000000000000000000 | . 15390 2019-09-24 12:40:54+00:00 | 5701000000000000000000 | . 15391 2019-09-24 12:40:54+00:00 | 3608000000000000000000 | . 15392 rows × 2 columns . Data transformation . After loading the dataset, it requires some transformations. . First, we want to sort it by timestamps. . token.dtypes . block_timestamp datetime64[ns, UTC] value object dtype: object . token = token.sort_values(&#39;block_timestamp&#39;) . Then we want to convert the values column to float. . token[&#39;value&#39;] = token[&#39;value&#39;].astype(float) . Visualization . As altair does not allow visualization with more than 5000 rows, we need to manually set it possible. . alt.data_transformers.disable_max_rows() . DataTransformerRegistry.enable(&#39;default&#39;) . We plot the average transaction values by minute. As we there are a few number of very high value transactions, we use a log scale. . alt.Chart(token).mark_line().encode( alt.X(&#39;utchoursminutes(block_timestamp):T&#39;), alt.Y(&#39;average(value):Q&#39;, scale=alt.Scale(type=&#39;log&#39;)) ).properties(width=800, title=&#39;Average token transaction values (log scale)&#39;) .",
            "url": "https://andrasnovoszath.com/2020/09/24/pandas-gbq.html",
            "relUrl": "/2020/09/24/pandas-gbq.html",
            "date": " • Sep 24, 2020"
        }
        
    
  
    
        ,"post26": {
            "title": "Random prediction",
            "content": "When we try to build a prediction algorithm, it is a useful practice to first try out a baseline algorithm and see how they perform. Here we will describe a case of random prediction. . (Inspiration and examples are from Jason Brownlee&#39;s Machine Learning Algorithms from Scratch book.) . import pandas as pd import altair as alt import numpy as np from vega_datasets import data . np.random.seed(42) . For the examples we will use the vega &#39;volcano&#39; dataset. The width and height values are constant, so we work only with the values column. . volcano = data(&#39;volcano&#39;) volcano.head() . width height values . 0 87 | 61 | 103 | . 1 87 | 61 | 104 | . 2 87 | 61 | 104 | . 3 87 | 61 | 105 | . 4 87 | 61 | 105 | . The random prediction algorithm . takes a training and a test set | generates the prediction by selecting random elements from the training set | We split the dataset to training and test sets, by taking the first 2/3 and last 1/3 of the data respectively. . train, test = volcano.iloc[: volcano.shape[0] * 2 // 3, :], volcano.iloc[volcano.shape[0] * 2 // 3 :, :] . A small check that the split did not leave out an entry or did not result in an overlap. . assert train.index[-1] + 1 == test.index[0] . We plot the training and the test sets, respectively. . alt.Chart(train.reset_index()).mark_line().encode(alt.Y(&#39;values&#39;), alt.X(&#39;index:Q&#39;), alt.Tooltip(&#39;values&#39;)).interactive().properties(title=&#39;Train data&#39;) . alt.Chart(test.reset_index()).mark_line().encode(alt.Y(&#39;values&#39;), alt.X(&#39;index:Q&#39;), alt.Tooltip(&#39;values&#39;)).interactive().properties(title=&#39;Test data&#39;) . First, we generate the random predictions with replacement. That is, the same values can occur more than once. . predictions = np.random.choice(train[&#39;values&#39;], size=test.shape[0], replace=True) predictions . array([166, 179, 96, ..., 105, 157, 95]) . We calculate the error terms . errors = test[&#39;values&#39;] - predictions errors . 3538 -16 3539 -29 3540 54 3541 54 3542 56 .. 5302 -8 5303 2 5304 -7 5305 -60 5306 2 Name: values, Length: 1769, dtype: int64 . We calculate the root mean squared error of the predictions. . def calculate_rmse(observed, predicted): return np.sqrt(sum((observed - predicted) ** 2)/len(observed)) . The RMSE is around 34.59 which stands for about 1.34 STD. . rmse = calculate_rmse(test[&#39;values&#39;], predictions) rmse . 34.93380641982711 . For reference, if we would simply use the simple mean, we would get an RMSE of 18.42, . calculate_rmse(test[&#39;values&#39;], test[&#39;values&#39;].mean()) . 18.420942507684327 . Let&#39;s plot the results . to_compare = pd.concat( [ test[&#39;values&#39;].rename(&#39;observed&#39;).reset_index(drop=True), pd.Series(predictions).rename(&#39;predictions&#39;).reset_index(drop=True) ], axis=1 ).stack().reset_index().rename(columns={&#39;level_1&#39;: &#39;status&#39;, &#39;level_0&#39;: &#39;index&#39;, 0: &#39;values&#39;}) to_compare . index status values . 0 0 | observed | 150 | . 1 0 | predictions | 166 | . 2 1 | observed | 150 | . 3 1 | predictions | 179 | . 4 2 | observed | 150 | . ... ... | ... | ... | . 3533 1766 | predictions | 105 | . 3534 1767 | observed | 97 | . 3535 1767 | predictions | 157 | . 3536 1768 | observed | 97 | . 3537 1768 | predictions | 95 | . 3538 rows × 3 columns . alt.Chart(to_compare).mark_line().encode( alt.X(&#39;index:Q&#39;), alt.Y(&#39;values&#39;), alt.Color(&#39;status&#39;), alt.OpacityValue(0.7) ).properties(width=1200, title=&#39;Predictions with replacement&#39;) . We put the steps into a function and rerun the prediction without replacement. . def predict_randomly(train, test, replace=True): predictions = np.random.choice(train, size=test.shape[0], replace=True) errors = test - predictions rmse = calculate_rmse(test, predictions) return predictions, rmse . predictions, rmse = predict_randomly(train[&#39;values&#39;], test[&#39;values&#39;], replace=False) rmse . 35.90392934559341 . Finally, we also put the plotting steps into a function . def plot_predictions(observed, predicted): to_compare = pd.concat( [observed.rename(&#39;observed&#39;).reset_index(drop=True), pd.Series(predicted).rename(&#39;predictions&#39;).reset_index(drop=True)], axis=1 ).stack().reset_index().rename(columns={&#39;level_1&#39;: &#39;status&#39;, &#39;level_0&#39;: &#39;index&#39;, 0: &#39;values&#39;}) chart = alt.Chart(to_compare).mark_line().encode( alt.X(&#39;index:Q&#39;), alt.Y(&#39;values&#39;), alt.Color(&#39;status&#39;), alt.OpacityValue(0.7) ).properties(width=1200) chart.display() . The predictions without replacement . plot_predictions(test[&#39;values&#39;], predictions) .",
            "url": "https://andrasnovoszath.com/2020/09/23/random_predictions.html",
            "relUrl": "/2020/09/23/random_predictions.html",
            "date": " • Sep 23, 2020"
        }
        
    
  
    
        ,"post27": {
            "title": "How does data standardization work?",
            "content": "With standardization, we transform the data so its distribution&#39;s center will become 0 and its standard deviation will become 1. . import altair as alt from vega_datasets import data . For this demo, we use the sp500 price index. . sp500 = data(&#39;sp500&#39;) sp500.head() . date price . 0 2000-01-01 | 1394.46 | . 1 2000-02-01 | 1366.42 | . 2 2000-03-01 | 1498.58 | . 3 2000-04-01 | 1452.43 | . 4 2000-05-01 | 1420.60 | . alt.Chart(sp500).mark_line(point=True, strokeWidth=1).encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price&#39;), alt.Tooltip(&#39;price&#39;) ).configure_point(size=10) . The original distribution of prices. . alt.Chart(sp500).mark_bar().encode( x=alt.X(&#39;price:Q&#39;, bin=alt.BinParams(maxbins=20)), y=alt.Y(&#39;count()&#39;), tooltip=alt.Tooltip([&#39;count()&#39;]) ) . We get the standardized value by dividing the values&#39; difference from the mean by the standard deviation. . $ text{standardized value} = frac { text{value} - text{mean}} { text{standard deviation}} $ . The mean and standard deviation of the prices . print(f&quot;&quot;&quot; mean: {sp500[&#39;price&#39;].mean():.2f} std: {sp500[&#39;price&#39;].std():.2f} &quot;&quot;&quot;) . mean: 1184.43 std: 195.41 . We standardize the data set. . standardized_prices = sp500.copy() standardized_prices[&#39;price&#39;] = (sp500[&#39;price&#39;] - sp500[&#39;price&#39;].mean()) / sp500[&#39;price&#39;].std() standardized_prices . date price . 0 2000-01-01 | 1.074812 | . 1 2000-02-01 | 0.931317 | . 2 2000-03-01 | 1.607646 | . 3 2000-04-01 | 1.371473 | . 4 2000-05-01 | 1.208583 | . ... ... | ... | . 118 2009-11-01 | -0.454452 | . 119 2009-12-01 | -0.354814 | . 120 2010-01-01 | -0.565809 | . 121 2010-02-01 | -0.409111 | . 122 2010-03-01 | -0.225086 | . 123 rows × 2 columns . alt.Chart(standardized_prices).mark_bar().encode( x=alt.X(&#39;price:Q&#39;, bin=alt.BinParams(maxbins=20)), y=alt.Y(&#39;count()&#39;), tooltip=alt.Tooltip([&#39;count()&#39;]) ) . The new mean and standard deviation . print(f&quot;&quot;&quot; mean: {standardized_prices[&#39;price&#39;].mean():.2f} std: {standardized_prices[&#39;price&#39;].std():.2f} &quot;&quot;&quot;) . mean: -0.00 std: 1.00 . When plotted on the dates, it provides a similar graph as before, but now with a value domain around 0. . alt.Chart(standardized_prices).mark_line(point=True, strokeWidth=1).encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price&#39;), alt.Tooltip(&#39;price&#39;) ).configure_point(size=10) .",
            "url": "https://andrasnovoszath.com/2020/09/22/standardization.html",
            "relUrl": "/2020/09/22/standardization.html",
            "date": " • Sep 22, 2020"
        }
        
    
  
    
        ,"post28": {
            "title": "Data scale normalization",
            "content": "Normalization is a common technique used in machine learning to render the scales of different magnitudes to a common range between 0 and 1. . Here we demonstrate how this is done with pandas and altair. . Original inspiration: (Jason Brownlee: Machine Learning Algorithms from Scratch)[https://machinelearningmastery.com/machine-learning-algorithms-from-scratch/] . import altair as alt # alt.renderers.enable(&#39;default&#39;) alt.renderers . RendererRegistry(active=&#39;default&#39;, registered=[&#39;colab&#39;, &#39;default&#39;, &#39;html&#39;, &#39;json&#39;, &#39;jupyterlab&#39;, &#39;kaggle&#39;, &#39;mimetype&#39;, &#39;notebook&#39;, &#39;nteract&#39;, &#39;png&#39;, &#39;svg&#39;, &#39;zeppelin&#39;]) . from vega_datasets import data . We use the Gapminder health and income dataset . health_income = data(&#39;gapminder-health-income&#39;) health_income.head() . country income health population . 0 Afghanistan | 1925 | 57.63 | 32526562 | . 1 Albania | 10620 | 76.00 | 2896679 | . 2 Algeria | 13434 | 76.50 | 39666519 | . 3 Andorra | 46577 | 84.10 | 70473 | . 4 Angola | 7615 | 61.00 | 25021974 | . income_domain = [health_income[&#39;income&#39;].min(), health_income[&#39;income&#39;].max()] health_domain = [health_income[&#39;health&#39;].min(), health_income[&#39;health&#39;].max()] alt.Chart(health_income).mark_point().encode( alt.X(&#39;income:Q&#39;, scale=alt.Scale(domain=income_domain)), alt.Y(&#39;health:Q&#39;, scale=alt.Scale(domain=health_domain)), alt.Size(&#39;population:Q&#39;), alt.Tooltip(&#39;country:N&#39;) ).properties(height=600, width=800) . The process: . Take the values&#39; difference from the smallest one; | Take the value range, that is, the difference between the largest and smallest values; | Divide the reduced values with the range. | $ text {scaled value} = frac{value - min} {max - min} $ . The first step ensures that the smallest value will become 0. Dividing the reduced values by the range &#39;compresses&#39; the values so the new maximum becomes 1. . quantitative_columns = [&#39;income&#39;, &#39;health&#39;, &#39;population&#39;] . The original minimum and maximum values . health_income.loc[health_income[quantitative_columns].idxmin(), :] . country income health population . 32 Central African Republic | 599 | 53.8 | 4900274 | . 93 Lesotho | 2598 | 48.5 | 2135022 | . 105 Marshall Islands | 3661 | 65.1 | 52993 | . minimums = health_income[quantitative_columns].min() minimums . income 599.0 health 48.5 population 52993.0 dtype: float64 . health_income.loc[health_income[quantitative_columns].idxmax(), :] . country income health population . 134 Qatar | 132877 | 82.0 | 2235355 | . 3 Andorra | 46577 | 84.1 | 70473 | . 35 China | 13334 | 76.9 | 1376048943 | . maximums = health_income[quantitative_columns].max() maximums . income 1.328770e+05 health 8.410000e+01 population 1.376049e+09 dtype: float64 . Difference of values from the column minimum . health_income[quantitative_columns] - minimums . income health population . 0 1326.0 | 9.13 | 32473569.0 | . 1 10021.0 | 27.50 | 2843686.0 | . 2 12835.0 | 28.00 | 39613526.0 | . 3 45978.0 | 35.60 | 17480.0 | . 4 7016.0 | 12.50 | 24968981.0 | . ... ... | ... | ... | . 182 5024.0 | 28.00 | 93394608.0 | . 183 3720.0 | 26.70 | 4615473.0 | . 184 3288.0 | 19.10 | 26779222.0 | . 185 3435.0 | 10.46 | 16158774.0 | . 186 1202.0 | 11.51 | 15549758.0 | . 187 rows × 3 columns . Value ranges: the difference between the maximum and the minimum . maximums - minimums . income 1.322780e+05 health 3.560000e+01 population 1.375996e+09 dtype: float64 . Let&#39;s normalize the dataset . def normalize_dataset(dataset, quantitative_columns): dataset = dataset.copy() minimums = dataset[quantitative_columns].min() maximums = dataset[quantitative_columns].max() dataset[quantitative_columns] = (dataset[quantitative_columns] - minimums) / (maximums - minimums) return dataset . normalized_health_income = normalize_dataset(health_income, quantitative_columns) normalized_health_income . country income health population . 0 Afghanistan | 0.010024 | 0.256461 | 0.023600 | . 1 Albania | 0.075757 | 0.772472 | 0.002067 | . 2 Algeria | 0.097030 | 0.786517 | 0.028789 | . 3 Andorra | 0.347586 | 1.000000 | 0.000013 | . 4 Angola | 0.053040 | 0.351124 | 0.018146 | . ... ... | ... | ... | ... | . 182 Vietnam | 0.037981 | 0.786517 | 0.067874 | . 183 West Bank and Gaza | 0.028123 | 0.750000 | 0.003354 | . 184 Yemen | 0.024857 | 0.536517 | 0.019462 | . 185 Zambia | 0.025968 | 0.293820 | 0.011743 | . 186 Zimbabwe | 0.009087 | 0.323315 | 0.011301 | . 187 rows × 4 columns . The new minimum and maximum values . normalized_health_income.loc[normalized_health_income[quantitative_columns].idxmin(), :] . country income health population . 32 Central African Republic | 0.000000 | 0.148876 | 0.003523 | . 93 Lesotho | 0.015112 | 0.000000 | 0.001513 | . 105 Marshall Islands | 0.023148 | 0.466292 | 0.000000 | . normalized_health_income.loc[normalized_health_income[quantitative_columns].idxmax(), :] . country income health population . 134 Qatar | 1.000000 | 0.941011 | 0.001586 | . 3 Andorra | 0.347586 | 1.000000 | 0.000013 | . 35 China | 0.096275 | 0.797753 | 1.000000 | . Plotting the normalized data, we got the same results, but with the income, health, and population scales all normalized to the [0, 1] range. . Maximum values . alt.Chart(normalized_health_income).mark_point().encode( alt.X(&#39;income:Q&#39;,), alt.Y(&#39;health:Q&#39;), alt.Size(&#39;population:Q&#39;), alt.Tooltip(&#39;country:N&#39;) ).properties(height=600, width=800) .",
            "url": "https://andrasnovoszath.com/2020/09/21/normalize-scale.html",
            "relUrl": "/2020/09/21/normalize-scale.html",
            "date": " • Sep 21, 2020"
        }
        
    
  
    
        ,"post29": {
            "title": "How to categorize your notes?",
            "content": "I take notes . I take lots of notes. Many of them are ideas to research, explore, and think about, while others are rather tasks or possible projects. . As I collect them continuously, their number grows. After a while, it becomes hard to look through them and prioritize for action. . The obvious way to do this, of course, is to categorize them under topics or themes. . Concept hierarchies . This produces a hierarchical tree-like structure. There a few problems with this: . There always be some idea or task which does not belong to a single category alone but under multiple ones. | As the number of notes and, therefore, the number of categories and sub-categories becomes high, you need to dig very deep to find some notes. If the hierarchies are obvious, this is not really a problem. However, as the number of hierarchy levels and branches grow, the previous issue (i.e., notes belonging under multiple categories) become even more prominent. | . Concept drift . There is also the problem of concept drift. While we refer to a particular thing or category with a specific term, later on, we start to refer to it with a related but different one. Similarly, we do the same with categories. . This shift can happen for multiple reasons, but the most obvious ones are terminological shifts or our changing ‘use’ of the original idea or category. . Concept drift, deep hierarchies, and the overlapping category boundaries can lead to the ‘forgetting’ of notes. They can also lead to making multiple, ‘almost similar’ but ‘slightly different’ versions of them at different places of the hierarchy. . And, even whether the relationship between two ideas should be hierarchical or not also can change from use case to use case. . Chaos everywhere . And note-taking is just the most obvious example for me. I had the same experience with software development (both as writing code and managing versions), technical writing, academic research, work organization, file organization, etc. .",
            "url": "https://andrasnovoszath.com/note-taking/categorization/hierarchies/concept%20drift/chaos%20&%20order/2020/09/20/note-categorization-pain.html",
            "relUrl": "/note-taking/categorization/hierarchies/concept%20drift/chaos%20&%20order/2020/09/20/note-categorization-pain.html",
            "date": " • Sep 20, 2020"
        }
        
    
  
    
        ,"post30": {
            "title": "The sum of stock trading",
            "content": "I thought further about my previous position on whether stock trading is a zero-sum game. Now I see a bigger space for it not to be one. . Trading growth stocks . The situation I am thinking about is an argument I found during my google search. . Very simply, one buys a stock at one point in time and sells it to someone else in another. . Let’s take the ‘classical’ position on stocks and consider it merely an investment asset. In this case, we can argue that stock buyers lent money to a company in exchange for a part of its profits. They sell the stocks when it wants to get their money back, say for their retirement. . Here the value comes for the original owner in that the price will be higher when they sell. . This is the same hope the sellers have for the time when they want to realize their investment. That is, when the original buyers sell the stock, the new buyers can get a ‘fair’ price if, in that given time, the stock is still promising some price growth. . This seems more positive-sum for me. However, this view works if we strip away the ‘speculative’ (?), arbitrage-seeking trading aspect of security trading. In this view, stock trading seems to resemble the trade of a good tool one does not need anymore. . Remaining questions . There are, of course, a number of issues still unclear for me. . The first is whether ‘speculation’ or ‘arbitrage seeking’ inevitably leads to a zero-sum outcome? Also, can we separate ‘straightforward’ investment from these activities? . The next is about the buying price: when is the price too high/low? . The buyer could always want to ask for a higher price. Aren’t selling a stock at a too high price basically strips away the seller’s possible gains? . On the other hand, one might argue that, if the price reflects the market price, there is less space for unfairness (or at least not directly between the two parties). . Third, there is the question of whether a company can grow infinitely or trading stock is always based on this false promise. . Finally, how do we calculate the outcomes when the ‘gain’, in big part, comes from luck and decisions made years before their realization. .",
            "url": "https://andrasnovoszath.com/stock%20trading/zero-sum/finance/valuation/2020/09/19/stock-trading-sum.html",
            "relUrl": "/stock%20trading/zero-sum/finance/valuation/2020/09/19/stock-trading-sum.html",
            "date": " • Sep 19, 2020"
        }
        
    
  
    
        ,"post31": {
            "title": "How to build up a writing habit?",
            "content": "If you tend to analyze a lot but want to build up a consistent writing habit, the following tips might help. . Take notes . Have a notepad or a very light-weight text editor/app that you can open at any time and dump your ideas effortlessly. . Find out what interests you . Review your notes, todo lists, emails, past writing, or any other traces about yourself. Look for frequent themes about which you feel strongly and, even better, about which you developed some depth in skill or knowledge. . Prioritize your themes . Take these themes and group or merge the closely related ones. Then, prioritize them based on their importance. . When you have them ranked, drop the last 80% and focus on the remaining 20%. If you have too too many themes remained (e.g., more than 10), drop even more. . Brainstorm post ideas . For each remaining theme, write as many post ideas as possible, but a minimum of ten. Do not bother with how good or viable they are. The important thing is to have a high number of them. . Rotate your themes . Now you have a list of topics organized under themes. Outline a rotating schedule where you order the themes into a sequence, so, in each day, you write about a topic belonging to the next theme. . Write . Each day, you take the topics belonging under that day’s theme and pick one randomly. Write about it impatiently, badly, and experimentally for a minimum 5 minutes. . When you find that you could write about multiple things but you do not know which to pick, pick the first one and move the second into your ideas under the same theme. . When you run out of time, you will find yourself wanting to write more about something. Move it as a new topic under the same theme. . Publish . Publish the writing to somewhere, where nobody sees is. For example, create an account on Medium or any blogging platform under some alias. . Repeat . When you arrive at the end of the sequence, start again. If you could fill out all your 5 minutes for each day for a week, add an extra 5 minutes for your writing time. .",
            "url": "https://andrasnovoszath.com/writing/habit/2020/09/18/writing-habit.html",
            "relUrl": "/writing/habit/2020/09/18/writing-habit.html",
            "date": " • Sep 18, 2020"
        }
        
    
  
    
        ,"post32": {
            "title": "Is finance a zero-sum activity?",
            "content": "I recently thought about finance and whether it is a purely zero-sum game or has some value-add to it. I would like to know this to see if it is worthy of support either as a personal wealth source or as a social institution. . I did not have time to read academic papers, so I simply did a google search (1-2 hours) and reviewed the top results. . What I found . Just based on this short research, I barely found any substantial arguments against the zero-sum claim. . What counterarguments I found were acknowledged that some forms, like forex or short-term trading, are zero-sum. They argued that, compared to the former types, ‘real’ stock trading can bring about benefits for both parties. Most of these were conflating terminology and concepts (like ‘luck’, or ‘value growth’). . There are probably more substantial arguments in academic literature. Also, probably, there is the additional question of the social usefulness of financial markets for overall capital allocation, but that’s a separate thing. . How I see it now . Of course, as I started to dive into the topic, I quickly realized that is is a bit more complex than I thought. . So let’s break up the question to the following cases: . Pure commercial relationship | Trading of goods | Finance | . Pure commercial transaction . When two people get into a commercial relationship, one is a ‘seller’, and the other is a ‘buyer’. Regarding the price, they have opposite aims. One’s income is another’s cost. . So, in this sense, every commercial transaction is zero-sum. . Trade of ‘comparatively advantageous’ goods . When the seller gives away something they cannot use, but the buyer can, the exchange might benefit both. There is still the issue of the price for which they exchange the good, but the buyer’s benefit from possessing the good might surpass its price. . So, here we can imagine a non-zero-sum relationship. . Finance . Most of the people who trade and invest mostly want to get the most payoff for their buck. So this primarily seems zero-sum. . The mutual benefit might come about when their relationship to their asset is more complex than just trying to get the highest price difference. . Conclusion . Again, I can see that I could think and write about this topic for a long time. Some ideas . How do we measure and compare ‘comparatively advantages’? | What are examples of not purely ‘buy low, sell high’ uses of finance? | How to measure the utility ‘ratio’ between price gain/loss and usability of the result of a buy event? | Is the social benefit of efficient capital distribution via financial markets a fluke, or has empirical evidence behind it? | Can we replace financial markets with an AI and delegate human creativity to more productive things? | .",
            "url": "https://andrasnovoszath.com/finance/zero-sum/valuation/2020/09/17/finance-zero-sum.html",
            "relUrl": "/finance/zero-sum/valuation/2020/09/17/finance-zero-sum.html",
            "date": " • Sep 17, 2020"
        }
        
    
  
    
        ,"post33": {
            "title": "The specific you",
            "content": "When you are thinking about what business area to pursue, it is useful to focus on your specific situation and circumstance. . Why is it useful to find a specialization? . When you do not look at your own specificities, you basically do not use knowledge and skills that others could use but cannot. . You have your history, your dis/abilities, your interests, and your own conceptual framing. Most of it might be useless, especially regarding all the people in the world. . However, there are a few problems which maybe you are the only one who can solve it. . This is not about ‘finding your passion’ but rather to find out your skills, knowledge, approach, etc. These are actually quite tangible. . How to know your specificities? . To see yourself better, write down your characteristics in as many dimensions as possible. . Here are some dimensions which you can look for: . Knowledge, | Skills, | Experience, | Interest. | . One thing you can do is to create a spreadsheet and write as many of these things as possible. Then, value them on a scale (e.g., 1-5) showing how specific these traits and dimensions are to you, especially compared to others. .",
            "url": "https://andrasnovoszath.com/self-knowledge/specialization/2020/09/16/specific-you.html",
            "relUrl": "/self-knowledge/specialization/2020/09/16/specific-you.html",
            "date": " • Sep 16, 2020"
        }
        
    
  
    
        ,"post34": {
            "title": "Valuation technology experimentation",
            "content": "It might be useful and promising to experiment with technologies of valuation. . How does valuation happen now, and how does technology play a role in it? . Current technologies of valuation are in place because of different reasons. However, implementers rarely take into account the way the given technology plays a role in valuing things. . Examples of the use of technology in valuation and some trends . Money is arguably the most prominent example. Probably this is not independent of that most of us still do not really understand all its effects. . Other examples are in accountancy, trade, work management, economic metrics, environmental management. . An obvious note is that, of course, other, not strictly technological components, like language, moral values, physical factors, etc., also have critial roles. . Think about and experiment with different technologies to generate new valuation situations . Accordingly, it might be beneficial to try out technologies with explicit valuation functions in mind. . For the most part, and especially in early stages, this is concept-driven (i.e., we are looking for a particular effect to emerge). However, an uncertain but promising benefit would be discovering ‘unthought’ features due to things’ specific and idiosyncratic character. . The process of valuation technology experiments and its main challenges . Most of the time, we cannot implement large scale infrastructures for the sake of experimentation. . Instead, we can try other paths: . Implementation of a small-scale but very focused version | Scenario building | Simulation | Game development | The empirical analysis of similar processes | The empirical analysis of current technologies’ edge cases | Speculation | Incremental implementation | . Current action possibilities and possible future directions . As, currently, I this all is very high level and conceptual, and I am also mostly interested in discovery activities: . Collecting ideas | Conceptual exploration of background ideas | Case study analysis | Code experimentations | . In the future, I might be able to focus it down and branch out toward more serious development work. .",
            "url": "https://andrasnovoszath.com/technology/valuation/experimentation/2020/09/15/technnology_valuation_experimentation.html",
            "relUrl": "/technology/valuation/experimentation/2020/09/15/technnology_valuation_experimentation.html",
            "date": " • Sep 15, 2020"
        }
        
    
  
    
        ,"post35": {
            "title": "Note-taking obsessions",
            "content": "I have an obsession with the process of note-taking. . I often write down my ideas in the form of notes or tasks. I do this through multiple channels. Then I spend a considerable amount of time organizing them. . One common frustration with organizing notes is that I can do this only along one hierarchical dimension. . Perhaps one reason I like to code is that you can implement hierarchies along multiple dimensions more easily, although not always. . Because of the single hierarchy, some notes tend to become forgotten as I put them in some deep end part of a category branch. This then requires me to constantly re- and review my notes and to recategorize and reorganize them. . I have looked into multiple note-taking software, but there is not really a perfect solution for this. . One thing I found helpful is to require only a single function from a particular tool. So, for instance, I would use one particular tool or process for note-taking, then another for categorization, and perhaps a further one for review or note management. . This, however, needs that the different tools speak to each other relatively well. Ready-made solutions often do not allow it, so I found myself moving toward text-based tools. However, you need to utilize some basic scripting to make them work. .",
            "url": "https://andrasnovoszath.com/note-taking/2020/09/14/note-taking-obsessions.html",
            "relUrl": "/note-taking/2020/09/14/note-taking-obsessions.html",
            "date": " • Sep 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About",
          "content": "Online mental dump to put things out. Currently, I practice writing posts within 30 minutes. Still getting there… .",
          "url": "https://andrasnovoszath.com/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  

  

  
  

  
      ,"page12": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://andrasnovoszath.com/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}